<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>云筑小站</title>
  
  <subtitle>脚踏实地、仰望星空;低头走路，抬头看天;既练轻功，也练内功。</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://xiejm.com/"/>
  <updated>2018-04-17T04:45:23.000Z</updated>
  <id>http://xiejm.com/</id>
  
  <author>
    <name>XieJM</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hive的JDBC使用Java API编程</title>
    <link href="http://xiejm.com/Hive/HIve--JDBC%20API.html"/>
    <id>http://xiejm.com/Hive/HIve--JDBC API.html</id>
    <published>2017-10-11T03:02:00.000Z</published>
    <updated>2018-04-17T04:45:23.000Z</updated>
    
    <content type="html"><![CDATA[<p>在之前的UDF函数编程的项目中再新建一个class<br>下面只是一个简单的模板，实际应用的话还可以再加工下，比如执行语句作为一个参数传入进来，封装成一个类。<br><a id="more"></a></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.xiejm.bigdata.hive;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.SQLException;</span><br><span class="line"><span class="keyword">import</span> java.sql.Connection;</span><br><span class="line"><span class="keyword">import</span> java.sql.ResultSet;</span><br><span class="line"><span class="keyword">import</span> java.sql.Statement;</span><br><span class="line"><span class="keyword">import</span> java.sql.DriverManager;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HiveJDBC</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> String driverName=<span class="string">"org.apache.hive.jdbc.HiveDriver"</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> String url=<span class="string">"jdbc:hive2://192.168.2.10:10000/default"</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> String username=<span class="string">"hadoop"</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> String password=<span class="string">""</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">       <span class="keyword">try</span>&#123;</span><br><span class="line">        Class.forName(driverName);</span><br><span class="line">        Connection connection = DriverManager.getConnection(url,username,password);</span><br><span class="line">        Statement stms = connection.createStatement();</span><br><span class="line">        ResultSet rs = stms.executeQuery(<span class="string">"select * from emp"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(rs.next()) &#123;</span><br><span class="line">            System.out.println(rs.getInt(<span class="number">1</span>) + <span class="string">" , "</span> + rs.getString(<span class="number">2</span>));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">"Table emp select successfully."</span>);</span><br><span class="line">       &#125;<span class="keyword">catch</span>(Exception e) &#123;</span><br><span class="line">            <span class="comment">//将可能抛出的Exception异常捕获</span></span><br><span class="line"></span><br><span class="line">            System.out.println(<span class="string">"Got a Exception："</span> + e.getMessage());</span><br><span class="line">            <span class="comment">//打印捕获的异常的堆栈信息，从堆栈信息中可以发现异常发生的位置和原因</span></span><br><span class="line"></span><br><span class="line">            e.printStackTrace();</span><br><span class="line">            <span class="keyword">throw</span> e;    <span class="comment">//不做进一步处理，将异常向外抛出</span></span><br><span class="line">        &#125;<span class="keyword">finally</span>&#123;</span><br><span class="line">            rs.close();</span><br><span class="line">            stms.close();</span><br><span class="line">            connection.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在之前的UDF函数编程的项目中再新建一个class&lt;br&gt;下面只是一个简单的模板，实际应用的话还可以再加工下，比如执行语句作为一个参数传入进来，封装成一个类。&lt;br&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://xiejm.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hive" scheme="http://xiejm.com/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive的元数据管理</title>
    <link href="http://xiejm.com/Hive/Hive--metastore.html"/>
    <id>http://xiejm.com/Hive/Hive--metastore.html</id>
    <published>2017-10-09T09:38:12.000Z</published>
    <updated>2018-04-17T04:45:25.000Z</updated>
    
    <content type="html"><![CDATA[<p>我们知道，Hive的元数据并不存放在HDFS上，而是存放在传统的RDBMS中，典型的如MySQL，这里我们以MySQL为元数据库，结合1.1.0版本的Hive为例进行说明。<br>连接上MySQL后可以看到Hive元数据对应的表有36个，以下是部分主要表的简要说明。</p><table><thead><tr><th>表名</th><th>说明</th><th>关联键</th></tr></thead><tbody><tr><td>BUCKETING_COLS</td><td>存储bucket字段信息，通过SD_ID与其他表关联</td></tr><tr><td>CDS</td><td>一个字段CD_ID，与SDS表关联</td></tr><tr><td>COLUMNS_V2</td><td>Hive表字段信息(字段注释，字段名，字段类型，字段序号)</td></tr><tr><td>DBS</td><td>Hive中所有数据库的基本信息</td></tr><tr><td>GLOBAL_PRIVS</td><td>全局变量，与表无关</td></tr><tr><td>PARTITIONS</td><td>Hive表分区信息(创建时间，具体的分区)</td></tr><tr><td>PARTITION_KEYS</td><td>Hive分区表分区字段(名称，类型，comment，序号)</td></tr><tr><td>PARTITION_KEY_VALS</td><td>Hive表分区名(键值，序号)</td></tr><tr><td>PARTITION_PARAMS</td><td>存储某分区相关信息，包括文件数，文件大小，记录条数等。通过PART_ID关联</td></tr><tr><td>ROLES</td><td>角色表，和GLOBAL_PRIVS配合，与表无关</td></tr><tr><td>SDS</td><td>所有hive表、表分区所对应的hdfs数据目录和数据格式</td><td>SD_ID,SERDE_ID</td></tr><tr><td>SEQUENCE_TABLE</td><td>存储sqeuence相关信息，与表无关</td></tr><tr><td>SERDES</td><td>Hive表序列化反序列化使用的类库信息</td></tr><tr><td>SERDE_PARAMS</td><td>序列化反序列化信息，如行分隔符、列分隔符、NULL的表示字符等</td></tr><tr><td>SORT_COLS</td><td>Hive表SORTED BY字段信息(字段名，sort类型，字段序号)</td></tr><tr><td>TABLE_PARAMS</td><td>表级属性，如是否外部表，表注释等</td></tr><tr><td>TBLS</td><td>所有hive表的基本信息</td></tr><tr><td>TBL_PRIVS</td><td>表赋权限相关信息，通过TBL_ID关联</td></tr><tr><td>VERSION</td><td>存储Hive版本的元数据表</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;我们知道，Hive的元数据并不存放在HDFS上，而是存放在传统的RDBMS中，典型的如MySQL，这里我们以MySQL为元数据库，结合1.1.0版本的Hive为例进行说明。&lt;br&gt;连接上MySQL后可以看到Hive元数据对应的表有36个，以下是部分主要表的简要说明。&lt;/p&gt;
      
    
    </summary>
    
      <category term="大数据" scheme="http://xiejm.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hive" scheme="http://xiejm.com/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive的分区表</title>
    <link href="http://xiejm.com/Hive/Hive--partition.html"/>
    <id>http://xiejm.com/Hive/Hive--partition.html</id>
    <published>2017-10-09T07:53:44.000Z</published>
    <updated>2018-04-17T04:45:27.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要是对Hive分区表的总结，Hive分区表有三种类型：静态单级分区表、静态多级分区表、动态分区表<br>在实际工作中Hive的分区表应用非常多，特别是动态分区表。</p><p>直接上干货！</p><h2 id="静态分区表的使用"><a href="#静态分区表的使用" class="headerlink" title="静态分区表的使用"></a>静态分区表的使用</h2><h3 id="单级分区表创建"><a href="#单级分区表创建" class="headerlink" title="单级分区表创建"></a>单级分区表创建</h3><p>新建一个分区表，并导入数据。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> order_partition(</span><br><span class="line">order_number <span class="keyword">string</span>,</span><br><span class="line">event_time <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line">PARTITIONED <span class="keyword">BY</span>(event_month <span class="keyword">string</span>,<span class="keyword">day</span> <span class="keyword">STRING</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/root/order_created.txt'</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> order_partition <span class="keyword">PARTITION</span>(event_month=<span class="string">'201709'</span>);</span><br><span class="line"></span><br><span class="line">Loading data to table default.order_partition partition (event_month=201709)</span><br><span class="line">Partition default.order_partition&#123;event_month=201709&#125; stats: [numFiles=1, numRows=0, totalSize=208, rawDataSize=0]</span><br><span class="line">OK</span><br><span class="line">Time taken: 1.121 seconds</span><br></pre></td></tr></table></figure><p>从上面的语句可以看出与创建普通表的区别是多了<code>PARTITIONED BY</code></p><p>在创建表时候，使用<code>PARTITIONED BY</code>关键字来指定该表为分区表，后面括号中指定了分区的字段和类型，分区字段可以有多个，在HDFS中对应多级目录。</p><p>以上语句如果报错显示乱码，解决办法：<br>在MySQL中修改Hive的元数据 数据库中的分区表的编码</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">database</span> hive3 <span class="built_in">character</span> <span class="keyword">set</span> latin1;</span><br><span class="line"><span class="keyword">use</span> hive3;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> <span class="keyword">PARTITIONS</span> <span class="keyword">convert</span> <span class="keyword">to</span> <span class="built_in">character</span> <span class="keyword">set</span> latin1;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> PARTITION_KEYS <span class="keyword">convert</span> <span class="keyword">to</span> <span class="built_in">character</span> <span class="keyword">set</span> latin1;</span><br></pre></td></tr></table></figure><p>用beeline查询分区表记录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop001:10000/default&gt; select * from order_partition;</span><br><span class="line"></span><br><span class="line">INFO  : OK</span><br><span class="line">+-------------------------------+-----------------------------+------------------------------+--+</span><br><span class="line">| order_partition.order_number  | order_partition.event_time  | order_partition.event_month  |</span><br><span class="line">+-------------------------------+-----------------------------+------------------------------+--+</span><br><span class="line">| 10703007267488                | 2014-05-01 06:01:12.334+01  | 201709                       |</span><br><span class="line">| 10101043505096                | 2014-05-01 07:28:12.342+01  | 201709                       |</span><br><span class="line">| 10103043509747                | 2014-05-01 07:50:12.33+01   | 201709                       |</span><br><span class="line">| 10103043501575                | 2014-05-01 09:27:12.33+01   | 201709                       |</span><br><span class="line">| 10104043514061                | 2014-05-01 09:03:12.324+01  | 201709                       |</span><br><span class="line">+-------------------------------+-----------------------------+------------------------------+--+</span><br><span class="line">5 rows selected (0.141 seconds)</span><br></pre></td></tr></table></figure><p>查看下order_partition.txt ，用来与表做对比</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ cat data/order_created.txt</span><br><span class="line">107030072674882014-05-01 06:01:12.334+01</span><br><span class="line">101010435050962014-05-01 07:28:12.342+01</span><br><span class="line">101030435097472014-05-01 07:50:12.33+01</span><br><span class="line">101030435015752014-05-01 09:27:12.33+01</span><br><span class="line">101040435140612014-05-01 09:03:12.324+01</span><br></pre></td></tr></table></figure><p>从<code>select * from order_partition;</code> 和 <code>cat data/order_created.txt</code>的结果对比得出一个结果：<br>分区列不是表中的一个实际的列，其实是一个伪列</p><h3 id="使用ALTER-TABLE修改分区"><a href="#使用ALTER-TABLE修改分区" class="headerlink" title="使用ALTER TABLE修改分区"></a>使用ALTER TABLE修改分区</h3><p>如果我们在HDFS中创建了分区表目录，并通过HDFS导入数据到该目录，那么在Hive端是查询不到该分区表的数据的<br>我们需要关联该数据</p><p>使用下列语句,具体语法看官网wiki</p><p>方法一：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> order_partition <span class="keyword">ADD</span> <span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> <span class="keyword">PARTITION</span>(event_month=<span class="string">'201709'</span>)</span><br></pre></td></tr></table></figure><p>方法二：</p><p>作用：将没有metastore中的分区表信息，添加到metastore中</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MSCK <span class="keyword">REPAIR</span> order_partition; 不推荐使用，太暴力了</span><br></pre></td></tr></table></figure><p>为什么暴力：<br>MSCK的操作是表级别的，<br>假设metastore数据存了一年，一天1个分区，一年就是365分区，这样MSCK一执行很可能把其他不需要刷新元数据信息的表也给刷新了，所以存在风险。</p><h3 id="使用INSERT添加分区"><a href="#使用INSERT添加分区" class="headerlink" title="使用INSERT添加分区"></a>使用INSERT添加分区</h3><p>往分区中追加数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> order_partition <span class="keyword">PARTITION</span> (<span class="keyword">month</span> = <span class="string">'2017-10'</span>,<span class="keyword">day</span> = <span class="string">'2017-10-01'</span>)</span><br></pre></td></tr></table></figure><p>覆盖分区数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> overwrite <span class="keyword">TABLE</span> order_partition <span class="keyword">PARTITION</span> (<span class="keyword">month</span> = <span class="string">'2017-10'</span>,<span class="keyword">day</span> = <span class="string">'2017-10-01'</span>)</span><br></pre></td></tr></table></figure><h3 id="查询分区表信息"><a href="#查询分区表信息" class="headerlink" title="查询分区表信息"></a>查询分区表信息</h3><p>查看一个分区对应的HDFS路径信息</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">partitions</span> order_partition;</span><br><span class="line"></span><br><span class="line">OK</span><br><span class="line">event_month=201709</span><br><span class="line">Time taken: 0.041 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure><h3 id="静态多级分区表"><a href="#静态多级分区表" class="headerlink" title="静态多级分区表"></a>静态多级分区表</h3><p>上面已经说了静态分区表的一些使用方法，这里在补充一个多级分区表的创建</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> order_mulit_partition(</span><br><span class="line">order_number <span class="keyword">string</span>,</span><br><span class="line">event_time <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line">PARTITIONED <span class="keyword">BY</span>(event_month <span class="keyword">string</span>,<span class="keyword">day</span> <span class="keyword">STRING</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/root/order_created.txt'</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> order_partition <span class="keyword">PARTITION</span>(event_month=<span class="string">'201709'</span>,<span class="keyword">day</span>=<span class="string">'2017-09-15'</span>);</span><br></pre></td></tr></table></figure><p>创建分区表返回信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Loading data to table default.order_mulit_partition partition (event_month=201709, day=2017-09-15)</span><br><span class="line">Partition default.order_mulit_partition&#123;event_month=201709, day=2017-09-15&#125; stats: [numFiles=1, numRows=0, totalSize=208, rawDataSize=0]</span><br><span class="line">OK</span><br><span class="line">Time taken: 1.121 seconds</span><br><span class="line">`</span><br></pre></td></tr></table></figure><p>说明：上面的表order_mulit_partition分区event_month=’201709’,day=’2017-09-15’对应HDFS上的路径为：/user/hive/warehouse/default.db/order_partition/event_month=201709/day=2017-09-15/，当查询中指定了event_month=’201709’ AND dday=’2017-09-15’,MapReduce直接从该目录中读取数据，如果只指定了event_month=’201709’，那么MapReduce将/month=2015-06/下所有的子目录都作为Input。</p><p>查看表</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from order_mulit_partition;</span><br><span class="line">OK</span><br><span class="line">107030072674882014-05-01 06:01:12.334+012017092017-09-15</span><br><span class="line">101010435050962014-05-01 07:28:12.342+012017092017-09-15</span><br><span class="line">101030435097472014-05-01 07:50:12.33+012017092017-09-15</span><br><span class="line">101030435015752014-05-01 09:27:12.33+012017092017-09-15</span><br><span class="line">101040435140612014-05-01 09:03:12.324+012017092017-09-15</span><br><span class="line">Time taken: 0.325 seconds, Fetched: 5 row(s)</span><br></pre></td></tr></table></figure><p>可以看到有两个字段,再执行一次载入数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/root/order_created.txt'</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> order_mulit_partition <span class="keyword">PARTITION</span>(event_month=<span class="string">'201709'</span>,<span class="keyword">day</span>=<span class="string">'2017-09-16'</span>);</span><br></pre></td></tr></table></figure><p>查看表</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from order_mulit_partition;</span><br><span class="line">OK</span><br><span class="line">107030072674882014-05-01 06:01:12.334+012017092017-09-15</span><br><span class="line">101010435050962014-05-01 07:28:12.342+012017092017-09-15</span><br><span class="line">101030435097472014-05-01 07:50:12.33+012017092017-09-15</span><br><span class="line">101030435015752014-05-01 09:27:12.33+012017092017-09-15</span><br><span class="line">101040435140612014-05-01 09:03:12.324+012017092017-09-15</span><br><span class="line">107030072674882014-05-01 06:01:12.334+012017092017-09-16</span><br><span class="line">101010435050962014-05-01 07:28:12.342+012017092017-09-16</span><br><span class="line">101030435097472014-05-01 07:50:12.33+012017092017-09-16</span><br><span class="line">101030435015752014-05-01 09:27:12.33+012017092017-09-16</span><br><span class="line">101040435140612014-05-01 09:03:12.324+012017092017-09-16</span><br></pre></td></tr></table></figure><p>再来查下order_mulit_partition分区表信息</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">partitions</span> order_mulit_partition;</span><br><span class="line">OK</span><br><span class="line">event_month=201709/day=2017-09-15</span><br><span class="line">event_month=201709/day=2017-09-16</span><br><span class="line">Time taken: 0.033 seconds, Fetched: 2 row(s)</span><br></pre></td></tr></table></figure><p>在多分区使用的时候注意：每个分区字段要写明，顺序等等</p><h2 id="动态分区表"><a href="#动态分区表" class="headerlink" title="动态分区表"></a>动态分区表</h2><p><strong>这种方式在工作中才是常用的</strong>,<br>动态分区无需手工指定数据导入的具体分区，<br>而由select语句中的字段自行决定导出到哪一个分区中，<br>并自动创建相应的分区，使用上更加方便便捷。</p><p>假设有一个需求：按照不同部门作为分区导入数据到目标表</p><p>如果这张表有很多列，全部这样写SQL 语句效率太差,那么我们就可以使用动态分区表</p><h3 id="创建分区表"><a href="#创建分区表" class="headerlink" title="创建分区表"></a>创建分区表</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> emp_dynamic_partition(</span><br><span class="line">empno <span class="built_in">int</span>, </span><br><span class="line">ename <span class="keyword">string</span>, </span><br><span class="line">job <span class="keyword">string</span>, </span><br><span class="line">mgr <span class="built_in">int</span>, </span><br><span class="line">hiredate <span class="keyword">string</span>, </span><br><span class="line">sal <span class="keyword">double</span>, </span><br><span class="line">comm <span class="keyword">double</span>)</span><br><span class="line">PARTITIONED <span class="keyword">BY</span>(deptno <span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure><h3 id="使用动态方式导入数据"><a href="#使用动态方式导入数据" class="headerlink" title="使用动态方式导入数据"></a>使用动态方式导入数据</h3><p>导入数据前先设置动态分区的模式：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; <span class="built_in">set</span> hive.exec.dynamic.partition.mode=nonstrict;</span><br></pre></td></tr></table></figure><p>开始导入数据</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert into table emp_dynamic_partition partition(deptno)</span><br><span class="line">              &gt; select empno , ename , job , mgr , hiredate , sal , comm, deptno from emp;</span><br><span class="line">Query ID = hadoop_20170925072626_d98526f6-67aa-4593-b723-5a5bd4e422a8</span><br><span class="line">Total <span class="built_in">jobs</span> = 3</span><br><span class="line">Launching Job 1 out of 3</span><br><span class="line">Number of reduce tasks is <span class="built_in">set</span> to 0 since there<span class="string">'s no reduce operator</span></span><br><span class="line"><span class="string">Starting Job = job_1506263819287_0002, Tracking URL = http://hadoop001:8088/proxy/application_1506263819287_0002/</span></span><br><span class="line"><span class="string">Kill Command = /home/hadoop/app/hadoop//bin/hadoop job  -kill job_1506263819287_0002</span></span><br><span class="line"><span class="string">Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0</span></span><br><span class="line"><span class="string">2017-09-25 07:28:03,866 Stage-1 map = 0%,  reduce = 0%</span></span><br><span class="line"><span class="string">2017-09-25 07:28:10,214 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.37 sec</span></span><br><span class="line"><span class="string">MapReduce Total cumulative CPU time: 2 seconds 370 msec</span></span><br><span class="line"><span class="string">Ended Job = job_1506263819287_0002</span></span><br><span class="line"><span class="string">Stage-4 is selected by condition resolver.</span></span><br><span class="line"><span class="string">Stage-3 is filtered out by condition resolver.</span></span><br><span class="line"><span class="string">Stage-5 is filtered out by condition resolver.</span></span><br><span class="line"><span class="string">Moving data to: hdfs://hadoop001:9000/user/hive/warehouse/emp_dynamic_partition/.hive-staging_hive_2017-09-25_07-27-57_012_6944175324299480316-1/-ext-10000</span></span><br><span class="line"><span class="string">Loading data to table default.emp_dynamic_partition partition (deptno=null)</span></span><br><span class="line"><span class="string"> Time taken for load dynamic partitions : 542</span></span><br><span class="line"><span class="string">Loading partition &#123;deptno=20&#125;</span></span><br><span class="line"><span class="string">Loading partition &#123;deptno=__HIVE_DEFAULT_PARTITION__&#125;</span></span><br><span class="line"><span class="string">Loading partition &#123;deptno=10&#125;</span></span><br><span class="line"><span class="string">Loading partition &#123;deptno=30&#125;</span></span><br><span class="line"><span class="string"> Time taken for adding to write entity : 3</span></span><br><span class="line"><span class="string">Partition default.emp_dynamic_partition&#123;deptno=10&#125; stats: [numFiles=1, numRows=3, totalSize=130, rawDataSize=127]</span></span><br><span class="line"><span class="string">Partition default.emp_dynamic_partition&#123;deptno=20&#125; stats: [numFiles=1, numRows=5, totalSize=214, rawDataSize=209]</span></span><br><span class="line"><span class="string">Partition default.emp_dynamic_partition&#123;deptno=30&#125; stats: [numFiles=1, numRows=6, totalSize=275, rawDataSize=269]</span></span><br><span class="line"><span class="string">Partition default.emp_dynamic_partition&#123;deptno=__HIVE_DEFAULT_PARTITION__&#125; stats: [numFiles=1, numRows=1, totalSize=44, rawDataSize=43]</span></span><br><span class="line"><span class="string">MapReduce Jobs Launched: </span></span><br><span class="line"><span class="string">Stage-Stage-1: Map: 1   Cumulative CPU: 2.37 sec   HDFS Read: 4705 HDFS Write: 961 SUCCESS</span></span><br><span class="line"><span class="string">Total MapReduce CPU Time Spent: 2 seconds 370 msec</span></span><br><span class="line"><span class="string">OK</span></span><br><span class="line"><span class="string">empnoenamejobmgrhiredatesalcommdeptno</span></span><br><span class="line"><span class="string">Time taken: 16.388 seconds</span></span><br></pre></td></tr></table></figure><p>从上面的mapreduce运行输出结果可以看出，已经根据deptno 动态分区了</p><h3 id="查询已导入数据"><a href="#查询已导入数据" class="headerlink" title="查询已导入数据"></a>查询已导入数据</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop001:10000/default&gt; select * from emp_dynamic_partition ;</span><br><span class="line"></span><br><span class="line">+------------------------------+------------------------------+----------------------------+----------------------------+---------------------------------+----------------------------+-----------------------------+-------------------------------+--+</span><br><span class="line">| emp_dynamic_partition.empno  | emp_dynamic_partition.ename  | emp_dynamic_partition.job  | emp_dynamic_partition.mgr  | emp_dynamic_partition.hiredate  | emp_dynamic_partition.sal  | emp_dynamic_partition.comm  | emp_dynamic_partition.deptno  |</span><br><span class="line">+------------------------------+------------------------------+----------------------------+----------------------------+---------------------------------+----------------------------+-----------------------------+-------------------------------+--+</span><br><span class="line">| 7782                         | CLARK                        | MANAGER                    | 7839                       | 1981-6-9                        | 2450.0                     | NULL                        | 10                            |</span><br><span class="line">| 7839                         | KING                         | PRESIDENT                  | NULL                       | 1981-11-17                      | 5000.0                     | NULL                        | 10                            |</span><br><span class="line">| 7934                         | MILLER                       | CLERK                      | 7782                       | 1982-1-23                       | 1300.0                     | NULL                        | 10                            |</span><br><span class="line">| 7369                         | SMITH                        | CLERK                      | 7902                       | 1980-12-17                      | 800.0                      | NULL                        | 20                            |</span><br><span class="line">| 7566                         | JONES                        | MANAGER                    | 7839                       | 1981-4-2                        | 2975.0                     | NULL                        | 20                            |</span><br><span class="line">| 7788                         | SCOTT                        | ANALYST                    | 7566                       | 1987-4-19                       | 3000.0                     | NULL                        | 20                            |</span><br><span class="line">| 7876                         | ADAMS                        | CLERK                      | 7788                       | 1987-5-23                       | 1100.0                     | NULL                        | 20                            |</span><br><span class="line">| 7902                         | FORD                         | ANALYST                    | 7566                       | 1981-12-3                       | 3000.0                     | NULL                        | 20                            |</span><br><span class="line">| 7499                         | ALLEN                        | SALESMAN                   | 7698                       | 1981-2-20                       | 1600.0                     | 300.0                       | 30                            |</span><br><span class="line">| 7521                         | WARD                         | SALESMAN                   | 7698                       | 1981-2-22                       | 1250.0                     | 500.0                       | 30                            |</span><br><span class="line">| 7654                         | MARTIN                       | SALESMAN                   | 7698                       | 1981-9-28                       | 1250.0                     | 1400.0                      | 30                            |</span><br><span class="line">| 7698                         | BLAKE                        | MANAGER                    | 7839                       | 1981-5-1                        | 2850.0                     | NULL                        | 30                            |</span><br><span class="line">| 7844                         | TURNER                       | SALESMAN                   | 7698                       | 1981-9-8                        | 1500.0                     | 0.0                         | 30                            |</span><br><span class="line">| 7900                         | JAMES                        | CLERK                      | 7698                       | 1981-12-3                       | 950.0                      | NULL                        | 30                            |</span><br><span class="line">| 8888                         | HIVE                         | PROGRAM                    | 7839                       | 1988-1-23                       | 10300.0                    | NULL                        | NULL                          |</span><br><span class="line">+------------------------------+------------------------------+----------------------------+----------------------------+---------------------------------+----------------------------+-----------------------------+-------------------------------+--+</span><br></pre></td></tr></table></figure><p>再来看下HDFS上的文件，就更加清楚了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ hadoop dfs -ls /user/hive/warehouse/emp_dynamic_partition</span><br><span class="line">DEPRECATED: Use of this script to execute hdfs <span class="built_in">command</span> is deprecated.</span><br><span class="line">Instead use the hdfs <span class="built_in">command</span> <span class="keyword">for</span> it.</span><br><span class="line"></span><br><span class="line">Found 4 items</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2017-09-25 07:28 /user/hive/warehouse/emp_dynamic_partition/deptno=10</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2017-09-25 07:28 /user/hive/warehouse/emp_dynamic_partition/deptno=20</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2017-09-25 07:28 /user/hive/warehouse/emp_dynamic_partition/deptno=30</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2017-09-25 07:28 /user/hive/warehouse/emp_dynamic_partition/deptno=__HIVE_DEFAULT_PARTITION__</span><br></pre></td></tr></table></figure><p>到此，我们看到了动态分区的好处了</p><p>更多关于分区的介绍，可参考官方文档：<br><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-AddPartitions" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-AddPartitions</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文主要是对Hive分区表的总结，Hive分区表有三种类型：静态单级分区表、静态多级分区表、动态分区表&lt;br&gt;在实际工作中Hive的分区表应用非常多，特别是动态分区表。&lt;/p&gt;
&lt;p&gt;直接上干货！&lt;/p&gt;
&lt;h2 id=&quot;静态分区表的使用&quot;&gt;&lt;a href=&quot;#静态分区表的
      
    
    </summary>
    
      <category term="大数据" scheme="http://xiejm.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hive" scheme="http://xiejm.com/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive--数据库和表</title>
    <link href="http://xiejm.com/Hive/Hive--DDL.html"/>
    <id>http://xiejm.com/Hive/Hive--DDL.html</id>
    <published>2017-10-08T10:23:30.000Z</published>
    <updated>2018-04-17T04:45:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文介绍Hive的<code>数据存储</code>和一些常用<code>DDL</code>、<code>DML</code>操作<br><a id="more"></a></p><h2 id="数据存储"><a href="#数据存储" class="headerlink" title="数据存储"></a>数据存储</h2><p>首先需要清楚Hive中的数据存储的位置，一般生产环境中元数据是存放在MySQL等数据库中，因为这些数据要不断更新、修改，不适合存储在HDFS中。<br>而真正的数据是存在HDFS中，这样更有利于对数据做分布式运算。</p><p>Hive中主要包括四类数据模型：<br>Hive中主要包含以下几种数据模型：<strong>Table(表)</strong>，<strong>External Table(外部表)</strong>，<strong>Partition(分区)</strong>，<strong>Bucket(桶)</strong></p><ul><li><strong>Table(表)</strong>：Hive中的表和关系型数据库中的表在概念上很相似，每个表在HDFS中都有对应的目录来存储表数据，这个目录可以通过${HIVE_HOME}/conf/hive-site.xml配置文件中的hive.metastore.warehouse.dir属性来配置，这个属性默认的值是/user/hive/warehouse(这个目录在 HDFS上)，可以根据实际需要修改这个路径。</li><li><strong>External Table(外部表)</strong>：Hive中外部表与表相似，但是外部表的数据不是存储在自己所属的目录中，而是存在其他地方。当你要删除外部表时，这个外部表所对应的数据文件是不会贝删除的，删除的仅仅是与之对应的元数据信息。</li><li><strong>Partition(分区)</strong>：在Hive中，所有分区数据都存储在表路径的对应子目录中。</li><li><strong>Bucket(桶)</strong>：对指定的列计算Hash，根据Hash切分数据，目的是为了并行，每个桶对应一个文件。</li></ul><p>Hive的数据默认存储有一个根目录，在hive-site.xml中，由参数hive.metastore.warehouse.dir指定。默认值为/user/hive/warehouse.</p><p>For example ：</p><p>如果创建一个表：emp，当创建成功后在HDFS中会创建<code>/user/hive/warehouse/emp</code>目录(这里假定hive.metastore.warehouse.dir配置为/user/hive/warehouse),<strong>emp表的所有数据都存在这个目录中</strong>。</p><p>如果创建分区表order_partition，指定分区列event_month=201405 ，当创建成功后在HDFS中对应的目录是<code>/user/hive/warehouse/order_partition/event_month=201405</code></p><p>如果将emp表分散到8个桶中，首先对ID列的值计算Hash，对应的Hash值为0-8的HDFS存储路径：/user/hive/warehouse/emp/part-00000;而hash值为2的数据存储的HDFS 目录为：/user/hive/warehouse/emp/part-00002。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://oss.xiejm.com/Note/Hive/201709181543.jpg" alt="Hive表结构" title="">                </div>                <div class="image-caption">Hive表结构</div>            </figure><h2 id="Hive的DDL和DML操作"><a href="#Hive的DDL和DML操作" class="headerlink" title="Hive的DDL和DML操作"></a>Hive的DDL和DML操作</h2><p>名词解释 <code>DDL</code>：Data Definition Language，数据定义语言</p><p>在关系型数据库中的CREATE、DELETE、ALTER，这些都是DDL</p><p>打开官网的<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL" target="_blank" rel="noopener">Hive wiki</a>。在User Documentation有一个DDL文档。</p><h3 id="DDL概述"><a href="#DDL概述" class="headerlink" title="DDL概述"></a>DDL概述</h3><p>常用的HiveQL DDL includeing:</p><ul><li>CREATE DATABASE,TABLE,FUNCTION</li><li>DROP DATABASE,TABLE</li><li>ALTER DATABASE,TABLE</li><li>MSCK PEPAIR TABLE</li><li>SHOW DATABASES,TABLES,FUNCTIONS</li></ul><h2 id="DDL示例"><a href="#DDL示例" class="headerlink" title="DDL示例"></a>DDL示例</h2><p>以下是一些常用的DDL操作示例。都来自于官网文档，所有官网文档是第一手资料，<strong>一定要会查官网文档</strong>。</p><p>【<strong>重点</strong>】当使用hive中的QL语句时，一定要知道这条语句对应的元数据信息时怎么存储的</p><h3 id="数据库DDL"><a href="#数据库DDL" class="headerlink" title="数据库DDL"></a>数据库DDL</h3><ol><li>create database 基本语法</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> (<span class="keyword">DATABASE</span>|<span class="keyword">SCHEMA</span>) [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] database_name</span><br><span class="line">  [<span class="keyword">COMMENT</span> database_comment]</span><br><span class="line">  [LOCATION hdfs_path]</span><br><span class="line">  [<span class="keyword">WITH</span> DBPROPERTIES (property_name=property_value, ...)];</span><br><span class="line">  说明：</span><br><span class="line">  - IF NOT EXISTS：如果不存在则创建</span><br><span class="line">  - <span class="keyword">COMMENT</span>：注释</span><br><span class="line">  - LOCATION：数据库存放目录</span><br><span class="line">  - <span class="keyword">WITH</span> DBPROPERTIES：拓展信息，<span class="keyword">key</span>/<span class="keyword">value</span></span><br></pre></td></tr></table></figure><p>我们知道hive的数据存储在HDFS件中，默认情况下，其存储在hive配置文件hive-site.xml参数hive.metastore.warehouse.dir指定的目录下，默认值为/user/hive/warehouse，当然我们也可以在创建数据库时，指定LOCATION值来修改默认的路径</p><ol><li>创建数据库</li></ol><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; CREATE DATABASE IF NOT EXISTS tsetdb</span><br><span class="line">    &gt;        COMMENT 'add comment:it is my database'</span><br><span class="line">    &gt;        WITH DBPROPERTIES ('create_time'='2017-09-19','creator'='xiejm');</span><br></pre></td></tr></table></figure><p>通过命令dfs -ls /user/hive/warehouse查看文件是否存在</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; dfs -ls /user/hive/warehouse;</span><br><span class="line">Found 1 items</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2017-09-19 13:39 /user/hive/warehouse/testdb.db</span><br><span class="line"></span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure><ol><li>创建数据库指定的存放目录（LOCATION参数)</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; CREATE DATABASE IF NOT EXISTS tsetdb_local  </span><br><span class="line">    &gt; COMMENT <span class="string">'tsetdb_local '</span>  </span><br><span class="line">    &gt; LOCATION <span class="string">'/user/hadoop'</span>         </span><br><span class="line">    &gt; WITH DBPROPERTIES (<span class="string">'create_time'</span>=<span class="string">'2017-09-19'</span>);</span><br></pre></td></tr></table></figure><p>通过命令dfs -ls /user/hadoop查看，发现并没有生产tsetdb_local.db目录</p><p>再使用命令DESC database database_name查看数据库的创建脚本，对比添加LOCATION参数不同之处</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc database testdb;</span><br><span class="line">OK</span><br><span class="line">testdb  hdfs://hadoop001:9000/user/hive/warehouse/testdb.dbrootUSER</span><br><span class="line">Time taken: 0.009 seconds, Fetched: 1 row(s)</span><br><span class="line">hive&gt; desc database tsetdb_local;</span><br><span class="line">OK</span><br><span class="line">tsetdb_local    tsetdb_local    hdfs://hadoop001:9000/user/hadoop   root    USER</span><br><span class="line">Time taken: 0.026 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure><p>我们还可以通过到MySQL命令行查hive的元数据信息，查看刚创建的数据库</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from DBS \G;</span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">          DB_ID: 1</span><br><span class="line">           DESC: Default Hive database</span><br><span class="line">DB_LOCATION_URI: hdfs://hadoop001:9000/user/hive/warehouse</span><br><span class="line">           NAME: default</span><br><span class="line">     OWNER_NAME: public</span><br><span class="line">     OWNER_TYPE: ROLE</span><br><span class="line">*************************** 2. row ***************************</span><br><span class="line">          DB_ID: 6</span><br><span class="line">           DESC: NULL</span><br><span class="line">DB_LOCATION_URI: hdfs://hadoop001:9000/user/hive/warehouse/hive.db</span><br><span class="line">           NAME: hive</span><br><span class="line">     OWNER_NAME: root</span><br><span class="line">     OWNER_TYPE: USER</span><br><span class="line"></span><br><span class="line">*************************** 3. row ***************************</span><br><span class="line">          DB_ID: 17</span><br><span class="line">           DESC: NULL</span><br><span class="line">DB_LOCATION_URI: hdfs://hadoop001:9000/user/hive/warehouse/testdb.db</span><br><span class="line">           NAME: testdb</span><br><span class="line">     OWNER_NAME: root</span><br><span class="line">     OWNER_TYPE: USER</span><br><span class="line">*************************** 4. row ***************************</span><br><span class="line">          DB_ID: 21</span><br><span class="line">           DESC: tsetdb_local </span><br><span class="line">DB_LOCATION_URI: hdfs://hadoop001:9000/user/hadoop</span><br><span class="line">           NAME: tsetdb_local</span><br><span class="line">     OWNER_NAME: root</span><br><span class="line">     OWNER_TYPE: USER</span><br><span class="line">9 rows <span class="keyword">in</span> <span class="built_in">set</span> (0.00 sec)</span><br></pre></td></tr></table></figure><ol><li>显示现有数据库</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show databases;</span><br></pre></td></tr></table></figure><p>通过上述操作我们发现，default为默认数据库，它在HDFS中的存储路径是<code>/user/hive/warehouse</code>,也就是我们配置文件中指定的默认路径。</p><ol><li>根据条件查询现有数据库</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show databases like <span class="string">'test*'</span>;</span><br><span class="line">OK</span><br><span class="line">testdb</span><br><span class="line">testdb_anthinfo</span><br><span class="line">testdb_comment</span><br><span class="line">testdb_local</span><br><span class="line">Time taken: 0.005 seconds, Fetched: 4 row(s)</span><br></pre></td></tr></table></figure><ol><li>修改数据库</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ALTER (DATABASE|SCHEMA) database_name SET DBPROPERTIES (property_name=property_value, ...);</span><br><span class="line">ALTER (DATABASE|SCHEMA) database_name SET OWNER [USER|ROLE] user_or_role;</span><br></pre></td></tr></table></figure><p>修改数据的create_time属性</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; alter database testdb <span class="built_in">set</span> dbproperties (<span class="string">'create_time'</span> =<span class="string">'2017-9-11'</span>);</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.046 seconds</span><br></pre></td></tr></table></figure><p>7.删除数据库</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DROP (DATABASE|SCHEMA) [IF EXISTS] database_name [RESTRICT|CASCADE];</span><br></pre></td></tr></table></figure><p>当删除database_name数据库，默认情况下只能删除空数据库，当数据库为非空时，可以使用drop database databse_name cascade 级联删除。<strong>实际生产中不建议使用cascade参数进行暴力删除</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 删除空数据库</span></span><br><span class="line">hive&gt; drop database tsetdb_local;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.507 seconds</span><br><span class="line"></span><br><span class="line"><span class="comment">#删除非空表报错</span></span><br><span class="line">hive&gt; drop database user;</span><br><span class="line">FAILED: Execution Error, <span class="built_in">return</span> code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. InvalidOperationException(message:Database user is not empty. One or more tables exist.)</span><br><span class="line"></span><br><span class="line"><span class="comment">#暴力删除表 加上cascade参数。</span></span><br><span class="line">hive&gt; drop database user cascade;</span><br><span class="line">OK</span><br><span class="line">Time taken: 1.441 seconds</span><br></pre></td></tr></table></figure><h2 id="表DDL"><a href="#表DDL" class="headerlink" title="表DDL"></a>表DDL</h2><p>语法基本结构</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> [<span class="keyword">EXTERNAL</span>] <span class="keyword">TABLE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] [db_name.]table_name</span><br><span class="line">  [(col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)]</span><br><span class="line">  [<span class="keyword">COMMENT</span> table_comment]</span><br><span class="line">  [PARTITIONED <span class="keyword">BY</span> (col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)]</span><br><span class="line">  [</span><br><span class="line">   [<span class="keyword">ROW</span> <span class="keyword">FORMAT</span> row_format]</span><br><span class="line">   [<span class="keyword">STORED</span> <span class="keyword">AS</span> file_format]</span><br><span class="line">  ]</span><br><span class="line">  [LOCATION hdfs_path]</span><br><span class="line">  [<span class="keyword">AS</span> select_statement];</span><br><span class="line">  说明：</span><br><span class="line">  - EXTERNAL：外部表</span><br><span class="line">  - IF NOT EXISTS：表不存在创建</span><br><span class="line">  - db_name：表所属数据库</span><br><span class="line">  - <span class="keyword">COMMENT</span> col_comment：列注释</span><br><span class="line">  - <span class="keyword">COMMENT</span> table_comment：表注释</span><br><span class="line">  - PARTITIONED <span class="keyword">BY</span>：分区字段</span><br><span class="line">  - <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> row_format:行的数据格式</span><br><span class="line">  - <span class="keyword">STORED</span> <span class="keyword">AS</span> file_format:文件存储格式</span><br><span class="line">  - LOCATION hdfs_path：存放路径</span><br><span class="line">  - <span class="keyword">AS</span> select_statement：查询语句为结果集</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] [db_name.]table_name</span><br><span class="line">  <span class="keyword">LIKE</span> existing_table_or_view_name</span><br><span class="line">  [LOCATION hdfs_path];</span><br><span class="line">  说明：</span><br><span class="line">  - IF NOT EXISTS：表不存在创建</span><br><span class="line">  - db_name：表所属数据库</span><br><span class="line">  - existing_table_or_view_name：结果集为存在的表或者师徒</span><br><span class="line">  - LOCATION hdfs_path：存放路径</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文介绍Hive的&lt;code&gt;数据存储&lt;/code&gt;和一些常用&lt;code&gt;DDL&lt;/code&gt;、&lt;code&gt;DML&lt;/code&gt;操作&lt;br&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://xiejm.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hive" scheme="http://xiejm.com/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>hive内部表与外部表区别详细介绍</title>
    <link href="http://xiejm.com/Hive/Hive--ManagerTableAndExtendTable.html"/>
    <id>http://xiejm.com/Hive/Hive--ManagerTableAndExtendTable.html</id>
    <published>2017-10-08T06:42:30.000Z</published>
    <updated>2018-04-17T04:45:34.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文记录了Hive的内部表与外部表区别<br><a id="more"></a></p><h2 id="内部表"><a href="#内部表" class="headerlink" title="内部表"></a>内部表</h2><ul><li>首先我们用下面语句在Hive中创建一个表：</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> emp(</span><br><span class="line">empno <span class="built_in">int</span>,ename <span class="keyword">string</span>,job <span class="keyword">string</span>,mgr <span class="built_in">int</span>,hiredate <span class="keyword">string</span>,sal <span class="keyword">double</span>,deptno <span class="built_in">int</span></span><br><span class="line">)<span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"></span><br><span class="line">OK</span><br><span class="line">Time taken: 0.359 seconds</span><br></pre></td></tr></table></figure><ul><li>接着我们就在Hive里面创建了一张普通的表，现在我们给这个表导入数据：</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/root/emp.txt'</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> emp;</span><br><span class="line">Loading data to table default.emp</span><br><span class="line">Table default.emp stats: [numFiles=1, numRows=0, totalSize=700, rawDataSize=0]</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.82 seconds</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hive&gt; select * from emp;</span><br><span class="line">OK</span><br><span class="line">7369SMITHCLERK79021980-12-17800.0NULL</span><br><span class="line">7499ALLENSALESMAN76981981-2-201600.0300</span><br><span class="line">7521WARDSALESMAN76981981-2-221250.0500</span><br><span class="line">7566JONESMANAGER78391981-4-22975.0NULL</span><br><span class="line">7654MARTINSALESMAN76981981-9-281250.01400</span><br><span class="line">7698BLAKEMANAGER78391981-5-12850.0NULL</span><br><span class="line">7782CLARKMANAGER78391981-6-92450.0NULL</span><br><span class="line">7788SCOTTANALYST75661987-4-193000.0NULL</span><br><span class="line">7839KINGPRESIDENTNULL1981-11-175000.0NULL</span><br><span class="line">7844TURNERSALESMAN76981981-9-81500.00</span><br><span class="line">7876ADAMSCLERK77881987-5-231100.0NULL</span><br><span class="line">7900JAMESCLERK76981981-12-3950.0NULL</span><br><span class="line">7902FORDANALYST75661981-12-33000.0NULL</span><br><span class="line">7934MILLERCLERK77821982-1-231300.0NULL</span><br><span class="line">8888HIVEPROGRAM78391988-1-2310300.0NULL</span><br><span class="line">Time taken: 1.027 seconds, Fetched: 15 row(s)</span><br></pre></td></tr></table></figure><ul><li>查看emp表存储的HDFS文件</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; dfs -ls /user/hive/warehouse/emp;</span><br><span class="line">Found 1 items</span><br><span class="line">-rwxr-xr-x   1 root supergroup        700 2017-09-20 17:13 /user/hive/warehouse/emp/emp.txt</span><br></pre></td></tr></table></figure><ul><li>接着我们来删除emp</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##############删除表</span></span><br><span class="line">hive&gt; drop table emp;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.239 seconds</span><br><span class="line"></span><br><span class="line"><span class="comment">################查emo表的文件提示没有这个目录</span></span><br><span class="line">hive&gt; dfs -ls /user/hive/warehouse/emp;</span><br><span class="line">ls: `/user/hive/warehouse/emp<span class="string">': No such file or directory</span></span><br><span class="line"><span class="string">Command failed with exit code = 1</span></span><br><span class="line"><span class="string">Query returned non-zero code: 1, cause: null</span></span><br></pre></td></tr></table></figure><p>看到没，当我们把emp表删除后，再去查HDFS文件提示目录不存在。表数据已经被删除了。<br>如果你的Hadoop启动垃圾箱机制，那么drop table 命令将会把表的所有移动到垃圾箱。</p><h2 id="外部表"><a href="#外部表" class="headerlink" title="外部表"></a>外部表</h2><ul><li>现在我们来创建一个外部表：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create external table emp_external(</span><br><span class="line">    &gt; empno int,ename string,job string,mgr int,hiredate string,sal double,deptno int);</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.213 seconds</span><br></pre></td></tr></table></figure><p>通过上面的创建表命令我们发现，创建表和外部表的区别，创建外部表多了external关键字<br>当然我们在创建外部表时还可以加一个<code>location &#39;hdfs PATH&#39;</code>参数来指定目录<br>默认情况下，Hive将在HDFS上的/user/hive/warehouse/文件夹下以外部表的表名创建一个文件夹，并将属于这个表的数据存放在这里<br>接着使用hive 中的dfs命令查下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; dfs -ls /user/hive/warehouse;</span><br><span class="line">Found 2 items</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2017-09-20 17:45 /user/hive/warehouse/emp_external</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2017-09-13 21:46 /user/hive/warehouse/hive.db</span><br></pre></td></tr></table></figure><ul><li>导入表数据</li></ul><p>和创建表的导入数据到表一样，将本地的数据导入到外部表</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; load data <span class="built_in">local</span> inpath <span class="string">'/root/emp.txt'</span> overwrite into table emp_external;</span><br><span class="line">Loading data to table default.emp_external</span><br><span class="line">Table default.emp_external stats: [numFiles=1, numRows=0, totalSize=700, rawDataSize=0]</span><br><span class="line">OK</span><br><span class="line">Time taken: 1.087 seconds</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">############再emp_external目录下多了一个数据文件</span></span><br><span class="line">hive&gt; dfs -ls /user/hive/warehouse/emp_external;</span><br><span class="line">Found 1 items</span><br><span class="line">-rwxr-xr-x   1 root supergroup        700 2017-09-20 17:56 /user/hive/warehouse/emp_external/emp.txt</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure><ul><li>删除外部表</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##############删除表</span></span><br><span class="line">hive&gt; drop table emp_external;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.326 seconds</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hive&gt; dfs -ls /user/hive/warehouse/emp_external;</span><br><span class="line">Found 1 items</span><br><span class="line">-rwxr-xr-x   1 root supergroup        700 2017-09-20 17:56 /user/hive/warehouse/emp_external/emp.txt</span><br></pre></td></tr></table></figure><p>通过上面删除表后查看HDFS文件发现表被删除了，但是表对应的数据文件并没有被删除</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>外部表中的数据并不是由它自己来管理的！而表则不一样；</li><li>在删除表的时候，Hive将会把属于表的元数据和数据全部删掉；而删除外部表的时候，Hive仅仅删除外部表的元数据，数据是不会删除的！</li><li>在与第三方共用数据时，可以使用外部表。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文记录了Hive的内部表与外部表区别&lt;br&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://xiejm.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hive" scheme="http://xiejm.com/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive--UDF开发</title>
    <link href="http://xiejm.com/Hive/Hive--UDF.html"/>
    <id>http://xiejm.com/Hive/Hive--UDF.html</id>
    <published>2017-10-08T06:39:22.000Z</published>
    <updated>2018-04-17T04:45:38.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>Hive中，除了提供丰富的内置函数之外还可以通过实现用户定义函数（User-Defined Functions，UDF）进行扩展（事实上，大多数Hive功能都是通过扩展UDF实现的）。</p><p>开发自定义UDF函数有两种方式，一个是继承<code>org.apache.hadoop.hive.ql.exec.UDF</code>，另一个是继承<code>org.apache.hadoop.hive.ql.udf.generic.GenericUDF</code>；并重载<code>evaluate</code>方法。Hive API提供@Description声明，使用声明可以在代码中添加UDF的具体信息。在Hive中可以使用<code>DESC</code>、<code>DESCRIBE</code>来展现这些信息。</p><p>如果是针对简单的数据类型（比如String、Integer等）可以使用UDF，如果是针对复杂的数据类型（比如Array、Map、Struct等），可以使用GenericUDF，另外，GenericUDF还可以在函数开始之前和结束之后做一些初始化和关闭的处理操作。</p><p>Hive的源码本身就是编写UDF最好的参考资料。在Hive源代码中很容易就能找到与需求功能相似的UDF实现，只需要复制过来，并加以适当的修改就可以满足需求。</p><h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><ul><li>CentOS6.X</li><li>JDK1.8</li><li>Hadoop2.6.0-cdh5.7.0</li><li>Hive-1.1.0-cdh5.7.0</li><li>Maven3.3.9</li></ul><h2 id="编写UDF类"><a href="#编写UDF类" class="headerlink" title="编写UDF类"></a>编写UDF类</h2><p>1.首先在IDEA中新建一个maven项目<br>2.pom.xml中添加相关依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">"http://maven.apache.org/POM/4.0.0"</span> <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span></span></span><br><span class="line"><span class="tag">  <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.xiejm.bigdata<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-train<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">packaging</span>&gt;</span>jar<span class="tag">&lt;/<span class="name">packaging</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive-train<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://maven.apache.org<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">project.build.sourceEncoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">project.build.sourceEncoding</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">hadoop.version</span>&gt;</span>2.6.0-cdh5.7.0<span class="tag">&lt;/<span class="name">hadoop.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">hive.version</span>&gt;</span>1.1.0-cdh5.7.0<span class="tag">&lt;/<span class="name">hive.version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">repositories</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">id</span>&gt;</span>cloudera<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">url</span>&gt;</span> https://repository.cloudera.com/artifactory/cloudera-repos/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">repositories</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hadoop.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--cdh的hive依赖--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-accumulo-handler<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hive.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-ant<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hive.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-beeline<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hive.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span>    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-cli<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hive.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hive.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-contrib<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hive.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hive.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.10<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">scope</span>&gt;</span>test<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure><p>3.新建一个Java Class，命名为ToLower</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.xiejm.bigdata.hive;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.Description;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Created by jimmy on 2017/9/29.</span></span><br><span class="line"><span class="comment"> * 这个函数是将字符串全部转化为大写字母</span></span><br><span class="line"><span class="comment"> * add jar samplecode.jar;</span></span><br><span class="line"><span class="comment"> * create temporary function toupper as 'com.xiejm.bigdata.hive.ToUpper';</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@Description</span>(name = <span class="string">"ToUpper"</span>,</span><br><span class="line">        value = <span class="string">"_FUNC_(str) - Converts a string to uppercase"</span>,</span><br><span class="line">        extended = <span class="string">"Example:\n "</span></span><br><span class="line">                + <span class="string">"  &gt; SELECT _FUNC_(str) FROM src LIMIT 1;\n"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ToUpper</span> <span class="keyword">extends</span> <span class="title">UDF</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> Text <span class="title">evaluate</span><span class="params">(Text input)</span> </span>&#123;</span><br><span class="line">        Text result = <span class="keyword">new</span> Text(<span class="string">""</span>);</span><br><span class="line">        <span class="keyword">if</span> (input != <span class="keyword">null</span>) &#123;</span><br><span class="line">            result.set(input.toString().toUpperCase());</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>4.在IDEA中编译该项目，将编译后的jar包上传到Linux中<br>5.hive中加载jar包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 添加UDF jar包</span></span><br><span class="line">hive (default)&gt; add jar /root/hive-train-1.0.jar</span><br><span class="line">Added [/root/hive-train-1.0.jar] to class path</span><br><span class="line">Added resources: [/root/hive-train-1.0.jar]</span><br><span class="line"></span><br><span class="line"><span class="comment">## 创建UDF函数</span></span><br><span class="line">hive (default)&gt; create temporary <span class="keyword">function</span> toupper as <span class="string">'com.xiejm.bigdata.hive.ToUpper'</span>;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.005 seconds</span><br><span class="line"></span><br><span class="line"><span class="comment">##查看UDF函数，显示函数注释</span></span><br><span class="line">hive (default)&gt; desc <span class="keyword">function</span> extended toupper;</span><br><span class="line">OK</span><br><span class="line">toupper(str) - Converts a string to uppercase</span><br><span class="line">Example:</span><br><span class="line">   &gt; SELECT toupper(str) FROM src LIMIT 1;</span><br><span class="line"></span><br><span class="line">Time taken: 0.007 seconds, Fetched: 4 row(s)</span><br><span class="line"></span><br><span class="line">hive (default)&gt; <span class="built_in">exit</span>;</span><br></pre></td></tr></table></figure><p>上面的方法介绍了如何添加临时的UDF函数，那么如果要永久加载这个函数，我们该怎么做呢？ </p><p>步骤和上面的方法是一样的，只需把CREATE语句中的TEMPORARY关键字删除即可。</p><p>当不需要这个函数了，可以把这个函数从Hive中删除,<br>使用下面的语法格式</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DROP [TEMPORARY] FUNCTION function_name</span><br></pre></td></tr></table></figure><h2 id="进阶：将自定义UDF编译到hive源码中"><a href="#进阶：将自定义UDF编译到hive源码中" class="headerlink" title="进阶：将自定义UDF编译到hive源码中"></a>进阶：将自定义UDF编译到hive源码中</h2><p>上面的方法每次使用都要add,create一下，还是很麻烦。那么问题来了：如何把UDF集成到Hive的源码中？</p><p><strong>步骤如下：</strong></p><p>1.下载hive1.1.0-cdh5.7.0的源码包<br>2.在Linux 上解压到/root目录<br>3.将编写好的 UDF 类文件ToUpper.java复制到Hive源码目录中,并修改</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cp ToUpper.java /root/hive-1.1.0-cdh5.7.0/ql/src/java/org/apache/hadoop/hive/ql/udf</span><br><span class="line"></span><br><span class="line">vim /root/hive-1.1.0-cdh5.7.0/ql/src/java/org/apache/hadoop/hive/ql/udf/ToUpper.java</span><br><span class="line"><span class="comment">##将 package com.xiejm.bigdata.hello; 修改为 package org.apache.hadoop.hive.ql.udf;</span></span><br></pre></td></tr></table></figure><p>4.修改FunctionRegistry.java 文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /root/hive-1.1.0-cdh5.7.0/ql/src/java/org/apache/hadoop/hive/ql/<span class="built_in">exec</span>/FunctionRegistry.java</span><br></pre></td></tr></table></figure><p>添加import</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.hadoop.hive.ql.udf.ToUpper;</span><br></pre></td></tr></table></figure><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// registry <span class="keyword">for</span> system <span class="built_in">functions</span></span><br><span class="line">  private static final Registry system = new Registry(<span class="literal">true</span>);</span><br></pre></td></tr></table></figure><p>找到上面的registry for system functions代码块中的static{}添加register</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">system.registerUDF(<span class="string">"ToUpper"</span>, ToUpper.class, <span class="literal">false</span>);</span><br></pre></td></tr></table></figure><p>5.编译Hive</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /root/hive-1.1.0-cdh5.7.0</span><br><span class="line"><span class="comment">## 编译时间比较长</span></span><br><span class="line">mvn clean package -DskipTests -Phadoop-2 -Pdist</span><br></pre></td></tr></table></figure><p>编译成功后会生成一个hive程序包和目录的地址</p><p>包地址：<code>/root/hive-1.1.0-cdh5.7.0/packaging/target/apache-hive-1.1.0-cdh5.7.0-bin.tar.gz</code></p><p>目录地址：<code>/root/hive-1.1.0-cdh5.7.0/packaging/target/apache-hive-1.1.0-cdh5.7.0-bin</code></p><p>6.使集成的UDF函数生效</p><p>方法一：<br>替换hive-exec-1.1.0-cdh5.7.0.jar包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mv <span class="variable">$HIVE_HOME</span>/lib/hive-exec-1.1.0-cdh5.7.0.jar <span class="variable">$HIVE_HOME</span>/lib/hive-exec-1.1.0-cdh5.7.0.jar.bak</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cp /root/hive-1.1.0-cdh5.7.0/packaging/target/apache-hive-1.1.0-cdh5.7.0-bin/apache-hive-1.1.0-cdh5.7.0-bin/lib/hive-exec-1.1.0-cdh5.7.0.jar <span class="variable">$HIVE_HOME</span>/lib/</span><br></pre></td></tr></table></figure><p>方法二：<br>复制编译生成的包重新部署。</p><p>7.测试UDF</p><p>测试先我们要结束hive的runjar进程，使用<code>jps</code>命令查看，如果有再用<code>kill -9</code> 结束进程</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 显示自定义UDF函数toupper信息</span></span><br><span class="line">hive (default)&gt; desc <span class="keyword">function</span> toupper;</span><br><span class="line">OK</span><br><span class="line">toupper(str) - Converts a string to uppercase</span><br><span class="line">Time taken: 0.014 seconds, Fetched: 1 row(s)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 调用toupper函数</span></span><br><span class="line">hive (default)&gt; select toupper(<span class="string">'jimmy'</span>) from emp <span class="built_in">limit</span> 2;</span><br><span class="line">OK</span><br><span class="line">JIMMY</span><br><span class="line">JIMMY</span><br><span class="line">Time taken: 0.087 seconds, Fetched: 2 row(s)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;Hive中，除了提供丰富的内置函数之外还可以通过实现用户定义函数（User-Defined Functions，UDF）进行扩展（事实上，大
      
    
    </summary>
    
      <category term="大数据" scheme="http://xiejm.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hive" scheme="http://xiejm.com/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive--概述</title>
    <link href="http://xiejm.com/Hive/Hive--Overview.html"/>
    <id>http://xiejm.com/Hive/Hive--Overview.html</id>
    <published>2017-10-06T13:17:22.000Z</published>
    <updated>2018-04-17T04:45:41.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一-Hive是什么"><a href="#一-Hive是什么" class="headerlink" title="一.Hive是什么"></a>一.Hive是什么</h2><ol><li>Hive是使用一种类似于用SQL的查询语言，直接作用在分布式存储系统之上。</li><li>由Facebook开源，解决海量数据<strong>结构化的日志数据</strong>统计问题</li><li>构建在Hadoop之上的数据仓库：<strong>数据存放在HDFS,计算通过YARN和MR</strong></li><li>引擎：Hive QL —&gt; MapReduce  重点关注HQL翻译成MR会产生几个作业</li><li>Hive底层：MapReduce、Spark（Hive on Spark）、Tez</li><li>压缩/存储格式： 在生产中如何选择合适的压缩方式，这才是关键。压缩会消耗一定的CPU资源。</li><li>生产环境慎选 Spark（Hive on Spark）</li><li>现在的至少有70%用Hive来做数据仓库的</li></ol><h2 id="二-产生背景"><a href="#二-产生背景" class="headerlink" title="二.产生背景"></a>二.产生背景</h2><ol><li><p>由于MapReduce的繁琐：Mapper—&gt;Reducer—&gt;Driver—&gt;package(打包)</p></li><li><p>大量数据存放在HDFS，如何快速的对HDFS上的文件进行统计和分析操作。<br>HDFS仅仅只是一个纯文本文件而已，没有schema的概念，没有schema，那么就没办法使用sql进行查询</p></li><li>如何为HDFS上的文件添加Schema信息</li></ol><h2 id="3-Hive-发展历程"><a href="#3-Hive-发展历程" class="headerlink" title="3.Hive 发展历程"></a>3.Hive 发展历程</h2><p>Hive推出至今已经10年了，以下是Hive的重要里程碑需牢记</p><pre><code>07/08 由facebook开源13/05 hive-0.11 stinger Phase 1  ORC/HiveServer213/10 hive-0.12 stinger Phase 2  ORC/improvement14/04 hive-0.13 stinger Phase 2  Tez/Vectorized query engine14/11 hive-0.14 Stinger.next Phase 1 Cost-based optimaizer(CBO)</code></pre><h2 id="4-Hive架构"><a href="#4-Hive架构" class="headerlink" title="4.Hive架构"></a>4.Hive架构</h2><h3 id="Hive的优点："><a href="#Hive的优点：" class="headerlink" title="Hive的优点："></a>Hive的优点：</h3><ul><li>简单易上手</li><li>容易扩展（基于HDFS和YARN）</li><li>统一的元数据<code>metastore</code>管理：</li></ul><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://oss.xiejm.com/Note/hadoop/201709171756.jpg" alt="image" title="">                </div>                <div class="image-caption">image</div>            </figure><h3 id="Hive主要部件"><a href="#Hive主要部件" class="headerlink" title="Hive主要部件"></a>Hive主要部件</h3><ul><li><p>用户接口：CLI（command-line interface，命令行界面）、JDBC/ODBC、Broswer</p></li><li><p>Driver：管理整个SQL作业的生命周期,接受query的组件，该组件实现session的概念，以处理和提供基于JDBC/ODBC执行。</p></li><li><p>SQL Parser：把SQL语句转换成抽象语法数，抽象语法数是不能执行的，先要转换成逻辑计划，逻辑执行计划优化以后生成物理执行计划，物理执行计划优化以后才能变成作业去运行</p></li><li><p>metastore包含：存储数据仓库所有的各种表与分区的结构化信息，包括列与列的信息，序列化器与反序列化，从而能够读写HDFS中的数据</p></li><li>metastore包含：<ul><li>database: name location  owner</li><li>table: name owner location column name/type/index createtime</li></ul></li></ul><p>metastore和Spark、impala等SQL引擎是通用的</p><h2 id="Hive-部署架构"><a href="#Hive-部署架构" class="headerlink" title="Hive 部署架构"></a>Hive 部署架构</h2><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://oss.xiejm.com/Note/hadoop/201709172131.png" alt="image" title="">                </div>                <div class="image-caption">image</div>            </figure><p>上图是Hive的部署架构：<br>Hive 默认元数据存放在derby里面；<br>derby只能单session ；<br><em>在测试环境也建议使用MySQL；</em><br><em>在生产环境中MySQL必须有主备；</em></p><h2 id="Hive与RDBMS的关系"><a href="#Hive与RDBMS的关系" class="headerlink" title="Hive与RDBMS的关系"></a>Hive与RDBMS的关系</h2><p>其实Hive与关系型数据库没有直接关系，只是SQL有点像<br>Hive的事务比较鸡肋<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://wpxiejm.oss-cn-beijing.aliyuncs.com/Note/hadoop/201709172155.png" alt="Hive与传统关系数据库的对比" title="">                </div>                <div class="image-caption">Hive与传统关系数据库的对比</div>            </figure></p><h2 id="使用Hive的好处："><a href="#使用Hive的好处：" class="headerlink" title="使用Hive的好处："></a>使用Hive的好处：</h2><ul><li>90%任务由Hive编写，代码量通常1、2行，开发周期通常很短</li><li>Hive的所有执行，最终都将转化成MapReduce任务</li><li>Hive长处在于数据统计，容量、投放量的计算，group by，join</li><li>更多精力放在数据逻辑上，优化数据结构和性能</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;一-Hive是什么&quot;&gt;&lt;a href=&quot;#一-Hive是什么&quot; class=&quot;headerlink&quot; title=&quot;一.Hive是什么&quot;&gt;&lt;/a&gt;一.Hive是什么&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;Hive是使用一种类似于用SQL的查询语言，直接作用在分布式存储系统之上
      
    
    </summary>
    
      <category term="大数据" scheme="http://xiejm.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hive" scheme="http://xiejm.com/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>MySQL 5.6基于Linux源码编译安装</title>
    <link href="http://xiejm.com/MySQL/MySQL--Source_Install.html"/>
    <id>http://xiejm.com/MySQL/MySQL--Source_Install.html</id>
    <published>2017-10-04T13:43:52.000Z</published>
    <updated>2018-04-17T04:44:18.000Z</updated>
    
    <content type="html"><![CDATA[<p>MySQL可以通过YUM或其他安装包快速安装，也可以下载源代码编译安装。从源代码编译安装MySQL是有好处，比如可以指定编译生产参数、优化编译、指定安装位置等。</p><p>本文的主要内容：</p><ol><li>获取源码</li><li>系统配置、编译和安装</li><li>配置MySQL</li></ol><a id="more"></a><h2 id="1-获取源码"><a href="#1-获取源码" class="headerlink" title="1. 获取源码"></a>1. 获取源码</h2><p>可以在以下页面找到</p><p><a href="http://dev.mysql.com/downloads/mysql/" target="_blank" rel="noopener">http://dev.mysql.com/downloads/mysql/</a></p><p>MySQL的源文件被打包为.tar.gz或.zip格式。在本例中，我下载的源代码文件为’mysql-5.6.23.tar.gz’。</p><p>可以使用tar命令解压文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment">#  tar -zxvf mysql-5.6.23.tar.gz -C /usr/local/</span></span><br><span class="line">[root@hadoop001 ~]<span class="comment"># cd /usr/local/</span></span><br><span class="line">[root@hadoop001 <span class="built_in">local</span>]<span class="comment"># mv mysql-5.6.23-linux-glibc2.5-x86_64 mysql</span></span><br></pre></td></tr></table></figure><h2 id="2-系统配置"><a href="#2-系统配置" class="headerlink" title="2.系统配置"></a>2.系统配置</h2><h3 id="2-1-检查系统是否有MySQL"><a href="#2-1-检查系统是否有MySQL" class="headerlink" title="2.1 检查系统是否有MySQL"></a>2.1 检查系统是否有MySQL</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@Master <span class="built_in">local</span>]<span class="comment"># ps -ef|grep mysqld</span></span><br><span class="line">root      2493  2423  0 19:48 pts/3    00:00:00 grep mysqld</span><br><span class="line">[root@Master <span class="built_in">local</span>]<span class="comment"># rpm -qa |grep -i mysql</span></span><br></pre></td></tr></table></figure><p>如果有系统自带的MySQL卸载之</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@Master <span class="built_in">local</span>]<span class="comment"># rpm -qa |grep mysql</span></span><br><span class="line">mysql-libs-5.1.73-5.el6_6.x86_64</span><br><span class="line">[root@Master <span class="built_in">local</span>]<span class="comment"># rpm -e --nodeps mysql-libs-5.1.73-5.el6_6.x86_64</span></span><br></pre></td></tr></table></figure><h3 id="2-2-创建用户和组"><a href="#2-2-创建用户和组" class="headerlink" title="2.2 创建用户和组"></a>2.2 创建用户和组</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Create group and user</span><br><span class="line">[root@Master <span class="built_in">local</span>]<span class="comment"># groupadd -g 101 dba</span></span><br><span class="line">[root@Master <span class="built_in">local</span>]<span class="comment"># useradd -u 520 -g dba -G root -md /usr/local/mysql mysqladmin</span></span><br><span class="line">[root@Master <span class="built_in">local</span>]<span class="comment"># id mysqladmin</span></span><br><span class="line">uid=520(mysqladmin) gid=101(dba) groups=101(dba),0(root)</span><br><span class="line"></span><br><span class="line"><span class="comment">#设置mysqladmin用户密码</span></span><br><span class="line">[root@Master <span class="built_in">local</span>]<span class="comment"># passwd mysqladmin</span></span><br><span class="line">Changing password <span class="keyword">for</span> user mysqladmin.</span><br><span class="line">New UNIX password:</span><br><span class="line">BAD PASSWORD: it is too simplistic/systematic</span><br><span class="line">Retype new UNIX password:</span><br><span class="line">passwd: all authentication tokens updated successfully.</span><br></pre></td></tr></table></figure><p>复制用户环境变量的初始配置文件至mysqladmin用户的home目录中</p><p>为了解决切换用户显示-bash-4.1$的问题</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@Master <span class="built_in">local</span>]<span class="comment"># cp /etc/skel/.* /usr/local/mysql</span></span><br></pre></td></tr></table></figure><h3 id="2-3-创建MySQL配置文件"><a href="#2-3-创建MySQL配置文件" class="headerlink" title="2.3 创建MySQL配置文件"></a>2.3 创建MySQL配置文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line">[root@Master <span class="built_in">local</span>]<span class="comment"># vim /etc/my.cnf</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#把以下配置文件内容复制到/etc/my.cnf中</span></span><br><span class="line"></span><br><span class="line">[client]</span><br><span class="line">port            = 3306</span><br><span class="line">socket          = /usr/<span class="built_in">local</span>/mysql/data/mysql.sock</span><br><span class="line"></span><br><span class="line">[mysqld]</span><br><span class="line">port            = 3306</span><br><span class="line">socket          = /usr/<span class="built_in">local</span>/mysql/data/mysql.sock</span><br><span class="line">explicit_defaults_for_timestamp=<span class="literal">true</span></span><br><span class="line">skip-external-locking</span><br><span class="line">key_buffer_size = 256M</span><br><span class="line">sort_buffer_size = 2M</span><br><span class="line">read_buffer_size = 2M</span><br><span class="line">read_rnd_buffer_size = 4M</span><br><span class="line">query_cache_size= 32M</span><br><span class="line">max_allowed_packet = 16M</span><br><span class="line">myisam_sort_buffer_size=128M</span><br><span class="line">tmp_table_size=32M</span><br><span class="line"></span><br><span class="line">table_open_cache = 512</span><br><span class="line">thread_cache_size = 8</span><br><span class="line">wait_timeout = 86400</span><br><span class="line">interactive_timeout = 86400</span><br><span class="line">max_connections = 600</span><br><span class="line"></span><br><span class="line"><span class="comment"># Try number of CPU's*2 for thread_concurrency</span></span><br><span class="line"><span class="comment">#thread_concurrency = 32</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#isolation level and default engine </span></span><br><span class="line">default-storage-engine = INNODB</span><br><span class="line">transaction-isolation = READ-COMMITTED</span><br><span class="line"></span><br><span class="line">server-id  = 1</span><br><span class="line">basedir     = /usr/<span class="built_in">local</span>/mysql</span><br><span class="line">datadir     = /usr/<span class="built_in">local</span>/mysql/data</span><br><span class="line">pid-file     = /usr/<span class="built_in">local</span>/mysql/data/hostname.pid</span><br><span class="line"></span><br><span class="line"><span class="comment">#open performance schema</span></span><br><span class="line"><span class="built_in">log</span>-warnings</span><br><span class="line">sysdate-is-now</span><br><span class="line"></span><br><span class="line">binlog_format = MIXED</span><br><span class="line">log_bin_trust_function_creators=1</span><br><span class="line"><span class="built_in">log</span>-error  = /usr/<span class="built_in">local</span>/mysql/data/hostname.err</span><br><span class="line"><span class="built_in">log</span>-bin=/usr/<span class="built_in">local</span>/mysql/arch/mysql-bin</span><br><span class="line"><span class="comment">#other logs</span></span><br><span class="line"><span class="comment">#general_log =1</span></span><br><span class="line"><span class="comment">#general_log_file  = /usr/local/mysql/data/general_log.err</span></span><br><span class="line"><span class="comment">#slow_query_log=1</span></span><br><span class="line"><span class="comment">#slow_query_log_file=/usr/local/mysql/data/slow_log.err</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#for replication slave</span></span><br><span class="line"><span class="comment">#log-slave-updates </span></span><br><span class="line"><span class="comment">#sync_binlog = 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#for innodb options </span></span><br><span class="line">innodb_data_home_dir = /usr/<span class="built_in">local</span>/mysql/data/</span><br><span class="line">innodb_data_file_path = ibdata1:500M:autoextend</span><br><span class="line">innodb_log_group_home_dir = /usr/<span class="built_in">local</span>/mysql/arch</span><br><span class="line">innodb_log_files_in_group = 2</span><br><span class="line">innodb_log_file_size = 200M</span><br><span class="line"></span><br><span class="line">innodb_buffer_pool_size = 2048M</span><br><span class="line">innodb_additional_mem_pool_size = 50M</span><br><span class="line">innodb_log_buffer_size = 16M</span><br><span class="line"></span><br><span class="line">innodb_lock_wait_timeout = 100</span><br><span class="line"><span class="comment">#innodb_thread_concurrency = 0</span></span><br><span class="line">innodb_flush_log_at_trx_commit = 1</span><br><span class="line">innodb_locks_unsafe_for_binlog=1</span><br><span class="line"></span><br><span class="line"><span class="comment">#innodb io features: add for mysql5.5.8</span></span><br><span class="line">performance_schema</span><br><span class="line">innodb_read_io_threads=4</span><br><span class="line">innodb-write-io-threads=4</span><br><span class="line">innodb-io-capacity=200</span><br><span class="line"><span class="comment">#purge threads change default(0) to 1 for purge</span></span><br><span class="line">innodb_purge_threads=1</span><br><span class="line">innodb_use_native_aio=on</span><br><span class="line"></span><br><span class="line"><span class="comment">#case-sensitive file names and separate tablespace</span></span><br><span class="line">innodb_file_per_table = 1</span><br><span class="line">lower_case_table_names=1</span><br><span class="line"></span><br><span class="line">[mysqldump]</span><br><span class="line">quick</span><br><span class="line">max_allowed_packet = 16M</span><br><span class="line"></span><br><span class="line">[mysql]</span><br><span class="line">no-auto-rehash</span><br><span class="line"></span><br><span class="line">[mysqlhotcopy]</span><br><span class="line">interactive-timeout</span><br><span class="line"></span><br><span class="line">[myisamchk]</span><br><span class="line">key_buffer_size = 256M</span><br><span class="line">sort_buffer_size = 256M</span><br><span class="line">read_buffer = 2M</span><br><span class="line">write_buffer = 2M</span><br></pre></td></tr></table></figure><h2 id="2-4-配置权限"><a href="#2-4-配置权限" class="headerlink" title="2.4 配置权限"></a>2.4 配置权限</h2><p>修改my.cnf文件权限</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@Master <span class="built_in">local</span>]<span class="comment"># chown mysqladmin:dba /etc/my.cnf</span></span><br><span class="line">[root@Master <span class="built_in">local</span>]<span class="comment"># chmod 640 !$</span></span><br><span class="line">chmod 640 /etc/my.cnf</span><br><span class="line">[root@Master <span class="built_in">local</span>]<span class="comment"># ll !$</span></span><br><span class="line">ll /etc/my.cnf</span><br><span class="line">-rw-r----- 1 mysqladmin dba 2218 Sep 25 11:30 /etc/my.cnf</span><br></pre></td></tr></table></figure><p>修改mysql目录权限</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 创建日志存放目录</span></span><br><span class="line">[root@Master ~]$ mkdir /usr/<span class="built_in">local</span>/mysql/arch</span><br><span class="line"></span><br><span class="line">[root@Master <span class="built_in">local</span>]<span class="comment"># chown -R mysqladmin:dba /usr/local/mysql/</span></span><br></pre></td></tr></table></figure><h3 id="2-5-安装"><a href="#2-5-安装" class="headerlink" title="2.5 安装"></a>2.5 安装</h3><p>安装glibc</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@Master <span class="built_in">local</span>]<span class="comment"># yum install -y glibc</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@Master ~]$ su - mysqladmin</span><br><span class="line">[mysqladmin@Master ~]$ scripts/mysql_install_db  --user=mysqladmin --basedir=/usr/<span class="built_in">local</span>/mysql --datadir=/usr/<span class="built_in">local</span>/mysql/data </span><br><span class="line"></span><br><span class="line">WARNING: The host <span class="string">'Master'</span> could not be looked up with /usr/<span class="built_in">local</span>/mysql/bin/resolveip.</span><br><span class="line">This probably means that your libc libraries are not 100 % compatible</span><br><span class="line">with this binary MySQL version. The MySQL daemon, mysqld, should work</span><br><span class="line">normally with the exception that host name resolving will not work.</span><br><span class="line">This means that you should use IP addresses instead of hostnames</span><br><span class="line">when specifying MySQL privileges !</span><br><span class="line"></span><br><span class="line">Installing MySQL system tables...OK</span><br><span class="line"></span><br><span class="line">Filling <span class="built_in">help</span> tables...OK</span><br></pre></td></tr></table></figure><p>退出到root用户操作</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[mysqladmin@Master ~]$ <span class="built_in">exit</span></span><br><span class="line"><span class="built_in">logout</span></span><br></pre></td></tr></table></figure><h3 id="2-6-配置开机启动"><a href="#2-6-配置开机启动" class="headerlink" title="2.6 配置开机启动"></a>2.6 配置开机启动</h3><p>将服务文件拷贝到init.d下，并重命名为mysql</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@Master mysql]<span class="comment"># cp support-files/mysql.server /etc/rc.d/init.d/mysql</span></span><br></pre></td></tr></table></figure><p>修改启动文件的执行权限</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@Master mysql]<span class="comment"># chmod +x /etc/rc.d/init.d/mysql</span></span><br></pre></td></tr></table></figure><p>添加MySQL服务启动级别</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@Master mysql]<span class="comment"># chkconfig --del mysql</span></span><br><span class="line">[root@Master mysql]<span class="comment"># chkconfig --add mysql</span></span><br><span class="line">[root@Master mysql]<span class="comment"># chkconfig |grep mysql</span></span><br><span class="line">mysql          0:off1:off2:on3:on4:on5:on6:off</span><br></pre></td></tr></table></figure><h3 id="2-7-启动MySQL"><a href="#2-7-启动MySQL" class="headerlink" title="2.7 启动MySQL"></a>2.7 启动MySQL</h3><p>首先要切换到mysqladmin用户下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@Master mysql]<span class="comment"># su - mysqladmin</span></span><br><span class="line">[mysqladmin@Master ~]$ <span class="built_in">pwd</span></span><br><span class="line">/usr/<span class="built_in">local</span>/mysql</span><br><span class="line">[mysqladmin@Master ~]$ mv my.cnf my.cnf.bak</span><br></pre></td></tr></table></figure><p>执行启动脚本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mysqladmin@Master ~]$ bin/mysqld_safe &amp;</span><br><span class="line">[1] 8966</span><br><span class="line">[mysqladmin@Master ~]$ 170925 16:33:51 mysqld_safe Logging to <span class="string">'/usr/local/mysql/data/hostname.err'</span>.</span><br><span class="line">170925 16:33:51 mysqld_safe Starting mysqld daemon with databases from /usr/<span class="built_in">local</span>/mysql/data</span><br><span class="line"></span><br><span class="line">[mysqladmin@Master ~]$</span><br></pre></td></tr></table></figure><p>验证启动进程</p><p>先切换到root用户下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@Master ~]<span class="comment"># ps -ef|grep mysqld</span></span><br><span class="line">514        4152   4122  0 13:51 pts/0    00:00:00 /bin/sh bin/mysqld_safe</span><br><span class="line">514        4795   4152  1 13:51 pts/0    00:00:03 /usr/<span class="built_in">local</span>/mysql/bin/mysqld --basedir=/usr/<span class="built_in">local</span>/mysql --datadir=/usr/<span class="built_in">local</span>/mysql/data --plugin-dir=/usr/<span class="built_in">local</span>/mysql/lib/plugin --<span class="built_in">log</span>-error=/usr/<span class="built_in">local</span>/mysql/data/hostname.err --pid-file=/usr/<span class="built_in">local</span>/mysql/data/hostname.pid --socket=/usr/<span class="built_in">local</span>/mysql/data/mysql.sock --port=3306</span><br><span class="line">root       4864   4819  0 13:55 pts/1    00:00:00 grep mysqld</span><br></pre></td></tr></table></figure><p>验证3306端口是否被监听</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@Master ~]<span class="comment"># netstat -tulnp | grep mysql</span></span><br><span class="line">tcp        0      0 :::3306                     :::*                        LISTEN      4795/mysqld</span><br></pre></td></tr></table></figure><p>查看mysql服务状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@Master ~]<span class="comment"># service mysql status</span></span><br><span class="line"> SUCCESS! MySQL running (4795)</span><br></pre></td></tr></table></figure><p>至此MySQL就安装成功了。yes! give me five!~~~ haha</p><h2 id="3-配置MySQL"><a href="#3-配置MySQL" class="headerlink" title="3.配置MySQL"></a>3.配置MySQL</h2><h3 id="3-1-登录"><a href="#3-1-登录" class="headerlink" title="3.1 登录"></a>3.1 登录</h3><p>使用mysql命令登录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">[mysqladmin@Master ~]$ mysql</span><br><span class="line">Welcome to the MySQL monitor.  Commands end with ; or \g.</span><br><span class="line">Your MySQL connection id is 1</span><br><span class="line">Server version: 5.6.23-log MySQL Community Server (GPL)</span><br><span class="line"></span><br><span class="line">Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.</span><br><span class="line"></span><br><span class="line">Oracle is a registered trademark of Oracle Corporation and/or its</span><br><span class="line">affiliates. Other names may be trademarks of their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line">Type <span class="string">'help;'</span> or <span class="string">'\h'</span> <span class="keyword">for</span> <span class="built_in">help</span>. Type <span class="string">'\c'</span> to clear the current input statement.</span><br><span class="line"></span><br><span class="line">mysql&gt; use mysql;</span><br><span class="line">Database changed</span><br><span class="line">mysql&gt; update user <span class="built_in">set</span> password=password(<span class="string">'123456'</span>) <span class="built_in">where</span> user=<span class="string">'root'</span>;</span><br><span class="line">Query OK, 4 rows affected (0.00 sec)</span><br><span class="line">Rows matched: 4  Changed: 4  Warnings: 0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mysql&gt; delete from user <span class="built_in">where</span> user=<span class="string">''</span>;</span><br><span class="line">Query OK, 2 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; flush privileges;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mysql&gt; select user,host from mysql.user;</span><br><span class="line">+------+-----------+</span><br><span class="line">| user | host      |</span><br><span class="line">+------+-----------+</span><br><span class="line">| root | 127.0.0.1 |</span><br><span class="line">| root | ::1       |</span><br><span class="line">| root | localhost |</span><br><span class="line">| root | master    |</span><br><span class="line">+------+-----------+</span><br><span class="line">4 rows <span class="keyword">in</span> <span class="built_in">set</span> (0.00 sec)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;MySQL可以通过YUM或其他安装包快速安装，也可以下载源代码编译安装。从源代码编译安装MySQL是有好处，比如可以指定编译生产参数、优化编译、指定安装位置等。&lt;/p&gt;
&lt;p&gt;本文的主要内容：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;获取源码&lt;/li&gt;
&lt;li&gt;系统配置、编译和安装&lt;/li&gt;
&lt;li&gt;配置MySQL&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="数据库" scheme="http://xiejm.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="MySQL" scheme="http://xiejm.com/tags/MySQL/"/>
    
  </entry>
  
  <entry>
    <title>Hive的几种数据导入方式</title>
    <link href="http://xiejm.com/Hive/Hive--load_data.html"/>
    <id>http://xiejm.com/Hive/Hive--load_data.html</id>
    <published>2017-10-03T14:49:38.000Z</published>
    <updated>2018-04-17T04:45:44.000Z</updated>
    
    <content type="html"><![CDATA[<p>在Hive中建好表之后，需要记载数据用于后续查询分析操作。<br>本文介绍向Hive表中加载数据的几种方式。<br><a id="more"></a></p><h2 id="建表时候直接指定"><a href="#建表时候直接指定" class="headerlink" title="建表时候直接指定"></a>建表时候直接指定</h2><p>如果数据已经存在HDFS中，那么可以直接在建表的时候使用<code>location</code>来指定数据所在HDFS路径。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> [<span class="keyword">EXTERNAL</span>] <span class="keyword">TABLE</span> emp (</span><br><span class="line"><span class="keyword">day</span> <span class="keyword">STRING</span>,</span><br><span class="line"><span class="keyword">url</span> <span class="keyword">STRING</span>)</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span></span><br><span class="line"><span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\t'</span></span><br><span class="line">location <span class="string">'/tmp/emp/</span></span><br></pre></td></tr></table></figure><h2 id="从本地文件系统导入数据到Hive表中"><a href="#从本地文件系统导入数据到Hive表中" class="headerlink" title="从本地文件系统导入数据到Hive表中"></a>从本地文件系统导入数据到Hive表中</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">"/path/to/dir"</span> <span class="keyword">into</span> <span class="keyword">table</span> tab_name [<span class="keyword">PARTITION</span>]</span><br></pre></td></tr></table></figure><h2 id="从HDFS上导入数据到Hive表中"><a href="#从HDFS上导入数据到Hive表中" class="headerlink" title="从HDFS上导入数据到Hive表中"></a>从HDFS上导入数据到Hive表中</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> inpath <span class="string">"/path/to/hdfs_dir"</span> <span class="keyword">into</span> <span class="keyword">table</span> tab_name [<span class="keyword">PARTITION</span>]</span><br></pre></td></tr></table></figure><h2 id="从别的表中查询数据到Hive表中"><a href="#从别的表中查询数据到Hive表中" class="headerlink" title="从别的表中查询数据到Hive表中"></a>从别的表中查询数据到Hive表中</h2><h3 id="基本模式"><a href="#基本模式" class="headerlink" title="基本模式"></a>基本模式</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> tab_name [<span class="keyword">PARTITION</span> ] <span class="keyword">select</span> ... <span class="keyword">from</span> tab_name ;</span><br></pre></td></tr></table></figure><h3 id="多插入模式"><a href="#多插入模式" class="headerlink" title="多插入模式"></a>多插入模式</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from tab_name</span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> tab_name1 <span class="keyword">select</span> ....</span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> tab_name2 select...</span><br></pre></td></tr></table></figure><p><strong>Hive不支持用Insert语句一条一条地进行插入操作，也不支持UPDATE操作</strong></p><h2 id="在创建表的时候从别的表中查询记录并插入相应的表中"><a href="#在创建表的时候从别的表中查询记录并插入相应的表中" class="headerlink" title="在创建表的时候从别的表中查询记录并插入相应的表中"></a>在创建表的时候从别的表中查询记录并插入相应的表中</h2><p>在实际情况下，表的输出结果可能很多，不适于显示在控制台上，这时候，将Hive的查询输出结果直接保存在一个新的表中是合适的，我们称这种情况是<strong>CTAS</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> tabe_name <span class="keyword">as</span> <span class="keyword">select</span> ......</span><br></pre></td></tr></table></figure><p><strong>注意：</strong> CTAS是原子的，如果select查询失败，则新表不会创建</p><h2 id="Hive导出数据到文件系统"><a href="#Hive导出数据到文件系统" class="headerlink" title="Hive导出数据到文件系统"></a>Hive导出数据到文件系统</h2><p>顺带也介绍下从Hive导出数据到HDFS和本地文件系统</p><p>先来看下语法：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE [<span class="keyword">LOCAL</span>] <span class="keyword">DIRECTORY</span> directory1</span><br><span class="line">  [<span class="keyword">ROW</span> <span class="keyword">FORMAT</span> row_format] [<span class="keyword">STORED</span> <span class="keyword">AS</span> file_format]</span><br><span class="line">  <span class="keyword">SELECT</span> ... <span class="keyword">FROM</span> ...</span><br></pre></td></tr></table></figure><p>在上面的语法中，如果指定了LOCAL关键字，则为导出到本地文件系统，否则导出到HDFS中。<br>使用ROW FORMAT关键字可以指定导出文件分隔符。</p><p>再来看下示例：</p><p>将emp表的所有数据导出到本地文件系统<code>/tmp/emp/</code>目录，字段间分割符为tab <code>\t</code></p><p>INSERT OVERWRITE LOCAL DIRECTORY ‘/tmp/emp/‘<br>  ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’<br>SELECT * FROM emp;</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在Hive中建好表之后，需要记载数据用于后续查询分析操作。&lt;br&gt;本文介绍向Hive表中加载数据的几种方式。&lt;br&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://xiejm.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hive" scheme="http://xiejm.com/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive的数据类型和分隔符</title>
    <link href="http://xiejm.com/Hive/Hive--DataType.html"/>
    <id>http://xiejm.com/Hive/Hive--DataType.html</id>
    <published>2017-10-03T13:37:16.000Z</published>
    <updated>2018-04-17T04:45:47.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文记录了Hive的数据类型和分隔符的最佳实践<br><a id="more"></a></p><h2 id="基本数据类型"><a href="#基本数据类型" class="headerlink" title="基本数据类型"></a>基本数据类型</h2><p>Hive的数据类型在官网用户手册中有详细介绍：<br><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types</a></p><p>这里列举一些常用的数据类型，以下按使用优先级排序</p><p>string &gt; int &gt; bigint &gt; float &gt; double &gt; boolean &gt; date &gt; timestamp</p><p>【<strong>最佳实践：</strong>】实际使用中建议都使用string类型，如果无法使用string类型再按上面的次序选择合适的数据类型</p><h2 id="分隔符"><a href="#分隔符" class="headerlink" title="分隔符"></a>分隔符</h2><p>列与列之间的分隔符是换行 <code>\n</code></p><p>Hive默认的字段分隔符是<code>^A</code> ,<code>\001</code></p><p>【<strong>最佳实践：</strong>】一般情况下，在创建表的时候直接指定分隔符：<code>\t</code>或者英文逗号<code>,</code></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文记录了Hive的数据类型和分隔符的最佳实践&lt;br&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://xiejm.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hive" scheme="http://xiejm.com/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop--集群(Fully Distributed Mode)模式部署+HA</title>
    <link href="http://xiejm.com/Hadoop/Hadoop--Fully_Distributed%20_Mode_HA.html"/>
    <id>http://xiejm.com/Hadoop/Hadoop--Fully_Distributed _Mode_HA.html</id>
    <published>2017-10-03T13:06:38.000Z</published>
    <updated>2018-04-17T04:45:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要记录Hadoop全分布模式（Fully Distributed Mode）部署的详细步骤：</p><p>Hadoop全分布模式（Fully Distributed Mode）是一种集群部署方式，它的守护进程运行在一个集群上。</p><a id="more"></a><ul><li>我们一般会在master节点上部署<code>NameNode</code>,<code>ResourceManager</code>；</li><li><code>SecondaryNameNode</code>可以安装在master节点，也可以单独安装。</li><li>slave节点能看到<code>DataNode</code>和<code>NodeManager</code>。</li></ul><h2 id="1-基本环境"><a href="#1-基本环境" class="headerlink" title="1.基本环境"></a>1.基本环境</h2><ul><li>CentOS 6.7</li><li>JDK1.7+</li><li>Hadoop 2.8.1</li><li>ZooKeeper 3.4.6</li></ul><blockquote><p>系统原始环境特别说明：<br>系统选择minimal最小系统安装，并自定义选装以下包：<br>Base System中的Base、Compatbillty libraries、Debugging Tools; Development中的 Development tools</p></blockquote><h3 id="1-1组件说明："><a href="#1-1组件说明：" class="headerlink" title="1.1组件说明："></a>1.1组件说明：</h3><table><thead><tr><th>组件</th><th>备注说明</th></tr></thead><tbody><tr><td>CentOS</td><td>使用下列命令查看系统版本和系统位数<br>lsb_release -a #系统版本<br>file /bin/ls #系统位数</td></tr><tr><td>JDK</td><td>本文采用JDK1.8.45</td></tr><tr><td>Hadoop</td><td>使用官网最新release版本2.8.1</td></tr><tr><td>ZooKeeper</td><td>用于HA热切,存储数据使用的协调服务<br><a href="http://mirrors.hust.edu.cn/apache/zookeeper/zookeeper-3.4.6/zookeeper-3.4.6.tar.gz" target="_blank" rel="noopener">下载地址：</a></td></tr></tbody></table><h2 id="2-主机节点规划"><a href="#2-主机节点规划" class="headerlink" title="2.主机节点规划"></a>2.主机节点规划</h2><table><thead><tr><th>HostName</th><th>IP Address</th><th>Software</th><th>Course</th></tr></thead><tbody><tr><td>hadoop001</td><td>192.168.194.111</td><td>Hadoop、ZooKeeper</td><td>NameNode<br>DFSZKFailoverController <br>ResourceManager<br>JobHistoryServer<br>QuorumPeerMain</td></tr><tr><td>hadoop002</td><td>192.168.194.112</td><td>Hadoop、ZooKeeper</td><td>NameNode<br>DFSZKFailoverController <br>ResourceManager<br>QuorumPeerMain</td></tr><tr><td>hadoop003</td><td>192.168.194.113</td><td>Hadoop、ZooKeeper</td><td>DataNode<br> NodeManager<br>JournalNode<br>QuorumPeerMain</td></tr><tr><td>hadoop004</td><td>192.168.194.114</td><td>Hadoop、ZooKeeper</td><td>DataNode<br> NodeManager<br>JournalNode<br>QuorumPeerMain</td></tr><tr><td>hadoop005</td><td>192.168.194.115</td><td>Hadoop、ZooKeeper</td><td>DataNode<br> NodeManager<br>JournalNode<br>QuorumPeerMain</td></tr></tbody></table><h2 id="3-目录规划"><a href="#3-目录规划" class="headerlink" title="3.目录规划"></a>3.目录规划</h2><table><thead><tr><th>名称</th><th>路径</th><th>备注</th></tr></thead><tbody><tr><td>$HADOOP_HOME</td><td>/opt/software/hadoop</td><td></td></tr><tr><td>Data</td><td>$HADOOP_HOME/data</td><td></td></tr><tr><td>Log</td><td>$HADOOP_HOME/logs</td><td></td></tr><tr><td>hadoop.tmp.dir</td><td>$HADOOP_HOME/tmp</td><td>需要手工创建,权限777,root:root</td><td></td></tr><tr><td>$ZOOKEEPER_HOME</td><td>/opt/software/zookeeper</td><td></td></tr></tbody></table><h2 id="4-系统环境配置"><a href="#4-系统环境配置" class="headerlink" title="4.系统环境配置"></a>4.系统环境配置</h2><p>本节内容所有节点都要配置，以下操作用hadoop001节点做示例</p><h3 id="4-1-配置网络和主机名"><a href="#4-1-配置网络和主机名" class="headerlink" title="4.1 配置网络和主机名"></a>4.1 配置网络和主机名</h3><h4 id="网络IP设置"><a href="#网络IP设置" class="headerlink" title="网络IP设置"></a>网络IP设置</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]# vi /etc/sysconfig/network-scripts/ifcfg-eth0</span><br><span class="line"><span class="meta">#</span>按以下格式修改，没有的字段需要自行添加</span><br><span class="line">DEVICE=eth0                                 #网卡名称</span><br><span class="line">HWADDR=00:0C:29:8F:61:EB                    #MAC地址</span><br><span class="line">TYPE=Ethernet                               #网卡类型</span><br><span class="line">UUID=039afc77-da3e-4e17-bcaa-062148464812   #网卡ID</span><br><span class="line">ONBOOT=yes                                  #是否开机启动</span><br><span class="line">NM_CONTROLLED=yes                           #Network Manager</span><br><span class="line">BOOTPROTO=static                            #静态IP模式</span><br><span class="line">IPADDR=192.168.194.111                      #IP地址</span><br><span class="line">NETMASK=255.255.255.0                       #子网掩码</span><br><span class="line">GATEWAY=192.168.194.2                       #网关地址</span><br><span class="line">DNS1=223.5.5.5                              #DNS</span><br></pre></td></tr></table></figure><p>保存退出后，重启网卡</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]# service network restart</span><br></pre></td></tr></table></figure><h4 id="主机名配置"><a href="#主机名配置" class="headerlink" title="主机名配置"></a>主机名配置</h4><p>如果在安装系统的时候未进行主机名设置，那么请根据以下命令对主机名进行配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]# vi /etc/sysconfig/network</span><br><span class="line"><span class="meta">#</span>修改下列信息，其中HOSTNAME项修改成我们需要的主机名</span><br><span class="line">NETWORKING=yes #启动网络</span><br><span class="line">HOSTNAME=hadoop001 #主机名</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>修改完成后保存并退出，使用下列命令更新当前bash的主机名</span><br><span class="line">[root@hadoop001 ~]# hostname hadoop001</span><br></pre></td></tr></table></figure><h3 id="4-2-添加集群用户"><a href="#4-2-添加集群用户" class="headerlink" title="4.2 添加集群用户"></a>4.2 添加集群用户</h3><p>增加ID号为1000的hadoopGroup,增加ID为2000的用户并加入hadoopGroup组</p><p>固定用户和组的ID号是为了用户和组的权限一致，我遇到过一次在两台机器上用户名和组名相同，ID号不同结果导致权限有问题</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># groupadd -g 1000 hadoopGroup</span></span><br><span class="line">[root@hadoop001 ~]<span class="comment"># useradd -u 2000 -g hadoopGroup hadoop</span></span><br></pre></td></tr></table></figure><p>设置hadoop用户的密码为hadoop，下面的这种方法免去了修改密码时的密码确认，在生产环境中使用此方式改密码需要清空history</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># echo "hadoop"|passwd --stdin hadoop</span></span><br></pre></td></tr></table></figure><h3 id="4-3-关闭SELINUX"><a href="#4-3-关闭SELINUX" class="headerlink" title="4.3 关闭SELINUX"></a>4.3 关闭SELINUX</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># sed -i 's#SELINUX=enforcing#SELINUX=disabled#g' /etc/selinux/config</span></span><br><span class="line"><span class="comment">#关闭当前的SELINUX状态,如果不用setenforce那么需要重启系统</span></span><br><span class="line">[root@hadoop001 ~]<span class="comment"># setenforce 0</span></span><br><span class="line">[root@hadoop001 ~]<span class="comment"># grep SELINUX=disabled /etc/selinux/config</span></span><br><span class="line">SELINUX=disabled</span><br></pre></td></tr></table></figure><h3 id="4-4-关闭防火墙"><a href="#4-4-关闭防火墙" class="headerlink" title="4.4 关闭防火墙"></a>4.4 关闭防火墙</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># service iptables stop</span></span><br><span class="line">iptables: Setting chains to policy ACCEPT: filter          [  OK  ]</span><br><span class="line">iptables: Flushing firewall rules:                         [  OK  ]</span><br><span class="line">iptables: Unloading modules:                               [  OK  ]</span><br><span class="line">[root@hadoop001 ~]<span class="comment"># chkconfig iptables off</span></span><br></pre></td></tr></table></figure><h3 id="4-5-优化开机自启动服务-可选"><a href="#4-5-优化开机自启动服务-可选" class="headerlink" title="4.5 优化开机自启动服务(可选)"></a>4.5 优化开机自启动服务(可选)</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># chkconfig --list|grep 3:on |wc -l</span></span><br><span class="line">[root@hadoop001 ~]<span class="comment"># chkconfig --list|grep 3:on |cut -d "" -f1</span></span><br><span class="line"><span class="comment">#关闭所有3级别的服务</span></span><br><span class="line">[root@hadoop001 ~]<span class="comment"># for name in `chkconfig --list|grep 3:on | cut -d " " -f1` ;do chkconfig $name off;done</span></span><br><span class="line"><span class="comment">#开启需要在3级别启动的服务，这里可以根据需要增加和修改</span></span><br><span class="line">[root@hadoop001 ~]<span class="comment"># for name in crond ntpd network rsyslog sshd;do chkconfig $name on;done</span></span><br><span class="line">[root@hadoop001 ~]<span class="comment"># chkconfig --list|grep 3:on</span></span><br></pre></td></tr></table></figure><h3 id="4-6-修改文件描述符"><a href="#4-6-修改文件描述符" class="headerlink" title="4.6 修改文件描述符"></a>4.6 修改文件描述符</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#修改系统limits.conf</span></span><br><span class="line">[root@hadoop001 ~]<span class="comment"># cat &gt;&gt; /etc/security/limits.conf &lt;&lt; EOF</span></span><br><span class="line">*           soft    nofile      262144</span><br><span class="line">*           hard    nofile      262144</span><br><span class="line">*           soft    nproc       262144</span><br><span class="line">*           hard    nproc       262144</span><br><span class="line">EOF</span><br><span class="line"><span class="comment">#查看系统ulimit文件句柄数</span></span><br><span class="line">[root@hadoop001 ~]<span class="comment"># ulimit -n</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#修改90-nproc.conf</span></span><br><span class="line">[root@hadoop001 ~]<span class="comment"># vi /etc/security/limits.d/90-nproc.conf</span></span><br><span class="line">*  soft nproc 262144</span><br></pre></td></tr></table></figure><h3 id="4-7-增加集群用户sudo权限"><a href="#4-7-增加集群用户sudo权限" class="headerlink" title="4.7 增加集群用户sudo权限"></a>4.7 增加集群用户sudo权限</h3><p>增加hadoop用户的sudo权限</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># vi /etc/sudoers</span></span><br><span class="line">hadoop ALL=(ALL)       ALL</span><br></pre></td></tr></table></figure><h3 id="4-8-配置主机IP映射"><a href="#4-8-配置主机IP映射" class="headerlink" title="4.8 配置主机IP映射"></a>4.8 配置主机IP映射</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># vi /etc/hosts</span></span><br><span class="line"><span class="comment">#在hosts文件末尾追加</span></span><br><span class="line"></span><br><span class="line">192.168.194.111    hadoop001</span><br><span class="line">192.168.194.112    hadoop002</span><br><span class="line">192.168.194.113    hadoop003</span><br><span class="line">192.168.194.114    hadoop004</span><br><span class="line">192.168.194.115    hadoop005</span><br></pre></td></tr></table></figure><h3 id="4-9-配置SSH免密登录"><a href="#4-9-配置SSH免密登录" class="headerlink" title="4.9 配置SSH免密登录"></a>4.9 配置SSH免密登录</h3><p>SSH免密码登录配置的详细配置步骤：<a href="http://blog.xiejm.com/41.html" target="_blank" rel="noopener">SSH免密登录</a></p><h3 id="4-10-配置NTP时钟同步"><a href="#4-10-配置NTP时钟同步" class="headerlink" title="4.10 配置NTP时钟同步"></a>4.10 配置NTP时钟同步</h3><ul><li>第一种方法 ：（所有节点都要操作）都从公共NTP服务器同步，执行如下：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime</span><br><span class="line">$ ntpdate us.pool.ntp.org</span><br><span class="line">$ crontab -e</span><br><span class="line">0-59/10 * * * * /usr/sbin/ntpdate us.pool.ntp.org | logger -t NTP</span><br></pre></td></tr></table></figure><p>以上crontab的配置说明详见 <a href="http://note.youdao.com/" target="_blank" rel="noopener">Linux-crontab命令</a></p><ul><li>第二种方法：选一个节点搭建一个NTP服务，其他节点从该NTP服务器同步，<a href="http://note.youdao.com/" target="_blank" rel="noopener">具体步骤参考本博客的NTP 配置</a></li></ul><h3 id="4-11-关闭交换分区"><a href="#4-11-关闭交换分区" class="headerlink" title="4.11 关闭交换分区"></a>4.11 关闭交换分区</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># echo '0' &gt; /proc/sys/vm/swappiness</span></span><br><span class="line">[root@hadoop001 ~]<span class="comment"># sysctl vm.swappiness=0</span></span><br><span class="line">[root@hadoop001 ~]<span class="comment"># echo 'vm.swappiness=0'&gt;&gt; /etc/sysctl.conf</span></span><br></pre></td></tr></table></figure><h3 id="4-12-禁用透明大页"><a href="#4-12-禁用透明大页" class="headerlink" title="4.12 禁用透明大页"></a>4.12 禁用透明大页</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># echo never &gt; /sys/kernel/mm/redhat_transparent_hugepage/defrag</span></span><br><span class="line">[root@hadoop001 ~]<span class="comment"># cat /sys/kernel/mm/redhat_transparent_hugepage/defrag</span></span><br><span class="line">always [never]</span><br><span class="line">[root@hadoop001 ~]<span class="comment"># echo 'echo never &gt; /sys/kernel/mm/redhat_transparent_hugepage/defrag ' &gt;&gt; /etc/rc.local</span></span><br></pre></td></tr></table></figure><h2 id="5-JDK安装"><a href="#5-JDK安装" class="headerlink" title="5.JDK安装"></a>5.JDK安装</h2><p><strong>【注意】所有节点配置</strong><br>详细安装步骤参考本博客的JDK安装的文章:<a href="http://blog.xiejm.com/8.html" target="_blank" rel="noopener">JDK安装步骤</a></p><h2 id="6-配置环境变量"><a href="#6-配置环境变量" class="headerlink" title="6.配置环境变量"></a>6.配置环境变量</h2><ul><li><strong>【注意】所有节点配置</strong></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 software]<span class="comment"># vim /etc/profile</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#按组合键shift+g 或大写G 到最后一行，按字母o  插入下列内容</span></span><br><span class="line">    <span class="comment">#SET HADOOP</span></span><br><span class="line">    exprot HADOOP_HOME=/opt/software/hadoop</span><br><span class="line"></span><br><span class="line">    <span class="comment">#SET ZOOKEEPER</span></span><br><span class="line">    <span class="built_in">export</span> ZOOKEEPER_HOME=/opt/software/zookeeper</span><br><span class="line"><span class="comment">#如果JDK环境变量没有配置，请加入下列内容</span></span><br><span class="line">    <span class="comment">#SET JDK</span></span><br><span class="line">    <span class="built_in">export</span> JAVA_HOME=/usr/java/jdk1.7.0_79</span><br><span class="line">    <span class="built_in">export</span> JRE_HOME=<span class="variable">$JAVA_HOME</span>/jre</span><br><span class="line">    <span class="built_in">export</span> CLASS_PATH=.:<span class="variable">$JAVA_HOME</span>/lib:<span class="variable">$JAVA_HOME</span>/lib/tools.jar</span><br><span class="line"></span><br><span class="line"><span class="comment">#设置PATH</span></span><br><span class="line">    <span class="built_in">export</span> PATH=<span class="variable">$ZOOKEEPER_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/sbin:<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$JRE_HOME</span>/bin:<span class="variable">$PATH</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#添加完成后  :wq 保存</span></span><br><span class="line"><span class="comment">#使修改生效：</span></span><br><span class="line">[root@hadoop001 software]<span class="comment">#source !$</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#验证JDK有效性</span></span><br><span class="line">[root@hadoop001 software]<span class="comment"># java -version</span></span><br><span class="line">java version <span class="string">"1.7.0_79"</span></span><br><span class="line">Java(TM) SE Runtime Environment (build 1.7.0_79-b15)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 24.79-b02, mixed mode)</span><br></pre></td></tr></table></figure><h2 id="7-安装配置ZooKeeper"><a href="#7-安装配置ZooKeeper" class="headerlink" title="7.安装配置ZooKeeper"></a>7.安装配置ZooKeeper</h2><h3 id="7-1-下载ZooKeeper"><a href="#7-1-下载ZooKeeper" class="headerlink" title="7.1 下载ZooKeeper"></a>7.1 下载ZooKeeper</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 software]<span class="comment"># pwd</span></span><br><span class="line">/opt/software</span><br><span class="line">[root@hadoop001 software]<span class="comment"># wget http://mirrors.hust.edu.cn/apache/zookeeper/zookeeper-3.4.6/zookeeper-3.4.6.tar.gz</span></span><br><span class="line">[root@hadoop001 software]<span class="comment"># tar -zxvf zookeeper-3.4.6.tar.gz</span></span><br><span class="line">[root@hadoop001 software]<span class="comment"># mv zookeeper-3.4.6 zookeeper</span></span><br><span class="line">[root@hadoop001 software]<span class="comment"># chown -R hadoop:hadoopGroup zookeeper</span></span><br></pre></td></tr></table></figure><h3 id="7-2-修改配置"><a href="#7-2-修改配置" class="headerlink" title="7.2 修改配置"></a>7.2 修改配置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 software]<span class="comment"># cd /opt/software/zookeeper/conf</span></span><br><span class="line">[root@hadoop001 conf]<span class="comment"># cp zoo_sample.cfg zoo.cfg</span></span><br><span class="line"></span><br><span class="line">[root@hadoop001 conf]<span class="comment"># vi zoo.cfg</span></span><br><span class="line">修改dataDir</span><br><span class="line">dataDir=/opt/software/zookeeper/data</span><br><span class="line">添加下面三行</span><br><span class="line">server.1=hadoop001:2888:3888</span><br><span class="line">server.2=hadoop002:2888:3888</span><br><span class="line">server.3=hadoop003:2888:3888</span><br><span class="line">server.2=hadoop004:2888:3888</span><br><span class="line">server.3=hadoop005:2888:3888</span><br><span class="line"></span><br><span class="line">[root@hadoop001 conf]<span class="comment"># cd ../</span></span><br><span class="line">[root@hadoop001 zookeeper]<span class="comment"># mkdir data</span></span><br><span class="line">[root@hadoop001 zookeeper]<span class="comment"># touch data/myid</span></span><br><span class="line">[root@hadoop001 zookeeper]<span class="comment"># echo 1 &gt; data/myid</span></span><br><span class="line">[root@hadoop001 zookeeper]<span class="comment"># cat data/myid</span></span><br><span class="line">1</span><br></pre></td></tr></table></figure><h3 id="7-3-文件分发"><a href="#7-3-文件分发" class="headerlink" title="7.3 文件分发"></a>7.3 文件分发</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 zookeeper]<span class="comment"># scp -r /opt/software/zookeeper root@hadoop002:/opt/software</span></span><br><span class="line">[root@hadoop001 zookeeper]<span class="comment"># scp -r /opt/software/zookeeper root@hadoop003:/opt/software</span></span><br><span class="line">[root@hadoop001 zookeeper]<span class="comment"># scp -r /opt/software/zookeeper root@hadoop004:/opt/software</span></span><br><span class="line">[root@hadoop001 zookeeper]<span class="comment"># scp -r /opt/software/zookeeper root@hadoop005:/opt/software</span></span><br></pre></td></tr></table></figure><h3 id="7-4-其他节点myid修改"><a href="#7-4-其他节点myid修改" class="headerlink" title="7.4 其他节点myid修改"></a>7.4 其他节点myid修改</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#不同zk节点配置不同的myid</span></span><br><span class="line"><span class="comment">#myid中的数值需要和 zoo.cfg中的配置一致。</span></span><br><span class="line"><span class="comment"># hadoop002/003/004/005,也修改配置,就如下不同</span></span><br><span class="line">[root@hadoop002 zookeeper]<span class="comment"># echo 2 &gt; data/myid</span></span><br><span class="line">[root@hadoop003 zookeeper]<span class="comment"># echo 3 &gt; data/myid</span></span><br><span class="line">[root@hadoop004 zookeeper]<span class="comment"># echo 4 &gt; data/myid</span></span><br><span class="line">[root@hadoop005 zookeeper]<span class="comment"># echo 5 &gt; data/myid</span></span><br><span class="line"><span class="comment">####注意！！！不能把 echo 3&gt;data/myid,将&gt;前后空格保留,否则无法将 3 写入myid文件</span></span><br></pre></td></tr></table></figure><h2 id="8-安装Hadoop"><a href="#8-安装Hadoop" class="headerlink" title="8.安装Hadoop"></a>8.安装Hadoop</h2><h3 id="8-1-解压"><a href="#8-1-解压" class="headerlink" title="8.1 解压"></a>8.1 解压</h3><p>之前编译好了hadoop-2.8.1-snappy.tar.gz，上传这个包并解压。<br>具体编译过程，参考本博客<a href="http://note.youdao.com/" target="_blank" rel="noopener"></a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># mkdir -p /opt/software</span></span><br><span class="line"><span class="comment">#将hadoop安装包放到/opt/software</span></span><br><span class="line">[root@hadoop001 ~]<span class="comment"># cd /opt/software</span></span><br><span class="line">[root@hadoop001 software]<span class="comment"># tar -zxvf hadoop-2.8.1.tar.gz</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#设置软连接</span></span><br><span class="line">[root@hadoop001 software]<span class="comment"># ln -s /opt/software/hadoop-2.8.1 /opt/software/hadoop</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 当我们使用hadoop用户才需要下面命令</span></span><br><span class="line"><span class="comment">#设置hadoop目录所有者、所属组</span></span><br><span class="line"><span class="comment">#[root@hadoop001 software]# chown -R hadoop:hadoopGroup hadoop/*</span></span><br><span class="line"><span class="comment">#[root@hadoop001 software]# chown -R hadoop:hadoopGroup hadoop-2.8.1</span></span><br><span class="line"><span class="comment">#[root@hadoop001 software]# chown -R hadoop:hadoopGroup hadoop-2.8.1/*</span></span><br></pre></td></tr></table></figure><h3 id="8-2-配置修改"><a href="#8-2-配置修改" class="headerlink" title="8.2 配置修改"></a>8.2 配置修改</h3><p>使用命令<code>cd $HADOOP_HOME/etc/hadoop</code>进入配置文件目录</p><ul><li>8.2.1 修改$HADOOP_HOME/etc/hadoop/hadoop-env.sh</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=<span class="string">"/usr/java/jdk1.7.0_67"</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_OPTS=<span class="string">"<span class="variable">$HADOOP_OPTS</span> -Djava.library.path=<span class="variable">$HADOOP_HOME</span>/lib:<span class="variable">$HADOOP_HOME</span>/lib/native"</span></span><br></pre></td></tr></table></figure><ul><li><p>8.2.2 修改$HADOOP_HOME/etc/hadoop/core-site.xml</p></li><li><p>8.2.3 修改$HADOOP_HOME/etc/hadoop/hdfs-site.xml</p></li><li><p>8.2.4 修改$HADOOP_HOEM/etc/hadoop/mapred-site.xml</p><p>  <em>首先复制mapred-site.xml的模板文件</em>，<br>然后修改mapred-site.xml</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01 hadoop]<span class="comment"># cp  mapred-site.xml.template  mapred-site.xml</span></span><br><span class="line">[root@sht-sgmhadoopnn-01 hadoop]<span class="comment"># vi  mapred-site.xml</span></span><br></pre></td></tr></table></figure><ul><li><p>8.2.5 修改$HADOOP_HOME/etc/hadoop/yarn-site.xml</p></li><li><p>8.2.6 修改slaves</p></li></ul><p><a href="http://wpxiejm.oss-cn-beijing.aliyuncs.com/Note/hadoop/Hadoop_HA_conf.rar" target="_blank" rel="noopener">点击下载配置文件</a>，解压后复制到$HADOOP_HOME/etc/hadoop目录下</p><h3 id="8-3-创建临时文件夹和分发文件夹"><a href="#8-3-创建临时文件夹和分发文件夹" class="headerlink" title="8.3 创建临时文件夹和分发文件夹"></a>8.3 创建临时文件夹和分发文件夹</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 hadoop]<span class="comment"># mkdir -p  /opt/software/hadoop/tmp</span></span><br><span class="line">[root@hadoop001 hadoop]<span class="comment"># chmod -R 777  /opt/software/hadoop/tmp</span></span><br><span class="line">[root@hadoop001 hadoop]<span class="comment"># chown -R root:root  /opt/software/hadoop/tmp</span></span><br><span class="line"></span><br><span class="line">[root@hadoop001 hadoop]<span class="comment"># scp -r  /opt/software/hadoop root@hadoop002:/hadoop</span></span><br><span class="line">[root@hadoop001 hadoop]<span class="comment"># scp -r  /opt/software/hadoop root@hadoop003:/hadoop</span></span><br><span class="line">[root@hadoop001 hadoop]<span class="comment"># scp -r  /opt/software/hadoop root@hadoop004:/hadoop</span></span><br><span class="line">[root@hadoop001 hadoop]<span class="comment"># scp -r  /opt/software/hadoop root@hadoop005:/hadoop</span></span><br></pre></td></tr></table></figure><h2 id="9-启动步骤和详细过程："><a href="#9-启动步骤和详细过程：" class="headerlink" title="9.启动步骤和详细过程："></a>9.启动步骤和详细过程：</h2><h3 id="9-1-启动ZK"><a href="#9-1-启动ZK" class="headerlink" title="9.1 启动ZK"></a>9.1 启动ZK</h3><p>在所有的ZK节点执行命令： <code>zkServer.sh start</code></p><p>可借助命令 <code>zkServer.sh status</code>  查看各个ZK的从属关系</p><h3 id="9-2-启动-hadoop-HDFS-YARN"><a href="#9-2-启动-hadoop-HDFS-YARN" class="headerlink" title="9.2 启动 hadoop(HDFS+YARN)"></a>9.2 启动 hadoop(HDFS+YARN)</h3><h4 id="9-2-1-格式化前-先在-journalnode-节点机器上先启动-JournalNode-进程"><a href="#9-2-1-格式化前-先在-journalnode-节点机器上先启动-JournalNode-进程" class="headerlink" title="9.2.1. 格式化前,先在 journalnode 节点机器上先启动 JournalNode 进程"></a>9.2.1. 格式化前,先在 journalnode 节点机器上先启动 JournalNode 进程</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop003 ~]<span class="comment"># cd /opt/software/hadoop/sbin</span></span><br><span class="line">[root@hadoop003 sbin]<span class="comment"># hadoop-daemon.sh start journalnode</span></span><br><span class="line">starting journalnode, logging to /opt/software/hadoop/logs/hadoop-root-journalnode-hadoop001.out</span><br><span class="line">[root@hadoop003 sbin]<span class="comment"># jps</span></span><br><span class="line">4016 Jps</span><br><span class="line">3683 QuorumPeerMain</span><br><span class="line">3981 JournalNode</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@hadoop004 hadoop]<span class="comment"># cd /opt/software/hadoop/sbin</span></span><br><span class="line">[root@hadoop004 sbin]<span class="comment"># hadoop-daemon.sh start journalnode</span></span><br><span class="line">starting journalnode, logging to /opt/software/hadoop/logs/hadoop-root-journalnode-hadoop002.out</span><br><span class="line">[root@hadoop004 sbin]<span class="comment"># jps</span></span><br><span class="line">9891 Jps</span><br><span class="line">9609 QuorumPeerMain</span><br><span class="line">9852 JournalNode</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@hadoop005 hadoop]<span class="comment"># cd /opt/software/hadoop/sbin</span></span><br><span class="line">[root@hadoop005 sbin]<span class="comment"># hadoop-daemon.sh start journalnode</span></span><br><span class="line">starting journalnode, logging to /opt/software/hadoop/logs/hadoop-root-journalnode-hadoop003.out</span><br><span class="line">[root@hadoop005 sbin]<span class="comment"># jps</span></span><br><span class="line">4425 JournalNode</span><br><span class="line">4460 Jps</span><br><span class="line">4191 QuorumPeerMain</span><br></pre></td></tr></table></figure><h4 id="9-2-2-NameNode-格式化"><a href="#9-2-2-NameNode-格式化" class="headerlink" title="9.2.2.NameNode 格式化"></a>9.2.2.NameNode 格式化</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 sbin]<span class="comment"># cd ../</span></span><br><span class="line">[root@hadoop001 hadoop]<span class="comment"># hadoop namenode -format</span></span><br></pre></td></tr></table></figure><h4 id="9-2-3-同步-NameNode-元数据"><a href="#9-2-3-同步-NameNode-元数据" class="headerlink" title="9.2.3.同步 NameNode 元数据"></a>9.2.3.同步 NameNode 元数据</h4><p>同步 hadoop001 元数据到 hadoop002<br>主要是： dfs.namenode.name.dir， dfs.namenode.edits.dir 还应该确保共享存储目录下<br>(dfs.namenode.shared.edits.dir ) 包含 NameNode 所有的元数据。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 hadoop]<span class="comment"># pwd</span></span><br><span class="line">/opt/software/hadoop</span><br><span class="line">[root@hadoop001 hadoop]<span class="comment"># scp -r data/ root@hadoop002:/opt/software/hadoop</span></span><br><span class="line">in_use.lock 100% 14 0.0KB/s 00:00</span><br><span class="line">VERSION 100% 167 0.2KB/s 00:00</span><br><span class="line">seen_txid 100% 2 0.0KB/s 00:00</span><br><span class="line">VERSION 100% 220 0.2KB/s 00:00</span><br><span class="line">fsimage_0000000000000000000.md5 100% 62 0.1KB/s</span><br><span class="line">00:00</span><br><span class="line">fsimage_0000000000000000000 100% 306 0.3KB/s</span><br><span class="line">00:00</span><br><span class="line">[root@hadoop001 hadoop]<span class="comment">#</span></span><br></pre></td></tr></table></figure><h4 id="9-2-4-初始化-ZFCK"><a href="#9-2-4-初始化-ZFCK" class="headerlink" title="9.2.4.初始化 ZFCK"></a>9.2.4.初始化 ZFCK</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 bin]<span class="comment"># hdfs zkfc -formatZK</span></span><br></pre></td></tr></table></figure><h4 id="9-2-5-启动-HDFS-分布式存储系统"><a href="#9-2-5-启动-HDFS-分布式存储系统" class="headerlink" title="9.2.5.启动 HDFS 分布式存储系统"></a>9.2.5.启动 HDFS 分布式存储系统</h4><p>集群启动,在 hadoop001 执行 start-dfs.sh<br>集群关闭,在 hadoop001 执行 stop-dfs.sh</p><ul><li><strong>集群启动</strong></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 sbin]<span class="comment"># start-dfs.sh</span></span><br><span class="line">[root@hadoop001 hadoop]<span class="comment"># start-dfs.sh</span></span><br><span class="line"><span class="comment">#补充集群启动信息</span></span><br></pre></td></tr></table></figure><ul><li><strong>如果需要使用单独启动某个进程，请使用以下命令</strong></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">NameNode(hadoop001, hadoop002):</span><br><span class="line">hadoop-daemon.sh start namenode</span><br><span class="line">DataNode(hadoop003, hadoop004, hadoop005):</span><br><span class="line">hadoop-daemon.sh start datanode</span><br><span class="line">JournamNode(hadoop003, hadoop003, hadoop005):</span><br><span class="line">hadoop-daemon.sh start journalnode</span><br><span class="line">ZKFC(hadoop001, hadoop002):</span><br><span class="line">hadoop-daemon.sh start zkfc</span><br></pre></td></tr></table></figure><h4 id="9-2-6-验证-namenode-datanode-zkfc"><a href="#9-2-6-验证-namenode-datanode-zkfc" class="headerlink" title="9.2.6.验证 namenode,datanode,zkfc"></a>9.2.6.验证 namenode,datanode,zkfc</h4><ul><li>HDFS进程检查</li></ul><p><code>hadoop001</code>和<code>hadoop002</code>节点的进程相同，<code>hadoop003</code>、<code>hadoop004</code>、<code>hadoop005</code>节点的进程相同,下面用hadoop001、hadoop003的jps 命令输出为示例</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 sbin]<span class="comment"># jps</span></span><br><span class="line">12712 Jps</span><br><span class="line">12593 DFSZKFailoverController</span><br><span class="line">12278 NameNode</span><br><span class="line"></span><br><span class="line">[root@hadoop003 ~]<span class="comment"># jps</span></span><br><span class="line">6348 JournalNode</span><br><span class="line">8775 Jps</span><br><span class="line">559 QuorumPeerMain</span><br><span class="line">8509 DataNode</span><br></pre></td></tr></table></figure><ul><li>webUI页面检查NN、DN节点情况</li></ul><p>hadoop001:<br><a href="http://hadoop001:50070/" target="_blank" rel="noopener">http://hadoop001:50070/</a></p><p>hadoop002:<br><a href="http://hadoop002:50070/" target="_blank" rel="noopener">http://hadoop002:50070/</a></p><h4 id="9-2-7-启动-YARN-框架"><a href="#9-2-7-启动-YARN-框架" class="headerlink" title="9.2.7.启动 YARN 框架"></a>9.2.7.启动 YARN 框架</h4><ul><li><strong>集群启动</strong><br>在hadoop001 启动 Yarn，命令所在目录： $HADOOP_HOME/sbin</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 hadoop]<span class="comment"># start-yarn.sh</span></span><br><span class="line">starting yarn daemons</span><br><span class="line">starting resourcemanager, logging to /opt/software/hadoop/logs/yarn-root-resourcemanagerhadoop001.out</span><br><span class="line">hadoop002: starting nodemanager, logging to /opt/software/hadoop/logs/yarn-rootnodemanager-hadoop002.out</span><br><span class="line">hadoop003: starting nodemanager, logging to /opt/software/hadoop/logs/yarn-rootnodemanager-hadoop003.out</span><br><span class="line">hadoop001: starting nodemanager, logging to /opt/software/hadoop/logs/yarn-rootnodemanager-hadoop001.out</span><br><span class="line">[root@hadoop001 hadoop]<span class="comment">#</span></span><br></pre></td></tr></table></figure><ul><li>在hadoop002 备机启动 RM</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop002 ~]<span class="comment"># yarn-daemon.sh start resourcemanager</span></span><br><span class="line">starting resourcemanager, logging to /opt/software/hadoop/logs/yarn-root-resourcemanagerhadoop002.out</span><br><span class="line">[root@hadoop002 ~]<span class="comment">#</span></span><br></pre></td></tr></table></figure><ul><li><strong>如果需要单独启动，请使用下面命令</strong></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">1) ResourceManager(hadoop001, hadoop002)</span><br><span class="line">yarn-daemon.sh start resourcemanager</span><br><span class="line">2) NodeManager(hadoop001, hadoop002, hadoop003)</span><br><span class="line">yarn-daemon.sh start nodemanager</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">- **如果需要关闭YARN集群可以使用下面命令**</span><br><span class="line">[root@hadoop001 sbin]<span class="comment"># stop-yarn.sh</span></span><br><span class="line"><span class="comment">#包含 namenode 的 resourcemanager 进程， datanode 的 nodemanager 进程</span></span><br><span class="line">[root@hadoop002 sbin]<span class="comment"># yarn-daemon.sh stop resourcemanager</span></span><br></pre></td></tr></table></figure><h4 id="9-2-7-验证-resourcemanager-nodemanager"><a href="#9-2-7-验证-resourcemanager-nodemanager" class="headerlink" title="9.2.7.验证 resourcemanager,nodemanager"></a>9.2.7.验证 resourcemanager,nodemanager</h4><ul><li>YARN进程验证</li></ul><p><code>hadoop001</code>和<code>hadoop002</code>节点的进程相同，<code>hadoop003</code>、<code>hadoop004</code>、<code>hadoop005</code>节点的进程相同,下面用hadoop001、hadoop003的jps 命令输出为示例</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># jps</span></span><br><span class="line">5043 NodeManager</span><br><span class="line">3683 QuorumPeerMain</span><br><span class="line">4679 DFSZKFailoverController</span><br><span class="line">5355 Jps</span><br><span class="line">4348 DataNode</span><br><span class="line">3981 JournalNode</span><br><span class="line">4943 ResourceManager</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@hadoop003 ~]<span class="comment"># jps</span></span><br><span class="line">4518 DataNode</span><br><span class="line">4679 NodeManager</span><br><span class="line">4425 JournalNode</span><br><span class="line">4794 Jps</span><br><span class="line">4191 QuorumPeerMain</span><br></pre></td></tr></table></figure><ul><li>页面验证</li></ul><p>ResourceManger（ Active） ： <a href="http://192.168.194.111:8088" target="_blank" rel="noopener">http://192.168.194.111:8088</a></p><p>ResourceManger（ Standby）： <a href="http://192.168.194.112:8088/cluster/cluster" target="_blank" rel="noopener">http://192.168.194.112:8088/cluster/cluster</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要记录Hadoop全分布模式（Fully Distributed Mode）部署的详细步骤：&lt;/p&gt;
&lt;p&gt;Hadoop全分布模式（Fully Distributed Mode）是一种集群部署方式，它的守护进程运行在一个集群上。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://xiejm.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="http://xiejm.com/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop--HDFS的shell(命令行客户端)操作</title>
    <link href="http://xiejm.com/Hadoop/Hadoop--HDFS_Shell.html"/>
    <id>http://xiejm.com/Hadoop/Hadoop--HDFS_Shell.html</id>
    <published>2017-10-03T13:01:46.000Z</published>
    <updated>2018-04-17T04:46:01.000Z</updated>
    
    <content type="html"><![CDATA[<p>Hadoop有两个常用命令  hadoop 和 hdfs</p><p>hadoop fs &lt;====&gt; hdfs dfs   这两个命令时相等的</p><h2 id="1-hadoop-命令使用"><a href="#1-hadoop-命令使用" class="headerlink" title="1.hadoop 命令使用"></a>1.hadoop 命令使用</h2><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://wpxiejm.oss-cn-beijing.aliyuncs.com/Note/hadoop/20170920131431.png" alt="image" title="">                </div>                <div class="image-caption">image</div>            </figure><p>命令行客户端支持的命令参数：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 bin]<span class="comment"># hadoop fs</span></span><br><span class="line">Usage: hadoop fs [generic options]</span><br><span class="line">    [-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">    [-cat [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">    [-checksum &lt;src&gt; ...]</span><br><span class="line">    [-chgrp [-R] GROUP PATH...]</span><br><span class="line">    [-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]</span><br><span class="line">    [-chown [-R] [OWNER][:[GROUP]] PATH...]</span><br><span class="line">    [-copyFromLocal [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">    [-copyToLocal [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">    [-count [-q] [-h] [-v] &lt;path&gt; ...]</span><br><span class="line">    [-cp [-f] [-p | -p[topax]] &lt;src&gt; ... &lt;dst&gt;]</span><br><span class="line">    [-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]]</span><br><span class="line">    [-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;]</span><br><span class="line">    [-df [-h] [&lt;path&gt; ...]]</span><br><span class="line">    [-du [-s] [-h] &lt;path&gt; ...]</span><br><span class="line">    [-expunge]</span><br><span class="line">    [-find &lt;path&gt; ... &lt;expression&gt; ...]</span><br><span class="line">    [-get [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">    [-getfacl [-R] &lt;path&gt;]</span><br><span class="line">    [-getfattr [-R] &#123;-n name | -d&#125; [-e en] &lt;path&gt;]</span><br><span class="line">    [-getmerge [-nl] &lt;src&gt; &lt;localdst&gt;]</span><br><span class="line">    [-<span class="built_in">help</span> [cmd ...]]</span><br><span class="line">    [-ls [-d] [-h] [-R] [&lt;path&gt; ...]]</span><br><span class="line">    [-mkdir [-p] &lt;path&gt; ...]</span><br><span class="line">    [-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">    [-moveToLocal &lt;src&gt; &lt;localdst&gt;]</span><br><span class="line">    [-mv &lt;src&gt; ... &lt;dst&gt;]</span><br><span class="line">    [-put [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">    [-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;]</span><br><span class="line">    [-rm [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...]</span><br><span class="line">    [-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; ...]</span><br><span class="line">    [-setfacl [-R] [&#123;-b|-k&#125; &#123;-m|-x &lt;acl_spec&gt;&#125; &lt;path&gt;]|[--<span class="built_in">set</span> &lt;acl_spec&gt; &lt;path&gt;]]</span><br><span class="line">    [-setfattr &#123;-n name [-v value] | -x name&#125; &lt;path&gt;]</span><br><span class="line">    [-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...]</span><br><span class="line">    [-<span class="built_in">stat</span> [format] &lt;path&gt; ...]</span><br><span class="line">    [-tail [-f] &lt;file&gt;]</span><br><span class="line">    [-<span class="built_in">test</span> -[defsz] &lt;path&gt;]</span><br><span class="line">    [-text [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">    [-touchz &lt;path&gt; ...]</span><br><span class="line">    [-usage [cmd ...]]</span><br><span class="line"></span><br><span class="line">Generic options supported are</span><br><span class="line">-conf &lt;configuration file&gt;     specify an application configuration file</span><br><span class="line">-D &lt;property=value&gt;            use value <span class="keyword">for</span> given property</span><br><span class="line">-fs &lt;<span class="built_in">local</span>|namenode:port&gt;      specify a namenode</span><br><span class="line">-jt &lt;<span class="built_in">local</span>|resourcemanager:port&gt;    specify a ResourceManager</span><br><span class="line">-files &lt;comma separated list of files&gt;    specify comma separated files to be copied to the map reduce cluster</span><br><span class="line">-libjars &lt;comma separated list of jars&gt;    specify comma separated jar files to include <span class="keyword">in</span> the classpath.</span><br><span class="line">-archives &lt;comma separated list of archives&gt;    specify comma separated archives to be unarchived on the compute machines.</span><br><span class="line"></span><br><span class="line">The general <span class="built_in">command</span> line syntax is</span><br><span class="line">bin/hadoop <span class="built_in">command</span> [genericOptions] [commandOptions]</span><br></pre></td></tr></table></figure><h2 id="2-常用命令参数介绍"><a href="#2-常用命令参数介绍" class="headerlink" title="2.常用命令参数介绍"></a>2.常用命令参数介绍</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">-<span class="built_in">help</span></span><br><span class="line">功能：输出这个命令参数手册</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">-ls</span><br><span class="line">功能：显示目录信息</span><br><span class="line">示例： hadoop fs -ls hdfs://hadoop001:9000/</span><br><span class="line">备注：这些参数中，所有的hdfs路径都可以简写</span><br><span class="line">--&gt;hadoop fs -ls /   等同于上一条命令的效果</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-mkdir</span><br><span class="line">功能：在hdfs上创建目录</span><br><span class="line">示例：hadoop fs  -mkdir  -p  /aaa/bbb/cc/dd</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-moveFromLocal</span><br><span class="line">功能：从本地剪切粘贴到hdfs</span><br><span class="line">示例：hadoop  fs  -moveFromLocal  /home/hadoop/a.txt  /aaa/bbb/cc/dd</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">--appendToFile</span><br><span class="line">功能：追加一个文件到已经存在的文件末尾</span><br><span class="line">示例：hadoop  fs  -appendToFile  ./hello.txt  hdfs://hadoop001:9000/hello.txt</span><br><span class="line">可以简写为：</span><br><span class="line">Hadoop  fs  -appendToFile  ./hello.txt  /hello.txt</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-cat</span><br><span class="line">功能：显示文件内容</span><br><span class="line">示例：hadoop fs -cat  /hello.txt</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-tail</span><br><span class="line">功能：显示一个文件的末尾</span><br><span class="line">示例：hadoop  fs  -tail  /hello.txt</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-text</span><br><span class="line">功能：以字符形式打印一个文件的内容</span><br><span class="line">示例：hadoop  fs  -text  /hello.txt</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">-chgrp</span><br><span class="line">-chmod</span><br><span class="line">-chown</span><br><span class="line">功能：linux文件系统中的用法一样，对文件所属权限</span><br><span class="line">示例：</span><br><span class="line">hadoop  fs  -chmod  666  /hello.txt</span><br><span class="line">hadoop  fs  -chown  someuser:somegrp   /hello.txt</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-copyFromLocal</span><br><span class="line">功能：从本地文件系统中拷贝文件到hdfs路径去</span><br><span class="line">示例：hadoop  fs  -copyFromLocal  ./jdk.tar.gz  /aaa/</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-copyToLocal</span><br><span class="line">功能：从hdfs拷贝到本地</span><br><span class="line">示例：hadoop fs -copyToLocal /aaa/jdk.tar.gz</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-cp</span><br><span class="line">功能：从hdfs的一个路径拷贝hdfs的另一个路径</span><br><span class="line">示例： hadoop  fs  -cp  /aaa/jdk.tar.gz  /bbb/jdk.tar.gz.2</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-mv</span><br><span class="line">功能：在hdfs目录中移动文件</span><br><span class="line">示例： hadoop  fs  -mv  /aaa/jdk.tar.gz  /</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-get</span><br><span class="line">功能：等同于copyToLocal，就是从hdfs下载文件到本地</span><br><span class="line">示例：hadoop fs -get  /aaa/jdk.tar.gz</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">-getmerge</span><br><span class="line">功能：合并下载多个文件</span><br><span class="line">示例：比如hdfs的目录 /aaa/下有多个文件:log.1, log.2,log.3,...</span><br><span class="line">hadoop fs -getmerge /aaa/<span class="built_in">log</span>.* ./log.sum</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-put</span><br><span class="line">功能：等同于copyFromLocal</span><br><span class="line">示例：hadoop  fs  -put  /aaa/jdk.tar.gz  /bbb/jdk.tar.gz.2</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-rm</span><br><span class="line">功能：删除文件或文件夹</span><br><span class="line">示例：hadoop fs -rm -r /aaa/bbb/</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-rmdir</span><br><span class="line">功能：删除空目录</span><br><span class="line">示例：hadoop  fs  -rmdir   /aaa/bbb/ccc</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">-df</span><br><span class="line">功能：统计文件系统的可用空间信息</span><br><span class="line">示例：hadoop  fs  -df  -h  /</span><br><span class="line"></span><br><span class="line">[root@hadoop001 ~]<span class="comment"># hadoop fs -df -h /</span></span><br><span class="line">Filesystem               Size    Used  Available  Use%</span><br><span class="line">hdfs://hadoop001:9000  15.1 G  17.1 M      7.6 G    0%</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">-du</span><br><span class="line">功能：统计文件夹的大小信息</span><br><span class="line">示例：</span><br><span class="line">hadoop  fs  -du  -s  -h /aaa/*</span><br><span class="line"></span><br><span class="line">[root@hadoop001 ~]<span class="comment"># hadoop fs -du -s -h /aaa/*</span></span><br><span class="line">419.8 K  419.8 K  /aaa/bbbb</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-count</span><br><span class="line">功能：统计一个指定目录下的文件节点数量</span><br><span class="line">示例：hadoop fs -count /aaa/</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">-setrep</span><br><span class="line">功能：设置hdfs中文件的副本数量</span><br><span class="line">示例：hadoop fs -setrep 3 /aaa/jdk.tar.gz</span><br><span class="line">&lt;这里设置的副本数只是记录在namenode的元数据中，是否真的会有这么多副本，还得看datanode的数量&gt;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Hadoop有两个常用命令  hadoop 和 hdfs&lt;/p&gt;
&lt;p&gt;hadoop fs &amp;lt;====&amp;gt; hdfs dfs   这两个命令时相等的&lt;/p&gt;
&lt;h2 id=&quot;1-hadoop-命令使用&quot;&gt;&lt;a href=&quot;#1-hadoop-命令使用&quot; class
      
    
    </summary>
    
      <category term="大数据" scheme="http://xiejm.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="http://xiejm.com/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop 源码编译安装</title>
    <link href="http://xiejm.com/Hadoop/Hadoop_source_code_building.html"/>
    <id>http://xiejm.com/Hadoop/Hadoop_source_code_building.html</id>
    <published>2017-10-03T12:58:16.000Z</published>
    <updated>2018-04-17T04:46:03.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要记录Hadoop2.x源码编译的详细步骤：<br><a id="more"></a></p><h2 id="1-编译基础环境"><a href="#1-编译基础环境" class="headerlink" title="1.编译基础环境"></a>1.编译基础环境</h2><ul><li>CentOS 6.7 minimal</li><li>Hadoop 2.7.4</li><li>JDK 1.7+</li><li>Maven 3.0 or later</li><li>Findbugs 1.3.9 (if running findbugs)</li><li>ProtocolBuffer 2.5.0</li><li>CMake 2.6 or newer (if compiling native code)</li><li>Zlib devel (if compiling native code)</li><li>openssl devel ( if compiling native hadoop-pipes and to get the best HDFS encryption performance )</li><li>Internet connection for first build</li></ul><blockquote><p>系统原始环境特别说明：<br>系统选择minimal最小系统安装，并自定义选装以下包： Base System中的Base、Compatbillty libraries、Debugging Tools; Development中的 Development tools</p></blockquote><h2 id="2-yum-源配置"><a href="#2-yum-源配置" class="headerlink" title="2.yum 源配置"></a>2.yum 源配置</h2><p>参考本博客的yum源配置的文章:<a href="http://xiejm.com/2017/08/31/Change_CentOS_yum_repo_to_aliyun.html">yum源配置为阿里云镜像站</a></p><h2 id="3-下载Hadoop源码包"><a href="#3-下载Hadoop源码包" class="headerlink" title="3.下载Hadoop源码包"></a>3.下载Hadoop源码包</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 sourcecode]<span class="comment"># pwd</span></span><br><span class="line">/opt/sourcecode</span><br><span class="line">[root@hadoop001 sourcecode]<span class="comment"># mkdir -p /opt/sourcecode</span></span><br><span class="line">[root@hadoop001 sourcecode]<span class="comment">#wget  http://mirror.bit.edu.cn/apache/hadoop/common/hadoop-2.7.4/hadoop-2.7.4-src.tar.gz</span></span><br><span class="line">[root@hadoop001 sourcecode]<span class="comment">#tar -xzvf  hadoop-2.7.4-src.tar.gz</span></span><br><span class="line">[root@hadoop001 sourcecode]<span class="comment">#cat ./hadoop-2.7.4-src/BUILDING.txt</span></span><br><span class="line"><span class="comment">#从BUILDING文件中我们可以看到编译的要求</span></span><br></pre></td></tr></table></figure><h2 id="4-JDK安装"><a href="#4-JDK安装" class="headerlink" title="4.JDK安装"></a>4.JDK安装</h2><p>参考本博客的JDK安装的文章:<a href="http://xiejm.com/2017/08/31/CentOS_setup_JDK.html">JDK安装</a></p><h2 id="5-Maven安装"><a href="#5-Maven安装" class="headerlink" title="5.Maven安装"></a>5.Maven安装</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /opt/software</span><br><span class="line"><span class="built_in">cd</span> /opt/software</span><br><span class="line">上传apache-maven-3.3.9-bin.zip</span><br><span class="line">unzip apache-maven-3.3.9-bin.zip</span><br><span class="line"><span class="comment">#添加maven环境变量</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">'#SET MAVEN'</span>&gt;&gt;/etc/profile</span><br><span class="line"><span class="built_in">echo</span> <span class="string">'export MAVEN_HOME=/opt/software/apache-maven-3.3.9'</span> &gt;&gt;/etc/profile</span><br><span class="line"><span class="built_in">echo</span> <span class="string">'export  MAVEN_OPTS="-Xms256m -Xmx512m"'</span>&gt;&gt;/etc/profile</span><br><span class="line"><span class="built_in">echo</span> <span class="string">'export PATH=$MAVEN_HOME/bin:$PATH'</span>&gt;&gt;/etc/profile</span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br><span class="line">mvn --version</span><br></pre></td></tr></table></figure><h2 id="6-protobuf安装"><a href="#6-protobuf安装" class="headerlink" title="6.protobuf安装"></a>6.protobuf安装</h2><p>protobuf要编译安装，需安装gcc、gcc-c++、 make<br>上传 protobuf-2.5.0.tar.gz</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tar -xzvf protobuf-2.5.0.tar.gz</span><br><span class="line"><span class="built_in">cd</span> protobuf-2.5.0</span><br><span class="line">yum install -y gcc gcc-c++ make</span><br><span class="line">./configure --prefix=/usr/<span class="built_in">local</span>/protobuf</span><br><span class="line">make &amp;&amp; make install</span><br><span class="line"><span class="comment">#添加protobuf环境变量</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">'#SET PROTOBUF'</span>&gt;&gt;/etc/profile</span><br><span class="line"><span class="built_in">echo</span> <span class="string">'export PROTOC_HOME=/usr/local/protobuf'</span>&gt;&gt;/etc/profile</span><br><span class="line"><span class="built_in">echo</span> <span class="string">'export PATH=$PROTOC_HOME/bin:$PATH'</span>&gt;&gt;/etc/profile</span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br><span class="line">protoc --version</span><br></pre></td></tr></table></figure><h2 id="7-Findbugs安装"><a href="#7-Findbugs安装" class="headerlink" title="7.Findbugs安装"></a>7.Findbugs安装</h2><p>上传 findbugs-1.3.9.zip</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">unzip findbugs-1.3.9.zip</span><br><span class="line"><span class="comment">#添加Findbugs环境变量</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">'#SET Findbugs'</span>&gt;&gt;/etc/profile</span><br><span class="line"><span class="built_in">echo</span> <span class="string">'export  FINDBUGS_HOME=/opt/software/findbugs-1.3.9'</span>&gt;&gt;/etc/profile</span><br><span class="line"><span class="built_in">echo</span> <span class="string">'export PATH=$FINDBUGS_HOME/bin:$PATH'</span>&gt;&gt;/etc/profile</span><br></pre></td></tr></table></figure><h2 id="8-Snappy压缩库安装"><a href="#8-Snappy压缩库安装" class="headerlink" title="8.Snappy压缩库安装"></a>8.Snappy压缩库安装</h2><p>安装snappy1.1.4,使Hadoop支持snappy压缩</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/google/snappy/releases/download/1.1.4/snappy-1.1.4.tar.gz</span><br><span class="line">tar -zxvf snappy-1.1.4.tar.gz</span><br><span class="line"><span class="built_in">cd</span> snappy-1.1.4</span><br><span class="line">./configure</span><br><span class="line">make &amp;&amp; make install</span><br><span class="line">ll -h /usr/<span class="built_in">local</span>/lib |grep snappy</span><br></pre></td></tr></table></figure><h2 id="9-其他依赖安装"><a href="#9-其他依赖安装" class="headerlink" title="9.其他依赖安装"></a>9.其他依赖安装</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum install -y ant openssl openssl-devel svn ncurses-devel zlib-devel libtool svn</span><br><span class="line">yum install -y snappy snappy-devel bzip2 bzip2-devel lzo lzo-devel lzop autoconf automake</span><br></pre></td></tr></table></figure><h2 id="10-编译"><a href="#10-编译" class="headerlink" title="10.编译"></a>10.编译</h2><p>进入Hadooop源码目录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> hadoop-2.7.4-src</span><br><span class="line">mvn clean package -DskipTests -Pdist,native -Dtar -Dsnappy.lib=/usr/<span class="built_in">local</span>/lib -Dbundle.snappy</span><br></pre></td></tr></table></figure><h2 id="11-生成tar包"><a href="#11-生成tar包" class="headerlink" title="11.生成tar包"></a>11.生成tar包</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/opt/sourcecode/hadoop-2.7.4-src/hadoop-dist/target/hadoop-2.7.4.tar.gz</span><br></pre></td></tr></table></figure><h2 id="注意事项："><a href="#注意事项：" class="headerlink" title="注意事项："></a>注意事项：</h2><ul><li>由于Maven仓库在墙外，Maven在编译项目时下载包卡住情况，ctrl+c 中断，重新执行编译。</li><li>如果出现提示缺少了某个文件的情况，则要先清理maven(使用命令 mvn clean) 再重新编译。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要记录Hadoop2.x源码编译的详细步骤：&lt;br&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://xiejm.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="http://xiejm.com/tags/Hadoop/"/>
    
      <category term="Maven" scheme="http://xiejm.com/tags/Maven/"/>
    
  </entry>
  
  <entry>
    <title>Linux Extended VM Disk In VMware</title>
    <link href="http://xiejm.com/Linux/Extended_VM%20Disk_In_%20VMware.html"/>
    <id>http://xiejm.com/Linux/Extended_VM Disk_In_ VMware.html</id>
    <published>2017-10-03T11:59:04.000Z</published>
    <updated>2018-04-17T04:43:47.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在做大数据相关开发的测试时候，会用到VM虚拟机。<br>用了时间久了以后，虚拟机磁盘爆满，这时就需要给虚拟机磁盘扩容了。</p><p>由于在当初创建虚拟机时选择了LVM分区，给现在扩容带来了便利。</p><a id="more"></a><h2 id="查看本地磁盘使用情况"><a href="#查看本地磁盘使用情况" class="headerlink" title="查看本地磁盘使用情况"></a>查看本地磁盘使用情况</h2><p>磁盘使用情况如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># df -Th</span></span><br><span class="line">Filesystem           Type   Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/mapper/vg_master-lvm_root</span><br><span class="line">                     ext4    18G  2.3G   15G  14% /</span><br><span class="line">tmpfs                tmpfs  932M     0  932M   0% /dev/shm</span><br><span class="line">/dev/sda1            ext4   190M   41M  140M  23% /boot</span><br></pre></td></tr></table></figure><h2 id="查看分区情况"><a href="#查看分区情况" class="headerlink" title="查看分区情况"></a>查看分区情况</h2><p>原磁盘为20G，先扩充到40G</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># fdisk /dev/sda</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Command (m <span class="keyword">for</span> <span class="built_in">help</span>): p</span><br><span class="line"></span><br><span class="line">Disk /dev/sda: 42.9 GB, 42949672960 bytes</span><br><span class="line">255 heads, 63 sectors/track, 5221 cylinders</span><br><span class="line">Units = cylinders of 16065 * 512 = 8225280 bytes</span><br><span class="line">Sector size (logical/physical): 512 bytes / 512 bytes</span><br><span class="line">I/O size (minimum/optimal): 512 bytes / 512 bytes</span><br><span class="line">Disk identifier: 0x00012968</span><br><span class="line"></span><br><span class="line">   Device Boot      Start         End      Blocks   Id  System</span><br><span class="line">/dev/sda1   *           1          26      204800   83  Linux</span><br><span class="line">Partition 1 does not end on cylinder boundary.</span><br><span class="line">/dev/sda2              26         281     2048000   82  Linux swap / Solaris</span><br><span class="line">Partition 2 does not end on cylinder boundary.</span><br><span class="line">/dev/sda3             281        2611    18717696   8e  Linux LVM</span><br></pre></td></tr></table></figure><h2 id="对新盘再分区"><a href="#对新盘再分区" class="headerlink" title="对新盘再分区"></a>对新盘再分区</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># fdisk /dev/sda</span></span><br><span class="line"></span><br><span class="line">WARNING: DOS-compatible mode is deprecated. It<span class="string">'s strongly recommended to</span></span><br><span class="line"><span class="string">         switch off the mode (command '</span>c<span class="string">') and change display units to</span></span><br><span class="line"><span class="string">         sectors (command '</span>u<span class="string">').</span></span><br><span class="line"><span class="string">Command (m for help):</span></span><br></pre></td></tr></table></figure><p>请安步骤输入下面操作</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">p　　　　　　　查看已分区数量（我看到有两个 /dev/sda1 /dev/sda2）</span><br><span class="line">n　　　　　　　新增加一个分区</span><br><span class="line"></span><br><span class="line">p　　　　　　　分区类型我们选择为主分区</span><br><span class="line"></span><br><span class="line">4　　　　　　　分区号选3（因为1,2已经用过了，见上）</span><br><span class="line"></span><br><span class="line">回车　　　　　　默认（起始扇区）</span><br><span class="line"></span><br><span class="line">回车　　　　　　默认（结束扇区）</span><br><span class="line"></span><br><span class="line">t　　　　　　　修改分区类型</span><br><span class="line"></span><br><span class="line">4　　　　　　　选分区3</span><br><span class="line"></span><br><span class="line">8e　　　　　　修改为LVM（8e就是LVM）</span><br><span class="line"></span><br><span class="line">w　　　　　　写分区表(可能直接退出了，没有关系的)</span><br><span class="line"></span><br><span class="line">q　　　　　　完成，退出fdisk命令</span><br></pre></td></tr></table></figure><h2 id="重启并格式化"><a href="#重启并格式化" class="headerlink" title="重启并格式化"></a>重启并格式化</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># reboot</span></span><br></pre></td></tr></table></figure><p>格式 /dev/sda4</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># ll /dev/sda4</span></span><br><span class="line">brw-rw----. 1 root disk 8, 4 Sep 23 08:19 /dev/sda4</span><br><span class="line">[root@hadoop001 ~]<span class="comment"># mkfs.ext4 /dev/sda4</span></span><br></pre></td></tr></table></figure><h2 id="添加新的LVM实现扩容"><a href="#添加新的LVM实现扩容" class="headerlink" title="添加新的LVM实现扩容"></a>添加新的LVM实现扩容</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#进入lvm管理</span></span><br><span class="line">[root@hadoop001 ~]<span class="comment"># lvm</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#创建PV 这是初始化刚才的分区，必须的</span></span><br><span class="line">lvm&gt; pvcreate /dev/sda4</span><br><span class="line"></span><br><span class="line"><span class="comment">#将初始化过的分区加入到虚拟卷组vg_master(卷组名根据系统内的实际卷组名为准)</span></span><br><span class="line">lvm&gt; vgextend vg_master /dev/sda3</span><br><span class="line"></span><br><span class="line"><span class="comment">#扩展已有卷的容量</span></span><br><span class="line">lvm&gt;lvextend -l +100% /dev/mapper/vg_master-lvm_root</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看卷容量，这时你会看到一个很大的卷了</span></span><br><span class="line">lvm&gt;pvdisplay</span><br><span class="line"></span><br><span class="line">lvm&gt;quit</span><br></pre></td></tr></table></figure><p>备注：lvextend扩展命令这一步，还可以选择扩容大小</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#扩展已有卷的容量（只扩充10G）</span></span><br><span class="line">lvm&gt;lvextend -L +10G /dev/mapper/vg_master-lvm_root</span><br></pre></td></tr></table></figure><h2 id="刷新磁盘容量"><a href="#刷新磁盘容量" class="headerlink" title="刷新磁盘容量"></a>刷新磁盘容量</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># resize2fs /dev/mapper/vg_master-lvm_root</span></span><br><span class="line"></span><br><span class="line">resize2fs 1.41.12 (17-May-2010)</span><br><span class="line">Filesystem at /dev/mapper/vg_master-lvm_root is mounted on /; on-line resizing required</span><br><span class="line">old desc_blocks = 2, new_desc_blocks = 3</span><br><span class="line">Performing an on-line resize of /dev/mapper/vg_master-lvm_root to 9919488 (4k) blocks.</span><br><span class="line">The filesystem on /dev/mapper/vg_master-lvm_root is now 9919488 blocks long.</span><br></pre></td></tr></table></figure><h2 id="再次查看磁盘分区情况"><a href="#再次查看磁盘分区情况" class="headerlink" title="再次查看磁盘分区情况"></a>再次查看磁盘分区情况</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># df -Th</span></span><br><span class="line">Filesystem           Type   Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/mapper/vg_master-lvm_root</span><br><span class="line">                     ext4    38G  2.4G   33G   7% /</span><br><span class="line">tmpfs                tmpfs  932M     0  932M   0% /dev/shm</span><br><span class="line">/dev/sda1            ext4   190M   41M  140M  23% /boot</span><br></pre></td></tr></table></figure><p>扩容后<code>/dev/mapper/vg_master-lvm_root</code>的大小是38G了</p><p>至此扩容完成！</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;在做大数据相关开发的测试时候，会用到VM虚拟机。&lt;br&gt;用了时间久了以后，虚拟机磁盘爆满，这时就需要给虚拟机磁盘扩容了。&lt;/p&gt;
&lt;p&gt;由于在当初创建虚拟机时选择了LVM分区，给现在扩容带来了便利。&lt;/p&gt;
    
    </summary>
    
      <category term="Linux" scheme="http://xiejm.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="http://xiejm.com/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop伪分布式部署</title>
    <link href="http://xiejm.com/Hadoop/Hadoop_Pseudo-Distributed_Mode.html"/>
    <id>http://xiejm.com/Hadoop/Hadoop_Pseudo-Distributed_Mode.html</id>
    <published>2017-10-03T06:16:06.000Z</published>
    <updated>2018-04-17T04:46:06.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要记录Hadoop伪分布式（Pseudo-Distributed Mode）部署的详细步骤：<br>伪分布模式在“单节点集群”上运行Hadoop，其中所有的守护进程都运行在同一台机器上。该模式在单机模式之上增加了代码调试功能，允许你检查内存使用情况，HDFS输入输出，以及其他的守护进程交互。</p><h2 id="1-基本环境"><a href="#1-基本环境" class="headerlink" title="1.基本环境"></a>1.基本环境</h2><ul><li>CentOS 6.7</li><li>JDK1.7+</li><li>Hadoop 2.7.4</li></ul><blockquote><p>系统原始环境特别说明：<br>系统选择minimal最小系统安装，并自定义选装以下包：<br>Base System中的Base、Compatbillty libraries、Debugging Tools; Development中的 Development tools</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#关闭防火墙</span></span><br><span class="line">[root@hadoop001 ~]<span class="comment"># service iptables stop</span></span><br><span class="line">iptables: Setting chains to policy ACCEPT: filter          [  OK  ]</span><br><span class="line">iptables: Flushing firewall rules:                         [  OK  ]</span><br><span class="line">iptables: Unloading modules:                               [  OK  ]</span><br><span class="line">[root@hadoop001 ~]<span class="comment"># chkconfig iptables off</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#关闭SELINUX</span></span><br><span class="line">[root@hadoop001 ~]<span class="comment"># sed -i 's#SELINUX=enforcing#SELINUX=disabled#g' /etc/selinux/config</span></span><br><span class="line"><span class="comment">#关闭当前的SELINUX状态,如果不用setenforce那么需要重启系统</span></span><br><span class="line">[root@hadoop001 ~]<span class="comment"># setenforce 0</span></span><br><span class="line">[root@hadoop001 ~]<span class="comment"># grep SELINUX=disabled /etc/selinux/config</span></span><br><span class="line">SELINUX=disabled</span><br></pre></td></tr></table></figure><h2 id="2-JDK安装"><a href="#2-JDK安装" class="headerlink" title="2.JDK安装"></a>2.JDK安装</h2><p>本文采用JDK1.7.79,详细安装步骤参考本博客的JDK安装的文章:<a href="http://xiejm.com/Linux/CentOS_setup_JDK.html">JDK安装</a></p><h2 id="3-创建Hadoop用户"><a href="#3-创建Hadoop用户" class="headerlink" title="3.创建Hadoop用户"></a>3.创建Hadoop用户</h2><p>增加ID号为1000的hadoopGroup,增加ID为2000的用户并加入hadoopGroup组</p><p>固定用户和组的ID号是为了用户和组的权限一致，我遇到过一次在两台机器上用户名和组名相同，ID号不同结果导致权限有问题</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># groupadd -g 1000 hadoopGroup</span></span><br><span class="line">[root@hadoop001 ~]<span class="comment"># useradd -u 2000 -g hadoopGroup hadoop</span></span><br></pre></td></tr></table></figure><p>设置hadoop用户的密码为hadoop，下面的这种方法免去了修改密码时的密码确认</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># echo "hadoop"|passwd --stdin hadoop</span></span><br></pre></td></tr></table></figure><p>设置hadoop用户sodu权限</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># vi /etc/sudoers</span></span><br><span class="line">hadoop ALL=(ALL)       ALL</span><br></pre></td></tr></table></figure><h2 id="4-上传（下载）解压Hadoop包"><a href="#4-上传（下载）解压Hadoop包" class="headerlink" title="4.上传（下载）解压Hadoop包"></a>4.上传（下载）解压Hadoop包</h2><p>hadoop-2.7.4.tar.gz包使用rz命令上传或通过wget命令去官网下载</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># mkdir -p /opt/software</span></span><br><span class="line"><span class="comment">#将hadoop安装包放到/opt/software</span></span><br><span class="line">[root@hadoop001 ~]<span class="comment"># cd /opt/software</span></span><br><span class="line">[root@hadoop001 software]<span class="comment"># tar -zxvf hadoop-2.7.4.tar.gz</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#设置软连接</span></span><br><span class="line">[root@hadoop001 software]<span class="comment"># ln -s /opt/software/hadoop-2.7.4 /opt/software/hadoop</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#设置hadoop目录所有者、所属组</span></span><br><span class="line">[root@hadoop001 software]<span class="comment"># chown -R hadoop:hadoopGroup hadoop/*</span></span><br><span class="line">[root@hadoop001 software]<span class="comment"># chown -R hadoop:hadoopGroup hadoop-2.7.4</span></span><br><span class="line">[root@hadoop001 software]<span class="comment"># chown -R hadoop:hadoopGroup hadoop-2.7.4/*</span></span><br></pre></td></tr></table></figure><h2 id="5-配置环境变量"><a href="#5-配置环境变量" class="headerlink" title="5.配置环境变量"></a>5.配置环境变量</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 software]<span class="comment"># vim /etc/profile</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#按组合键shift+g 或大写G 到最后一行，按字母o  插入下列内容</span></span><br><span class="line">    <span class="comment">#SET HADOOP</span></span><br><span class="line">    exprot HADOOP_HOME=/opt/software/hadoop</span><br><span class="line"><span class="comment">#如果JDK环境变量没有配置，请加入下列内容</span></span><br><span class="line">    <span class="comment">#SET JDK</span></span><br><span class="line">    <span class="built_in">export</span> JAVA_HOME=/usr/java/jdk1.7.0_79</span><br><span class="line">    <span class="built_in">export</span> JRE_HOME=<span class="variable">$JAVA_HOME</span>/jre</span><br><span class="line">    <span class="built_in">export</span> CLASS_PATH=.:<span class="variable">$JAVA_HOME</span>/lib:<span class="variable">$JAVA_HOME</span>/lib/tools.jar</span><br><span class="line"></span><br><span class="line"><span class="comment">#设置PATH</span></span><br><span class="line">    <span class="built_in">export</span> PATH=<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/sbin:<span class="variable">$&#123;JAVA_HOME&#125;</span>/bin:<span class="variable">$&#123;JRE_HOME&#125;</span>/bin:<span class="variable">$PATH</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#添加完成后  :wq 保存</span></span><br><span class="line"><span class="comment">#使修改生效：</span></span><br><span class="line">[root@hadoop001 software]<span class="comment">#source !$</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#验证JDK有效性</span></span><br><span class="line">[root@hadoop001 software]<span class="comment"># java -version</span></span><br><span class="line">java version <span class="string">"1.7.0_79"</span></span><br><span class="line">Java(TM) SE Runtime Environment (build 1.7.0_79-b15)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 24.79-b02, mixed mode)</span><br></pre></td></tr></table></figure><h2 id="6-配置SSH"><a href="#6-配置SSH" class="headerlink" title="6.配置SSH"></a>6.配置SSH</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#检查sshd</span></span><br><span class="line">[root@hadoop001 software]<span class="comment"># service sshd status</span></span><br><span class="line">openssh-daemon (pid  1844) is running...</span><br><span class="line">[root@hadoop001 software]<span class="comment"># su - hadoop</span></span><br><span class="line"></span><br><span class="line">[hadoop@hadoop001 ~]<span class="comment"># cd /opt/software/hadoop</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#SSH 免密设置</span></span><br><span class="line"><span class="comment">#由于是伪分布式需要用到ssh localhost的情况，所以需要在本机生成公钥</span></span><br><span class="line">[hadoop@hadoop001 hadoop]$ ssh-keygen -t rsa -P <span class="string">''</span> -f ~/.ssh/id_rsa</span><br><span class="line">[hadoop@hadoop001 hadoop]$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br><span class="line">[hadoop@hadoop001 hadoop]$ chmod 0600 ~/.ssh/authorized_keys</span><br><span class="line"></span><br><span class="line"><span class="comment"># 验证SSH</span></span><br><span class="line">[hadoop@hadoop001 hadoop]<span class="variable">$ssh</span> hadoop001 date</span><br><span class="line">The authenticity of host <span class="string">'hadoop001 (192.168.137.130)'</span> can<span class="string">'t be established.</span></span><br><span class="line"><span class="string">RSA key fingerprint is 09:f6:4a:f1:a0:bd:79:fd:34:e7:75:94:0b:3c:83:5a.</span></span><br><span class="line"><span class="string">Are you sure you want to continue connecting (yes/no)? yes   #第一次回车输入yes</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Warning: Permanently added '</span>hadoop001,192.168.137.130<span class="string">' (RSA) to the list of known hosts.</span></span><br><span class="line"><span class="string">Sun Aug 20 14:22:28 CST 2017</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[hadoop@hadoop001 hadoop]$ssh hadoop001 date   #不需要回车输入yes,即SSH配置成功</span></span><br><span class="line"><span class="string">Sun Aug 20 14:22:29 CST 2017</span></span><br></pre></td></tr></table></figure><h2 id="7-配置core-site-xml"><a href="#7-配置core-site-xml" class="headerlink" title="7.配置core-site.xml"></a>7.配置core-site.xml</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ vi etc/hadoop/core-site.xml</span><br><span class="line"><span class="comment">#配置HDFS地址</span></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://hadoop001:9000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h2 id="8-配置hdfs-site-xml"><a href="#8-配置hdfs-site-xml" class="headerlink" title="8.配置hdfs-site.xml"></a>8.配置hdfs-site.xml</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ vi etc/hadoop/hdfs-site.xml</span><br><span class="line"><span class="comment">#配置HDFS副本数</span></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hadoop001:50090&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.secondary.https-address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hadoop001:50091&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h2 id="9-修改mapred-site-xml"><a href="#9-修改mapred-site-xml" class="headerlink" title="9.修改mapred-site.xml"></a>9.修改mapred-site.xml</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ vi etc/hadoop/mapred-site.xml:</span><br><span class="line"><span class="comment">#设置mapreduce框架为yarn</span></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h2 id="10-修改yarn-site-xml"><a href="#10-修改yarn-site-xml" class="headerlink" title="10.修改yarn-site.xml"></a>10.修改yarn-site.xml</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ etc/hadoop/yarn-site.xml:</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h2 id="11-修改slave文件"><a href="#11-修改slave文件" class="headerlink" title="11.修改slave文件"></a>11.修改slave文件</h2><p>将本机的主机名写入slave节点</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ehco <span class="string">'hadoop001'</span> &gt;/opt/software/hadoop/etc/hadooop/slave</span><br></pre></td></tr></table></figure><h2 id="12-启动HDFS和YARN"><a href="#12-启动HDFS和YARN" class="headerlink" title="12.启动HDFS和YARN"></a>12.启动HDFS和YARN</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#添加环境变量</span></span><br><span class="line">[hadoop@hadoop001 hadoop]$  vi etc/hadoop/hadoop-env.sh</span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/java/jdk1.7.0_79</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop001 hadoop]$ <span class="built_in">which</span> hdfs</span><br><span class="line">/opt/software/hadoop/bin/hdfs</span><br><span class="line">[hadoop@hadoop001 hadoop]$ hdfs namenode -format</span><br><span class="line">........</span><br><span class="line">17/08/20 16:43:19 INFO common.Storage: Storage directory</span><br><span class="line">/tmp/hadoop-hadoop/dfs/name has been successfully formatted.</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop001 hadoop]$ <span class="built_in">which</span> start-dfs.sh</span><br><span class="line">/opt/software/hadoop/sbin/start-dfs.sh</span><br><span class="line">[hadoop@hadoop001 hadoop]$</span><br><span class="line">[hadoop@hadoop001 hadoop]$ start-dfs.sh</span><br><span class="line">[hadoop@hadoop001 hadoop]$ sbin/start-yarn.sh</span><br><span class="line">[hadoop@hadoop001 hadoop]$ sbin/mr-jobhistory-daemon.sh start historyserver</span><br></pre></td></tr></table></figure><p>如果要结束进程，使用如下命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ sbin/stop-yarn.sh</span><br><span class="line">[hadoop@hadoop001 hadoop]$ stop-dfs.sh</span><br><span class="line">[hadoop@hadoop001 hadoop]$ sbin/mr-jobhistory-daemon.sh stop historyserver</span><br></pre></td></tr></table></figure><h2 id="13-检查启动进程"><a href="#13-检查启动进程" class="headerlink" title="13.检查启动进程"></a>13.检查启动进程</h2><p>通过jps命令查看</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ jps</span><br><span class="line">19536 DataNode</span><br><span class="line">19440 NameNode</span><br><span class="line">19876 Jps</span><br><span class="line">19740 SecondaryNameNode</span><br><span class="line">19888 ResourceManager</span><br><span class="line">19345 NodeManager</span><br><span class="line">4613 JobHistoryServer</span><br></pre></td></tr></table></figure><p>通过WebUI查看</p><blockquote><p>Browse the web interface for the NameNode - <a href="http://hadoop001:50070/" target="_blank" rel="noopener"></a><br>Browse the web interface for the ResourceManager - <a href="http://hadoop001:8088/" target="_blank" rel="noopener"></a></p></blockquote><h2 id="14-验证MapReduce"><a href="#14-验证MapReduce" class="headerlink" title="14.验证MapReduce"></a>14.验证MapReduce</h2><p>我们使用hadoop 的mapreduce-examples  jar包来计算PI</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Run a MapReduce job.</span></span><br><span class="line">[hadoop@hadoop001 mapreduce]$ <span class="built_in">pwd</span></span><br><span class="line">/opt/software/hadoop/share/hadoop/mapreduce</span><br><span class="line">[hadoop@hadoop002 mapreduce]$</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop002 mapreduce]$ ll</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop  301740 Aug 20 15:12 hadoop-mapreduce-examples-2.7.4.jar</span><br><span class="line">[hadoop@hadoop002 mapreduce]$ hadoop jar hadoop-mapreduce-examples-2.7.4.jar pi 5 10</span><br></pre></td></tr></table></figure><h2 id="15-参考资料"><a href="#15-参考资料" class="headerlink" title="15.参考资料"></a>15.参考资料</h2><p> <a href="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html" target="_blank" rel="noopener">Setting up a Single Node Cluster：</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文主要记录Hadoop伪分布式（Pseudo-Distributed Mode）部署的详细步骤：&lt;br&gt;伪分布模式在“单节点集群”上运行Hadoop，其中所有的守护进程都运行在同一台机器上。该模式在单机模式之上增加了代码调试功能，允许你检查内存使用情况，HDFS输入输出，
      
    
    </summary>
    
      <category term="大数据" scheme="http://xiejm.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="http://xiejm.com/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hive--安装</title>
    <link href="http://xiejm.com/Hive/Hive--install.html"/>
    <id>http://xiejm.com/Hive/Hive--install.html</id>
    <published>2017-10-02T15:44:44.000Z</published>
    <updated>2018-04-17T04:45:53.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要是Hive部署的详细步骤</p><h2 id="1-基本环境"><a href="#1-基本环境" class="headerlink" title="1.基本环境"></a>1.基本环境</h2><ul><li>CentOS 6.7</li><li>JDK1.8</li><li>Hadoop-2.6.0-cdh5.7.0</li><li>Hive-1.1.0-cdh5.7.0</li></ul><p>Hive是基于Hadoop的，Hadoop的部署本文不在复述。</p><h2 id="2-上传解压"><a href="#2-上传解压" class="headerlink" title="2.上传解压"></a>2.上传解压</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf Hive-1.1.0-cdh5.7.0.tar.gz</span><br></pre></td></tr></table></figure><h2 id="3-配置环境变量"><a href="#3-配置环境变量" class="headerlink" title="3.配置环境变量"></a>3.配置环境变量</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br><span class="line"><span class="comment">#添加下面内容到/etc/profile文件的最后</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#set HIVE</span></span><br><span class="line"><span class="built_in">export</span> HIVE_HOME=/opt/software/hive</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$HIVE_HOME</span>/bin:<span class="variable">$PATH</span></span><br></pre></td></tr></table></figure><p>保存退出后使用命令<code>source /etc/profile</code>更新环境变量</p><h2 id="4-修改hive-env-sh"><a href="#4-修改hive-env-sh" class="headerlink" title="4.修改hive-env.sh"></a>4.修改hive-env.sh</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">//hadoop环境设置</span><br><span class="line">HADOOP_HOME=/opt/software/hadoop</span><br><span class="line">//Hive的控制目录</span><br><span class="line"><span class="built_in">export</span> HIVE_CONF_DIR=/opt/software/hive/conf</span><br></pre></td></tr></table></figure><h2 id="5-修改Hive-site-xml"><a href="#5-修改Hive-site-xml" class="headerlink" title="5.修改Hive-site.xml"></a>5.修改Hive-site.xml</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim <span class="variable">$HIVE_HOME</span>/conf/hive-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=<span class="string">"1.0"</span>?&gt;</span><br><span class="line">&lt;?xml-stylesheet <span class="built_in">type</span>=<span class="string">"text/xsl"</span> href=<span class="string">"configuration.xsl"</span>?&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;!--<span class="comment">######配置MySQL数据库连接串#######--&gt;</span></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;jdbc:mysql://localhost:3306/xiejm_hive?createDatabaseIfNotExist=<span class="literal">true</span>&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;!--连接驱动--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;!--连接用户名--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;root&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;!--连接密码--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;root&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!--仓库路径配置：使用时注意权限，要有写的权限--&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/user/hive/warehouse&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;location of default database <span class="keyword">for</span> the warehouse&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;!--在cli命令行上显示当前数据库，以及查询表的行头信息:<span class="variable">$HIVE_HOME</span>/conf/hive-site.xml--&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.cli.print.header&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;<span class="literal">true</span>&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;Whether to <span class="built_in">print</span> the names of the columns <span class="keyword">in</span> query output.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.cli.print.current.db&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;<span class="literal">true</span>&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;Whether to include the current database <span class="keyword">in</span> the Hive prompt.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h2 id="6-修改日志输出"><a href="#6-修改日志输出" class="headerlink" title="6.修改日志输出"></a>6.修改日志输出</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#copy日志配置文件的模板</span></span><br><span class="line">cp <span class="variable">$HIVE_HOME</span>/conf/conf/hive-log4j.properties.template hive-log4j.properties</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive.log.dir=<span class="variable">$&#123;java.io.tmpdir&#125;</span>/<span class="variable">$&#123;user.name&#125;</span></span><br><span class="line">hive.log.file=hive.log</span><br><span class="line"></span><br><span class="line"><span class="comment">#将上面内容改成下面的</span></span><br><span class="line">hive.log.dir=/opt/software/hive-1.1.0-cdh5.7.0/<span class="built_in">log</span>/</span><br><span class="line">hive.log.file=xjm_hive.log</span><br></pre></td></tr></table></figure><h2 id="7-Hive启动验证"><a href="#7-Hive启动验证" class="headerlink" title="7.Hive启动验证"></a>7.Hive启动验证</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># which hive</span></span><br><span class="line">/opt/software/hive-1.1.0-cdh5.7.0/bin/hive</span><br><span class="line"><span class="comment">###</span></span><br><span class="line">启动前记得加上mysql驱动包</span><br><span class="line"><span class="comment">###</span></span><br><span class="line">//启动hive</span><br><span class="line">[root@hadoop001 ~]<span class="comment"># hive</span></span><br><span class="line">Logging initialized using configuration <span class="keyword">in</span> file:/opt/software/hive-1.1.0-cdh5.7.0/conf/hive-log4j.properties</span><br><span class="line">WARNING: Hive CLI is deprecated and migration to Beeline is recommended.</span><br><span class="line">//创建表emp</span><br><span class="line">create table emp(</span><br><span class="line">empno int,ename string,job string,mgr int,hiredate string,sal double,deptno int</span><br><span class="line">)row format delimited fields terminated by <span class="string">'\t'</span>;</span><br><span class="line">//创建表加载数据</span><br><span class="line">load data <span class="built_in">local</span> inpath <span class="string">"/opt/emp.txt"</span> into table pepole;</span><br><span class="line"></span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure><p>至此安装结束</p><h2 id="8-Hive的基本操作"><a href="#8-Hive的基本操作" class="headerlink" title="8.Hive的基本操作"></a>8.Hive的基本操作</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看数据库</span></span><br><span class="line">show databases;</span><br><span class="line"><span class="comment">#使用数据库</span></span><br><span class="line">use datbase_name;</span><br><span class="line"><span class="comment">#查看表</span></span><br><span class="line">show tables;</span><br><span class="line"><span class="comment">#创建数据库</span></span><br><span class="line">create database datbase_name ;</span><br><span class="line"><span class="comment">#创建表</span></span><br><span class="line">create table pepole(pid int,name string,phone string) ROW FORMAT DELIMITED FIELDS TERMINATED BY <span class="string">','</span>;</span><br><span class="line"><span class="comment">#加载HDFS数据到表</span></span><br><span class="line">load data inpath <span class="string">'/user/hive/warehouse/pepole/customers.csv'</span> into table pepole ;</span><br><span class="line"><span class="comment">#加载本地数据表</span></span><br><span class="line">load data <span class="built_in">local</span> inpath <span class="string">"/opt/datas/customers.csv"</span> into table pepole;</span><br><span class="line"><span class="comment">#查询表</span></span><br><span class="line">select * from table-name ;</span><br><span class="line"><span class="comment">#查看表属性</span></span><br><span class="line">desc table-name;</span><br><span class="line"><span class="comment">#查看表详细属性</span></span><br><span class="line">desc extended table-name ;</span><br><span class="line"><span class="comment">#查看格式化表属性</span></span><br><span class="line">desc formatted table-name ;</span><br><span class="line"><span class="comment">#查看函数</span></span><br><span class="line">show <span class="built_in">functions</span> ;</span><br><span class="line"><span class="comment">#查看函数使用</span></span><br><span class="line">desc <span class="keyword">function</span> fun-name ;</span><br><span class="line"><span class="comment">#查看函数详细属性</span></span><br><span class="line">desc <span class="keyword">function</span> extended upper ;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文主要是Hive部署的详细步骤&lt;/p&gt;
&lt;h2 id=&quot;1-基本环境&quot;&gt;&lt;a href=&quot;#1-基本环境&quot; class=&quot;headerlink&quot; title=&quot;1.基本环境&quot;&gt;&lt;/a&gt;1.基本环境&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;CentOS 6.7&lt;/li&gt;
&lt;li&gt;JDK1
      
    
    </summary>
    
      <category term="大数据" scheme="http://xiejm.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hive" scheme="http://xiejm.com/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop--HDFS架构</title>
    <link href="http://xiejm.com/Hadoop/Hadoop--HDFSdesign.html"/>
    <id>http://xiejm.com/Hadoop/Hadoop--HDFSdesign.html</id>
    <published>2017-10-02T15:42:22.000Z</published>
    <updated>2018-04-17T04:46:09.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要介绍HDFS架构概要<br><a id="more"></a><br><strong>Hadoop分布式文件系统</strong>(<code>HDFS</code>)是一个高度容错性的系统，适合部署在廉价的机器上。HDFS提供了高吞吐量的数据访问，适合大规模数据集的应用。<strong>HDFS采用master/slave的主从架构</strong>，HDFS集群是由<strong>一个NameNode</strong>()和<strong>一定数目的DataNode</strong>组成。</p><h2 id="1-设计前提和目标"><a href="#1-设计前提和目标" class="headerlink" title="1.设计前提和目标"></a>1.设计前提和目标</h2><ol><li><strong>专为存储超大文件而设计</strong>：hdfs应该能够支持GB级别大小的文件；它应该能够提供很大的数据带宽并且能够在集群中拓展到成百上千个节点；它的一个实例应该能够支持千万数量级别的文件</li><li><strong>适用于流式的数据访问</strong>：hdfs适用于批处理的情况而不是交互式处理；它的重点是保证高吞吐量而不是低延迟的用户响应</li><li><strong>容错性</strong>：完善的冗余备份机制</li><li><strong>支持简单的一致性模型</strong>：HDFS需要支持<code>一次写入多次读取</code>的模型，而且写入过程文件不会经常变化</li><li><strong>移动计算优于移动数据</strong>：HDFS提供了使应用计算移动到离它最近数据位置的接口</li><li><strong>兼容各种硬件和软件平台</strong></li></ol><h2 id="2-不适合的场景"><a href="#2-不适合的场景" class="headerlink" title="2.不适合的场景"></a>2.不适合的场景</h2><ol><li><strong>大量小文件</strong>：文件的元数据都存储在NameNode内存中，大量小文件会占用大量内存。</li><li><strong>低延迟数据访问</strong>：hdfs是专门针对高数据吞吐量而设计的</li><li><strong>多用户写入，任意修改文件</strong></li></ol><h2 id="3-HDFS架构和设计："><a href="#3-HDFS架构和设计：" class="headerlink" title="3. HDFS架构和设计："></a>3. HDFS架构和设计：</h2><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/images/hdfsarchitecture.png" alt="HDFS架构" title="">                </div>                <div class="image-caption">HDFS架构</div>            </figure>  <p>HDFS主要由3个组件构成，分别是NameNode、SecondaryNameNode和DataNode：</p><ol><li>NameNode<ul><li>NameNode是整个文件系统的管理节点；</li><li>它维护着整个文件系统的文件及目录，以及接收HDFS Client的操作请求；</li><li>NameNode 只有三种交互。<ul><li>client访问NameNode获取相关DataNode信息。</li><li>DataNode心跳汇报当前block情况。</li><li>SecondaryNameNode做checkpoint交互。</li></ul></li></ul></li><li>DataNode</li></ol><ul><li>提供真实文件数据的存储服务</li><li>文件块（block）：最基本的存储单位。<strong>HDFS默认Block大小是64MB(1.0版本),128(2.0版本)</strong>，如果一个文件小于一个数据块的大小，HDFS并不占用整个数据块存储空间。<br>Replication：多复本，默认是三个。</li></ul><ol><li>SecondaryNameNode</li></ol><ul><li>HA的一个解决方案。但不支持热备。</li><li><strong>执行过程</strong>：<strong>从NameNode上下载元数据信息（fsimage,edits），然后把二者合并，生成新的fsimage，在本地保存，并将其推送到NameNode，同时重置NameNode的edits.</strong>（默认在安装在NameNode节点上，但这样…不安全！）</li></ul><p>参考资料：<br><a href="hadoop.apache.org">Hadoop官网</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要介绍HDFS架构概要&lt;br&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://xiejm.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="http://xiejm.com/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Python 基础</title>
    <link href="http://xiejm.com/Python/Python-Base.html"/>
    <id>http://xiejm.com/Python/Python-Base.html</id>
    <published>2017-09-14T02:04:14.000Z</published>
    <updated>2017-09-14T02:04:14.000Z</updated>
    
    <content type="html"><![CDATA[<p>Python 基础：定义函数、常用数据结构、条件判断语句、for循环、字符串处理、异常处理、模块引入<br><a id="more"></a></p><h1 id="1-定义函数："><a href="#1-定义函数：" class="headerlink" title="1. 定义函数："></a>1. 定义函数：</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#函数定义格式：def 函数名(参数)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_your_name</span><span class="params">(name)</span>:</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">"badou hadoop study!, your name"</span>, name</span><br><span class="line">    <span class="keyword">return</span> (<span class="string">"%s, is good student"</span> % name)</span><br><span class="line"></span><br><span class="line"><span class="comment">#函数调用格式： 函数名(参数)</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'================='</span></span><br><span class="line"><span class="keyword">print</span> print_your_name(<span class="string">'zhangsan'</span>)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'================='</span></span><br><span class="line"><span class="keyword">print</span> print_your_name(<span class="string">'lisi'</span>)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'================='</span></span><br></pre></td></tr></table></figure><hr><h1 id="2-常用数据结构"><a href="#2-常用数据结构" class="headerlink" title="2.常用数据结构"></a>2.常用数据结构</h1><p>哈希表：hashmap,数组：array,集合：set</p><p>python中：</p><ul><li>hashmap -&gt; dict</li><li>array -&gt; list</li><li>set -&gt; set</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#字典代码演示</span></span><br><span class="line"><span class="comment">#dict</span></span><br><span class="line"> color = &#123;<span class="string">"red"</span>:<span class="number">0.2</span>,<span class="string">"green"</span>:<span class="number">0.4</span>,<span class="string">"blue"</span>:<span class="number">0.4</span>&#125;</span><br><span class="line"> </span><br><span class="line"> <span class="keyword">print</span> color [<span class="string">'red'</span>]</span><br><span class="line"> <span class="keyword">print</span> color [<span class="string">'green'</span>]</span><br><span class="line"> </span><br><span class="line"> <span class="comment">#输出结果</span></span><br><span class="line"> <span class="number">0.2</span></span><br><span class="line"> <span class="number">0.4</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#数组代码演示</span></span><br><span class="line"><span class="comment">#list</span></span><br><span class="line">color_list = [<span class="string">'red'</span>,<span class="string">'blue'</span>,<span class="string">'green'</span>,<span class="string">'yellow'</span>]</span><br><span class="line"><span class="keyword">print</span> color_list[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#输出结果</span></span><br><span class="line">blue</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#集合代码演示</span></span><br><span class="line"><span class="comment">#set</span></span><br><span class="line"><span class="comment">#使用前需声明</span></span><br><span class="line">a_set = set()</span><br><span class="line">a_set.add(<span class="string">'111'</span>)</span><br><span class="line">a_set.add(<span class="string">'222'</span>)</span><br><span class="line">a_set.add(<span class="string">'333'</span>)</span><br><span class="line">a_set.add(<span class="string">'111'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> a_set</span><br><span class="line"></span><br><span class="line"><span class="comment">#输出结果</span></span><br><span class="line">set([<span class="string">'111'</span>,<span class="string">'222'</span>,<span class="string">'333'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#set()的特性：在元素内不会出现重复元素</span></span><br></pre></td></tr></table></figure><hr><h1 id="3-条件判断语句"><a href="#3-条件判断语句" class="headerlink" title="3.条件判断语句"></a>3.条件判断语句</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#if</span></span><br><span class="line"></span><br><span class="line">a=<span class="number">1</span></span><br><span class="line"><span class="keyword">if</span> a &gt; <span class="number">0</span>:</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'a gt 0'</span></span><br><span class="line"><span class="keyword">elif</span> a == <span class="number">0</span>:</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'a eq 0'</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'a lt 0'</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># gt: 大于，greater than缩写</span></span><br><span class="line"><span class="comment"># eq: 等于，equality 缩写</span></span><br><span class="line"><span class="comment"># lt: 小于，less-than 缩写</span></span><br></pre></td></tr></table></figure><hr><h1 id="4-for循环"><a href="#4-for循环" class="headerlink" title="4.for循环"></a>4.for循环</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#for</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#数组</span></span><br><span class="line">a_list = []</span><br><span class="line">a_list.append[<span class="string">'111'</span>]</span><br><span class="line">a_list.append[<span class="string">'222'</span>]</span><br><span class="line">a_list.append[<span class="string">'333'</span>]</span><br><span class="line">a_list.append[<span class="string">'444'</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> value <span class="keyword">in</span> a_list:</span><br><span class="line">    <span class="keyword">print</span> value</span><br><span class="line">    </span><br><span class="line"><span class="comment">#输出结果</span></span><br><span class="line"><span class="number">111</span></span><br><span class="line"><span class="number">222</span></span><br><span class="line"><span class="number">333</span></span><br><span class="line"><span class="number">444</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#集合 set</span></span><br><span class="line">a_set = set()</span><br><span class="line">a_set.add(<span class="string">'a'</span>)</span><br><span class="line">a_set.add(<span class="string">'b'</span>)</span><br><span class="line">a_set.add(<span class="string">'d'</span>)</span><br><span class="line">a_set.add(<span class="string">'d'</span>)</span><br><span class="line">a_set.add(<span class="string">'e'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> value <span class="keyword">in</span> a_set:</span><br><span class="line">    <span class="keyword">print</span> value</span><br><span class="line">    </span><br><span class="line"><span class="comment">#输出结果： set内部通过hash排序，所以输出顺序与追加的item顺序不一致</span></span><br><span class="line"></span><br><span class="line">a</span><br><span class="line">c</span><br><span class="line">b</span><br><span class="line">e</span><br><span class="line">d</span><br><span class="line"></span><br><span class="line"><span class="comment">#字典 dict</span></span><br><span class="line">b_dict = &#123;&#125;</span><br><span class="line">b_dict[<span class="string">'aaa'</span>] = <span class="number">1</span></span><br><span class="line">b_dict[<span class="string">'bbb'</span>] = <span class="number">2</span></span><br><span class="line">b_dict[<span class="string">'ccc'</span>] = <span class="number">3</span></span><br><span class="line">b_dict[<span class="string">'ddd'</span>] = <span class="number">4</span></span><br><span class="line">b_dict[<span class="string">'eee'</span>] = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#字段 读取需要用到key,value</span></span><br><span class="line"><span class="keyword">for</span> key, value int b_dict.items():</span><br><span class="line">    <span class="keyword">print</span> key + <span class="string">" ===&gt;"</span> + str(value)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#输出结果 :字典内部顺序通过hashmap排序</span></span><br><span class="line">eee ===&gt; 5</span><br><span class="line">aaa ===&gt; 1</span><br><span class="line">bbb ===&gt; 2</span><br><span class="line">ccc ===&gt; 3</span><br><span class="line">ddd ===&gt; 4</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sum = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> value <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">11</span>):</span><br><span class="line">    sum += value</span><br><span class="line"><span class="keyword">print</span> sum</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#while</span></span><br><span class="line"></span><br><span class="line">cnt = <span class="number">2</span></span><br><span class="line"><span class="keyword">while</span> cnt &gt; <span class="number">0</span>:</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'i love python'</span></span><br><span class="line">    cnt -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># print dual</span></span><br><span class="line">i = <span class="number">1</span></span><br><span class="line"><span class="keyword">while</span> i &lt; <span class="number">10</span>:</span><br><span class="line">    i += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">2</span> &gt; <span class="number">0</span>:</span><br><span class="line"><span class="keyword">continue</span> </span><br><span class="line">    <span class="keyword">print</span> i</span><br></pre></td></tr></table></figure><hr><h2 id="4-字符串处理"><a href="#4-字符串处理" class="headerlink" title="4.字符串处理"></a>4.字符串处理</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#string</span></span><br><span class="line"></span><br><span class="line">str = <span class="string">'qwert'</span></span><br><span class="line"><span class="keyword">print</span> len(str)</span><br><span class="line"></span><br><span class="line"><span class="comment">#输出第二到第五个字符</span></span><br><span class="line"><span class="comment">#string 取字符</span></span><br><span class="line"><span class="keyword">print</span> str[<span class="number">2</span>:<span class="number">5</span>]</span><br><span class="line"></span><br><span class="line">str <span class="string">'AbcDEf'</span></span><br><span class="line"><span class="comment">#全部字符转换为小写</span></span><br><span class="line"><span class="keyword">print</span> str.lower()</span><br></pre></td></tr></table></figure><hr><h2 id="5-异常处理"><a href="#5-异常处理" class="headerlink" title="5.异常处理"></a>5.异常处理</h2><hr><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#exception</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#try... catch</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#try:</span></span><br><span class="line">a=<span class="number">6</span></span><br><span class="line">b=a/<span class="number">0</span></span><br><span class="line">exception Exception e:</span><br><span class="line">    <span class="keyword">print</span> Exception,<span class="string">":"</span> e</span><br></pre></td></tr></table></figure><hr><h2 id="6-模块引入"><a href="#6-模块引入" class="headerlink" title="6.模块引入"></a>6.模块引入</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math <span class="comment">#引入模块</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> math.pow(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"><span class="keyword">print</span> round(<span class="number">4</span>,<span class="number">9</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line">item = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]</span><br><span class="line">random.shuffle(items)</span><br><span class="line"><span class="keyword">print</span> items</span><br><span class="line"></span><br><span class="line">a = random.randint(<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line"><span class="keyword">print</span> a</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">s_list = random.sample(<span class="string">'abcdefg'</span>,<span class="number">3</span>)</span><br><span class="line"><span class="keyword">print</span> s_list</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Python 基础：定义函数、常用数据结构、条件判断语句、for循环、字符串处理、异常处理、模块引入&lt;br&gt;
    
    </summary>
    
      <category term="技术" scheme="http://xiejm.com/categories/tech/"/>
    
    
      <category term="Python" scheme="http://xiejm.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop--HDFS之机架感知和副本策略</title>
    <link href="http://xiejm.com/Hadoop/Hadoop--HDFSreplication.html"/>
    <id>http://xiejm.com/Hadoop/Hadoop--HDFSreplication.html</id>
    <published>2017-09-14T02:02:32.000Z</published>
    <updated>2018-04-17T04:46:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要介绍Hadoop HDFS的机架感知和副本策略<br><a id="more"></a></p><p>其实NameNode在挑选合适的DataNode去存储Block的时候，不仅仅考虑了DataNode的存储空间够不够，还会考虑这些DataNode在不在同一个机架上。</p><p>这就需要NameNode必须知道所有的DataNode分别位于哪个机架上(所以也称为机架感知)。</p><p>当然，默认情况下NameNode是不会知道机架的存在的，也就是说，默认情况下，NameNode会认为所有的DataNode都在同一个机架上(/defaultRack)。</p><p>除非我们在hdfs-site.xml里面配置topology.script.file.name选项，这个选项的值是一个可执行文件的位置，而该只执行文件的作用是将输入的DataNode的ip地址按照一定规则计算，然后输出它所在的机架的名字，如/rack1, /rack2之类。借助这个文件，NameNode就具备了机架感知了。当它在挑选DataNode去存储Block的时候，它会遵循以下原则：</p><ol><li><p>首先挑选跟HDFS Client所在的DataNode作为存放第一个Block副本的位置，如果HDFS Client不在任何一个DataNode上，比如说Hadoop集群外你自己的电脑，那么就任意选取一个DataNode。</p></li><li><p>其次，会借助NameNode的机架感知特性，选取跟第一个Block副本所在DataNode不同的机架上的任意一个DataNode来存放Block的第二个副本，比如说/rack2。Block的第三个副本也会存在这个/rack2上，但是是另外一个DataNode。</p></li><li><p>最后，如果我们设置的副本的数量大于3，那么剩下的副本则随意存储在集群中。</p></li></ol><p>我们把上面的描述简单说下：</p><ul><li><strong>1st replica</strong>. 如果写请求方所在机器是其中一个个DataNode,则直接存放在本地,否则随机在集群中选择一个DataNode。</li><li><strong>2nd replica</strong>. 第二个副本存放于不同第一个副本的所在的机架。</li><li><strong>3rd replica</strong>.第三个副本存放于第二个副本所在的机架,但是属于不同的节点。<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://wpxiejm.oss-cn-beijing.aliyuncs.com/Note/hadoop/201709062251.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要介绍Hadoop HDFS的机架感知和副本策略&lt;br&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://xiejm.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="http://xiejm.com/tags/Hadoop/"/>
    
      <category term="HDFS" scheme="http://xiejm.com/tags/HDFS/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop--HDFS读写流程</title>
    <link href="http://xiejm.com/Hadoop/Hadoop--HDFSReadAndWrite.html"/>
    <id>http://xiejm.com/Hadoop/Hadoop--HDFSReadAndWrite.html</id>
    <published>2017-09-13T13:57:06.000Z</published>
    <updated>2018-04-17T04:46:15.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要记录Hadoop–HDFS读写流程的详细步骤：<br><a id="more"></a><br>HDFS的设计是一次写入，多次读取，并且不支持文件修改，所以文件的读写就变的相当重要，下面我们就来一起看看HDFS文件的读写流程！</p><h2 id="1-文件写入流程"><a href="#1-文件写入流程" class="headerlink" title="1. 文件写入流程"></a>1. 文件写入流程</h2><ul><li><strong>HDFS的文件写入流程</strong>可以分为两个部分：</li></ul><ol><li>准备工作，包括与NameNode的通信等；</li><li>真正的写入操作。</li></ol><p><strong>我们先来看看第一部分的流程：建立连接</strong><br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://wpxiejm.oss-cn-beijing.aliyuncs.com/Note/hadoop/20160412195903.png" alt="写流程的第一阶段" title="">                </div>                <div class="image-caption">写流程的第一阶段</div>            </figure></p><ol><li>首先<strong>HDFS Client会去询问NameNode</strong>有哪些DataNode可以存储文件。</li><li><strong>HDFS Client会把文件拆分</strong>，如果上图中文件被拆分成<code>A</code>、<code>B</code>、<code>C</code>三个Block</li><li><strong>NameNode通过查询它的元数据信息</strong>，发现DataNode<code>1</code>、<code>2</code>、<code>7</code>上有空间可以存储Block A，于是将此信息告诉HDFS Client。</li><li><strong>HDFS Client接到NameNode返回的DataNode列表信息后</strong>，会直接与列表中的<strong>第一个DataNode联系</strong>，让它准备好接收<strong>Block A</strong>（建立TCP连接）<strong>并把NameNode返回的DataNode List信息发送给DataNode1</strong></li><li>DataNode1与Client建立好TCP连接后，它会把HDFS Client要写入<strong>Block A</strong>的请求按顺序发送给DataNode2。同理，传递给DataNode7。</li><li>当DataNode7准备好后，会依次<strong>回传准备好接收Block A的反馈信息</strong>。</li><li><strong>HDFS Client接收到反馈信息后，表示都准备好了</strong>，就可以写数据了。</li></ol><br><p><strong>写入数据</strong><br>下图展示了HDFS Client如何写入数据的流程<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://wpxiejm.oss-cn-beijing.aliyuncs.com/Note/hadoop/201709061619.png" alt="写流程的第二阶段" title="">                </div>                <div class="image-caption">写流程的第二阶段</div>            </figure></p><ol><li><strong>HDFS Client开始往DataNode1写入Block A数据</strong>。与第一部分的建立连接一样，当DataNode1接收完Block A数据后，它会按顺序将Block A数据传输给DataNode2，然后DataNode2再传输给DataNode7。</li><li>每个<strong>DataNode</strong>在接收完Block A数据后，会发消息给<strong>NameNode</strong>，告诉它Block数据已经接收完毕。</li><li><strong>NameNode</strong>同时会根据它接收到的消息更新它保存的文件系统元数据信息。</li><li>当<strong>Block A成功写入3个DataNode之后，DataNode1会发送一个成功信息给HDFS Client</strong>，同时<strong>HDFS Client也会发一个Block A成功写入的信息给NameNode</strong>。之后，HDFS Client才能开始继续处理下一个Block-Block B。</li></ol><h2 id="2-读取文件流程"><a href="#2-读取文件流程" class="headerlink" title="2. 读取文件流程"></a>2. 读取文件流程</h2><p>现在，我们来看看HDFS Client是如何从DataNode读取数据的。</p><p><img src="http://wpxiejm.oss-cn-beijing.aliyuncs.com/Note/hadoop/201709061633.png" alt=""></p><p>如上图所示：</p><ol><li>首先<strong>HDFS Client会去联系NameNode</strong>，询问file.txt总共有<strong>几个Block</strong>，这些Block<strong>存放在哪些DataNode上</strong>。</li><li>。由于每个Block都会存在几个<strong>副本</strong>，所以NameNode会把file.txt文件组成的Block所对应的<strong>所有DataNode列表</strong>都返回给HDFS Client。</li><li>然后<strong>HDFS Client会选择DataNode列表里的第一个DataNode去读取对应的Block</strong>，比如由于Block A存储在DataNode<code>1</code>，<code>2</code>，<code>7</code>，那么HDFS Client会到DataNode1去读取Block A；Block C存储在DataNode<code>7</code>，<code>8</code>，<code>9</code>，那么HDFS Client就回到DataNode7去读取Block C。</li><li>全部读完后关闭连接</li></ol><p>参考资料：</p><p><a href="http://www.jianshu.com/p/7e52a7f8d16d" target="_blank" rel="noopener">http://www.jianshu.com/p/7e52a7f8d16d</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要记录Hadoop–HDFS读写流程的详细步骤：&lt;br&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://xiejm.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="http://xiejm.com/tags/Hadoop/"/>
    
      <category term="HDFS" scheme="http://xiejm.com/tags/HDFS/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop-2.6.0-cdh5.7.0-native库缺失的问题</title>
    <link href="http://xiejm.com/Hadoop/Hadoop-2.6.0-cdh5.7.0-native.html"/>
    <id>http://xiejm.com/Hadoop/Hadoop-2.6.0-cdh5.7.0-native.html</id>
    <published>2017-09-13T12:25:44.000Z</published>
    <updated>2018-04-17T04:46:21.000Z</updated>
    
    <content type="html"><![CDATA[<p>在cloudera下载的 hadoop-2.6.0-cdh5.7.0，/lib/native 的静态库文件不存在<br><a id="more"></a></p><h2 id="发现过程"><a href="#发现过程" class="headerlink" title="发现过程"></a>发现过程</h2><h6 id="执行hadoop命令的时候会出现如下警告信息"><a href="#执行hadoop命令的时候会出现如下警告信息" class="headerlink" title="执行hadoop命令的时候会出现如下警告信息"></a>执行hadoop命令的时候会出现如下警告信息</h6><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">12/09/17 10:46:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using <span class="built_in">builtin</span>-java classes <span class="built_in">where</span> applicable</span><br></pre></td></tr></table></figure><h6 id="开启调试模式"><a href="#开启调试模式" class="headerlink" title="开启调试模式"></a>开启调试模式</h6><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HADOOP_ROOT_LOGGER=DEBUG,console</span><br></pre></td></tr></table></figure><h6 id="再次执行hadoop命令的时候出现如下信息"><a href="#再次执行hadoop命令的时候出现如下信息" class="headerlink" title="再次执行hadoop命令的时候出现如下信息"></a>再次执行hadoop命令的时候出现如下信息</h6><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">12/09/17 16:46:48 DEBUG util.NativeCodeLoader: Trying to load the custom-built native-hadoop library...</span><br><span class="line">12/09/17 16:46:48 DEBUG util.NativeCodeLoader: Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop <span class="keyword">in</span> java.library.path</span><br><span class="line">12/09/17 16:46:48 DEBUG util.NativeCodeLoader: java.library.path=/opt/software/hadoop-2.6.0-cdh5.7.0/lib/native</span><br><span class="line">12/09/17 16:46:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using <span class="built_in">builtin</span>-java classes <span class="built_in">where</span> applicable</span><br><span class="line">12/09/17 16:46:48 DEBUG util.PerformanceAdvisory: Falling back to shell based</span><br></pre></td></tr></table></figure><h6 id="发现hadoop的native本地库文件不存在"><a href="#发现hadoop的native本地库文件不存在" class="headerlink" title="发现hadoop的native本地库文件不存在"></a>发现hadoop的native本地库文件不存在</h6><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /opt/software/hadoop-2.6.0-cdh5.7.0/lib/native</span><br></pre></td></tr></table></figure><h2 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h2><p>下载<code>hadoop-2.6.0-cdh5.7.0-src.tar.gz</code>源码包，本地编译。具体编译流程详见本博的<a href="http://note.youdao.com/" target="_blank" rel="noopener">Hadoop源码文章</a><br>编译完成后hadoop-2.6.0-cdh5.7.0-src/hadoop-dist/target/hadoop-2.6.0-cdh5.7.0/lib/native，里面有lib/native本地库文件，复制过去就可以了。</p><p>你也可以从我编译好的文件中下载，链接：<a href="http://pan.baidu.com/s/1bDtxXO" target="_blank" rel="noopener">http://pan.baidu.com/s/1bDtxXO</a> 密码：8fif</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在cloudera下载的 hadoop-2.6.0-cdh5.7.0，/lib/native 的静态库文件不存在&lt;br&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://xiejm.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="http://xiejm.com/tags/Hadoop/"/>
    
      <category term="native" scheme="http://xiejm.com/tags/native/"/>
    
  </entry>
  
  <entry>
    <title>Spark2.2基于CentOS编译</title>
    <link href="http://xiejm.com/Spark/SparkBuildOnCentOS.html"/>
    <id>http://xiejm.com/Spark/SparkBuildOnCentOS.html</id>
    <published>2017-09-12T03:04:36.000Z</published>
    <updated>2018-04-17T04:45:11.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要介绍Spark2.2的源码编译详细步骤。<br>我们在生产环境中，Spark的版本并一定和Hadoop集群匹配，这样我们就需要编译Spark<br><a id="more"></a></p><h2 id="1-基本环境要求"><a href="#1-基本环境要求" class="headerlink" title="1.基本环境要求"></a>1.基本环境要求</h2><p><a href="http://spark.apache.org/docs/2.1.0/building-spark.html" target="_blank" rel="noopener"> Spar编译的官方文档 </a> 中介绍可知Spark2.2需要JDK1.8+,Maven 3.3.9</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">The Maven-based build is the build of reference for Apache Spark. Building Spark using Maven requires Maven 3.3.9 or newer and Java 8+. Note that support for Java 7 was removed as of Spark 2.2.0.</span><br></pre></td></tr></table></figure><p>编译所需环境如下：</p><ul><li>CentOS 6.7</li><li>JDK1.8+</li><li>Maven 3.3.9</li><li>Scala 2.11</li><li>Git</li></ul><h2 id="2-编译环境安装"><a href="#2-编译环境安装" class="headerlink" title="2.编译环境安装"></a>2.编译环境安装</h2><h4 id="JDK安装"><a href="#JDK安装" class="headerlink" title="JDK安装"></a>JDK安装</h4><p>参考本博客的JDK安装的文章:<a href="http://xiejm.com/2017/08/31/CentOS_setup_JDK.html">JDK安装</a></p><h4 id="Maven安装"><a href="#Maven安装" class="headerlink" title="Maven安装"></a>Maven安装</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 software]<span class="comment"># mkdir -p /opt/software</span></span><br><span class="line">[root@hadoop001 software]<span class="comment"># cd /opt/software</span></span><br><span class="line"><span class="comment">#上传apache-maven-3.3.9-bin.zip</span></span><br><span class="line">[root@hadoop001 software]<span class="comment"># unzip apache-maven-3.3.9-bin.zip</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#添加maven环境变量</span></span><br><span class="line">[root@hadoop001 software]<span class="comment"># echo '#SET MAVEN'&gt;&gt;/etc/profile</span></span><br><span class="line">[root@hadoop001 software]<span class="comment"># echo 'export MAVEN_HOME=/opt/software/apache-maven-3.3.9' &gt;&gt;/etc/profile</span></span><br><span class="line">[root@hadoop001 software]<span class="comment"># echo 'export MAVEN_OPTS="-Xmx2g -XX:ReservedCodeCacheSize=512m"'&gt;&gt;/etc/profile</span></span><br><span class="line">[root@hadoop001 software]<span class="comment"># echo 'export PATH=$MAVEN_HOME/bin:$PATH'&gt;&gt;/etc/profile</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 让配置文件生效</span></span><br><span class="line">[root@hadoop001 software]<span class="comment"># source /etc/profile</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#执行mvn 查看版本，如果安装成功会输出Maven版本号</span></span><br><span class="line">[root@hadoop001 software]<span class="comment"># mvn --version</span></span><br></pre></td></tr></table></figure><h4 id="Scala安装"><a href="#Scala安装" class="headerlink" title="Scala安装"></a>Scala安装</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#解压scala-2.11.11</span></span><br><span class="line">[root@hadoop001 software]<span class="comment"># tar -zxvf scala-2.11.11.tgz</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#添加Scala环境变量</span></span><br><span class="line">[root@hadoop001 software]<span class="comment"># echo '#SET SCALA'&gt;&gt;/etc/profile</span></span><br><span class="line">[root@hadoop001 software]<span class="comment"># echo 'export SCALA_HOME=/opt/software/scala-2.11.11' &gt;&gt;/etc/profile</span></span><br><span class="line">[root@hadoop001 software]<span class="comment"># echo 'export PATH=$SCALA_HOME/bin:$PATH'&gt;&gt;/etc/profile</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 让配置文件生效</span></span><br><span class="line">[root@hadoop001 software]<span class="comment"># source /etc/profile</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 验证Scala</span></span><br><span class="line"></span><br><span class="line">[root@hadoop001 software]<span class="comment"># scala -version</span></span><br><span class="line">Scala code runner version 2.11.11 -- Copyright 2002-2017, LAMP/EPFL</span><br></pre></td></tr></table></figure><h4 id="Git安装"><a href="#Git安装" class="headerlink" title="Git安装"></a>Git安装</h4><p>使用YUM命令安装git<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 software]<span class="comment"># yum install -y git</span></span><br></pre></td></tr></table></figure></p><h2 id="3-源码包下载"><a href="#3-源码包下载" class="headerlink" title="3.源码包下载"></a>3.源码包下载</h2><p>从Spark官网下载源码包：<a href="http://spark.apache.org/downloads.html" target="_blank" rel="noopener">http://spark.apache.org/downloads.html</a></p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://wpxiejm.oss-cn-beijing.aliyuncs.com/Note/spark/20170912101958.png" alt="image" title="">                </div>                <div class="image-caption">image</div>            </figure><p>下载后将源码包上传至CentOS的<code>/opt/sourcecode</code><br>解压源码包<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 sourcecode]<span class="comment"># tar -zxvf spark-2.2.0.tgz</span></span><br></pre></td></tr></table></figure></p><h2 id="4-Maven相关配置"><a href="#4-Maven相关配置" class="headerlink" title="4.Maven相关配置"></a>4.Maven相关配置</h2><p>我们采用CDH版本Hadoop，所以需要修改Spark源码目录下的pom.xml文件，在<code>&lt;repositories&gt;&lt;/repositories&gt;</code>标签对内添加以下内容<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">id</span>&gt;</span>cdh<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>cloudera repository<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">url</span>&gt;</span>https://repository.cloudera.com/artifactory/cloudera-repos/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>因Maven中心库在墙外，下载依赖包时会出现地址解析失败的情况，这里加上一个阿里云的Maven仓库来解决此问题<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">id</span>&gt;</span>alimaven<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>aliyun maven<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://maven.aliyun.com/nexus/content/groups/public/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p><code>make-distribution.sh</code>编译脚本执行时会检查系统Scala的版本，这个过程非常占时间，用vim命令把该脚本的120-133行注释掉，再添加以下内容：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">VERSION=2.2.0</span><br><span class="line">134 SCALA_VERSION=2.11</span><br><span class="line">135 SPARK_HADOOP_VERSION=2.6.0-cdh5.7.0</span><br><span class="line">136 SPARK_HIVE=1</span><br></pre></td></tr></table></figure></p><h2 id="4-开始编译"><a href="#4-开始编译" class="headerlink" title="4.开始编译"></a>4.开始编译</h2><p>执行以下脚本：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./dev/make-distribution.sh --name 2.6.0-cdh5.7.0 --tgz  -Pyarn -Phadoop-2.6 -Phive -Phive-thriftserver -Dhadoop.version=2.6.0-cdh5.7.0</span><br></pre></td></tr></table></figure></p><p><strong>参数说明</strong></p><ul><li>-name:指定Spark编译成功后的安装包名称</li><li>-tgz:指定tgz压缩方式</li><li>-Pyarn：Spark on yarn</li><li>-Phadoop-2.6:指定Hadoop版本</li><li>-Phive 和 -Phive-thriftserver：使支持Hive</li><li>-Dhadoop.version=2.6.0-cdh5.7.0：指定Hadoop版本号</li></ul><p>编译过程耗时很长，等编译成功后会在Spark源码目录下生成一个<code>spark-2.2.0-bin-2.6.0-cdh5.7.0.tgz</code>文件</p><p>参考资料： <a href="http://spark.apache.org/docs/latest/building-spark.html" target="_blank" rel="noopener">Spark官网文档</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要介绍Spark2.2的源码编译详细步骤。&lt;br&gt;我们在生产环境中，Spark的版本并一定和Hadoop集群匹配，这样我们就需要编译Spark&lt;br&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://xiejm.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="http://xiejm.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>CentOS 安装JDK常用的两种方法</title>
    <link href="http://xiejm.com/Linux/CentOS_setup_JDK.html"/>
    <id>http://xiejm.com/Linux/CentOS_setup_JDK.html</id>
    <published>2017-09-12T00:17:16.000Z</published>
    <updated>2018-04-17T04:43:52.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要记录在CentOS6.x中JDK的安装方法和详细步骤。<br>JDK的安装方式一般有两种RPM包安装和tar包安装<br>使用JDK1.7.79做示例<br><a id="more"></a></p><h2 id="一-tar包安装"><a href="#一-tar包安装" class="headerlink" title="一.tar包安装"></a>一.tar包安装</h2><p>通过rz命令上传JDK的tar包，或者使用wget命令下载</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.在/usr/目录下创建java目录</span></span><br><span class="line">[root@localhost ~]<span class="comment"># mkdir -p /usr/java</span></span><br><span class="line">[root@localhost ~]<span class="comment"># cd /usr/java</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#2.下载jdk,然后解压</span></span><br><span class="line">[root@localhost java]<span class="comment"># wget http://download.Oracle.com/otn-pub/java/jdk/7u79-b15/jdk-7u79-linux-x64.tar.gz </span></span><br><span class="line">[root@localhost java]<span class="comment"># tar -zxvf jdk-7u79-linux-x64.tar.gz</span></span><br><span class="line">[root@localhost java]<span class="comment">#chown -R root:root jdk1.7.0_79</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#3.设置JDK环境变量</span></span><br><span class="line">[root@localhost java]<span class="comment"># vi /etc/profile</span></span><br><span class="line">    <span class="comment">#SET JDK</span></span><br><span class="line">    <span class="built_in">export</span> JAVA_HOME=/usr/java/jdk1.7.0_79</span><br><span class="line">    <span class="built_in">export</span> JRE_HOME=<span class="variable">$JAVA_HOME</span>/jre</span><br><span class="line">    <span class="built_in">export</span> CLASS_PATH=.:<span class="variable">$JAVA_HOME</span>/lib:<span class="variable">$JAVA_HOME</span>/lib/tools.jar</span><br><span class="line">    <span class="built_in">export</span> PATH=<span class="variable">$&#123;JAVA_HOME&#125;</span>/bin:<span class="variable">$&#123;JRE_HOME&#125;</span>/bin:<span class="variable">$PATH</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">#4.添加完成后  :wq 保存</span></span><br><span class="line"><span class="comment">#5.使修改生效：</span></span><br><span class="line">[root@localhost java]<span class="comment">#source !$</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#6.验证JDK有效性</span></span><br><span class="line">[root@localhost java]<span class="comment"># java -version</span></span><br><span class="line">java version <span class="string">"1.7.0_79"</span></span><br><span class="line">Java(TM) SE Runtime Environment (build 1.7.0_79-b15)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 24.79-b02, mixed mode)</span><br></pre></td></tr></table></figure><h2 id="二-RPM包安装"><a href="#二-RPM包安装" class="headerlink" title="二.RPM包安装"></a>二.RPM包安装</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.下载rpm安装文件</span></span><br><span class="line"></span><br><span class="line">[root@localhost ~]$ wget http://download.oracle.com/otn-pub/java/jdk/7u79-b15/jdk-7u79-linux-x64.rpm</span><br><span class="line"></span><br><span class="line"><span class="comment">#2.使用rpm命令安装</span></span><br><span class="line"></span><br><span class="line">[root@localhost  ~]<span class="comment"># rpm -ivh jdk-7u79-linux-x64.rpm</span></span><br><span class="line"></span><br><span class="line">接下来的设置与tar包安装的第3-6步骤一样</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要记录在CentOS6.x中JDK的安装方法和详细步骤。&lt;br&gt;JDK的安装方式一般有两种RPM包安装和tar包安装&lt;br&gt;使用JDK1.7.79做示例&lt;br&gt;
    
    </summary>
    
      <category term="Linux" scheme="http://xiejm.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="http://xiejm.com/tags/Linux/"/>
    
      <category term="JDK" scheme="http://xiejm.com/tags/JDK/"/>
    
  </entry>
  
  <entry>
    <title>将CentOS的yum源更换为国内的阿里云源</title>
    <link href="http://xiejm.com/Linux/Change_CentOS_yum_repo_to_aliyun.html"/>
    <id>http://xiejm.com/Linux/Change_CentOS_yum_repo_to_aliyun.html</id>
    <published>2017-09-12T00:15:38.000Z</published>
    <updated>2018-04-17T04:43:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>我们使用默认的yum源，有时会连接到国外的镜像站导致yum下载比较慢。<br>所以将默认的yum源替换为阿里云的镜像站<br>阿里云Linux安装镜像源地址：<a href="http://mirrors.aliyun.com/" target="_blank" rel="noopener">http://mirrors.aliyun.com/</a><br><a id="more"></a></p><h2 id="1-备份你的原镜像文件，以免出错后可以恢复。"><a href="#1-备份你的原镜像文件，以免出错后可以恢复。" class="headerlink" title="1.备份你的原镜像文件，以免出错后可以恢复。"></a>1.备份你的原镜像文件，以免出错后可以恢复。</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup</span><br></pre></td></tr></table></figure><h2 id="2-下载新的CentOS-Base-repo-到-etc-yum-repos-d"><a href="#2-下载新的CentOS-Base-repo-到-etc-yum-repos-d" class="headerlink" title="2.下载新的CentOS-Base.repo 到/etc/yum.repos.d/"></a>2.下载新的CentOS-Base.repo 到/etc/yum.repos.d/</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#CentOS 5 使用下面的链接</span></span><br><span class="line">wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-5.repo</span><br><span class="line"></span><br><span class="line"><span class="comment">#CentOS 6 使用下面的链接</span></span><br><span class="line">wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repo</span><br><span class="line"></span><br><span class="line"><span class="comment">#CentOS 7 使用下面的链接</span></span><br><span class="line">wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo</span><br></pre></td></tr></table></figure><h2 id="3-运行yum-makecache生成缓存"><a href="#3-运行yum-makecache生成缓存" class="headerlink" title="3.运行yum makecache生成缓存"></a>3.运行yum makecache生成缓存</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum clean all</span><br><span class="line">yum makecache</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我们使用默认的yum源，有时会连接到国外的镜像站导致yum下载比较慢。&lt;br&gt;所以将默认的yum源替换为阿里云的镜像站&lt;br&gt;阿里云Linux安装镜像源地址：&lt;a href=&quot;http://mirrors.aliyun.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://mirrors.aliyun.com/&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Linux" scheme="http://xiejm.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="http://xiejm.com/tags/Linux/"/>
    
      <category term="yum" scheme="http://xiejm.com/tags/yum/"/>
    
      <category term="aliyun" scheme="http://xiejm.com/tags/aliyun/"/>
    
  </entry>
  
  <entry>
    <title>Linux(CentOS 6)配置SSH无密码登陆</title>
    <link href="http://xiejm.com/Linux/sshRsaLogin.html"/>
    <id>http://xiejm.com/Linux/sshRsaLogin.html</id>
    <published>2017-09-12T00:14:24.000Z</published>
    <updated>2018-04-17T04:43:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近在搭建Hadoop环境需要设置无密码登陆，所谓无密码登陆其实是指通过证书认证的方式登陆，使用一种被称为”公私钥”认证的方式来进行ssh登录。<br>这篇教程介绍使用SSH Key来实现SSH无密码登录，而且使用scp复制文件时也不需要再输入密码．除了方便SSH登录，scp复制文件外，SSH无密码登录也为Linux服务器增加了又一道安全防线．<br><a id="more"></a></p><h2 id="1-何为“公私钥“认证方式"><a href="#1-何为“公私钥“认证方式" class="headerlink" title="1.何为“公私钥“认证方式:"></a>1.何为“公私钥“认证方式:<br></h2><p>首先在客户端上创建一对公私钥 （公钥文件：<code>~/.ssh/id_rsa.pub</code>； 私钥文件：<code>~/.ssh/id_rsa</code>）。然后把公钥放到服务器上（<code>~/.ssh/authorized_keys</code>）, 自己保留好私钥.在使用ssh登录时,ssh程序会发送私钥去和服务器上的公钥做匹配.如果匹配成功就可以登录了。</p><h2 id="2-配置环境说明："><a href="#2-配置环境说明：" class="headerlink" title="2.配置环境说明："></a>2.配置环境说明：</h2><ul><li>CentOS 6.7 minimal</li><li>集群节点4台，并已经配置<code>/etc/hosts</code>主机映射<ul><li>192.168.194.111 hadoop001</li><li>192.168.194.112 hadoop002</li><li>192.168.194.113 hadoop003</li><li>192.168.194.114 hadoop004<blockquote><p>如果你有更多的节点，操作是一样的。当你的集群达到一定规模时，可以使用shell脚本或者python脚本来实现SSH免密配置</p></blockquote></li></ul></li></ul><h2 id="3-方法一配置步骤"><a href="#3-方法一配置步骤" class="headerlink" title="3.方法一配置步骤"></a>3.方法一配置步骤</h2><p><strong>以下使用hadoop用户做示例,如果是其他用户请参照本示例操作</strong></p><h4 id="3-1-检查sshd"><a href="#3-1-检查sshd" class="headerlink" title="3.1 检查sshd"></a>3.1 检查sshd</h4><p><em>每个节点都要操作</em><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 software]<span class="comment"># service sshd status</span></span><br><span class="line">openssh-daemon (pid  1844) is running...</span><br><span class="line">[root@hadoop001 software]<span class="comment"># su - hadoop</span></span><br></pre></td></tr></table></figure></p><h4 id="3-2-生成公私钥"><a href="#3-2-生成公私钥" class="headerlink" title="3.2 生成公私钥"></a>3.2 生成公私钥</h4><p><em>在所有节点终端生成公钥，生成的密钥在.ssh 目录下，并进入.ssh目录</em></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ ssh-keygen -t rsa -P <span class="string">''</span> -f ~/.ssh/id_rsa</span><br><span class="line">[hadoop@hadoop001 hadoop]$ <span class="built_in">cd</span> .ssh/</span><br><span class="line">[hadoop@hadoop001 hadoop]$ chmod 0600 ~/.ssh/authorized_keys`</span><br></pre></td></tr></table></figure><h4 id="3-3-合成并分发公钥"><a href="#3-3-合成并分发公钥" class="headerlink" title="3.3 合成并分发公钥"></a>3.3 合成并分发公钥</h4><p><em>只需在<code>hadoop001</code>节点操作，复制所有节点的公钥文件合并到<code>authorized_keys</code></em><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ ssh-keygen -t rsa -P <span class="string">''</span> -f ~/.ssh/id_rsa</span><br><span class="line">[hadoop@hadoop001 hadoop]$ <span class="built_in">cd</span> .ssh/</span><br><span class="line">[hadoop@hadoop001 hadoop]$ chmod 0600 ~/.ssh/authorized_keys</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop001 .ssh]$ ssh hadoop001 cat /home/hadoop/.ssh/id_rsa.pub &gt;&gt;authorized_keys</span><br><span class="line">[hadoop@hadoop001 .ssh]$ ssh hadoop002 cat /home/hadoop/.ssh/id_rsa.pub &gt;&gt;authorized_keys</span><br><span class="line">[hadoop@hadoop001 .ssh]$ ssh hadoop003 cat /home/hadoop/.ssh/id_rsa.pub &gt;&gt;authorized_keys</span><br><span class="line">[hadoop@hadoop001 .ssh]$ ssh hadoop004 cat /home/hadoop/.ssh/id_rsa.pub &gt;&gt;authorized_keys</span><br></pre></td></tr></table></figure></p><h4 id="3-4-分发authorized-keys"><a href="#3-4-分发authorized-keys" class="headerlink" title="3.4 分发authorized_keys"></a>3.4 分发authorized_keys</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 .ssh]$ chmod 0600 ~/.ssh/authorized_keys</span><br><span class="line">[hadoop@hadoop001 .ssh]$ scp authorized_keys hadoop@hadoop002:/home/hadoop/.ssh/</span><br><span class="line">[hadoop@hadoop001 .ssh]$ scp authorized_keys hadoop@hadoop003:/home/hadoop/.ssh/</span><br><span class="line">[hadoop@hadoop001 .ssh]$ scp authorized_keys hadoop@hadoop004:/home/hadoop/.ssh/</span><br></pre></td></tr></table></figure><h4 id="3-5-分发known-hosts"><a href="#3-5-分发known-hosts" class="headerlink" title="3.5 分发known_hosts"></a>3.5 分发known_hosts</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 .ssh]$ chmod 0600 ~/.ssh/authorized_keys</span><br><span class="line">[hadoop@hadoop001 .ssh]$ scp known_hosts hadoop@hadoop002:/home/hadoop/.ssh/</span><br><span class="line">[hadoop@hadoop001 .ssh]$ scp known_hosts hadoop@hadoop003:/home/hadoop/.ssh/</span><br><span class="line">[hadoop@hadoop001 .ssh]$ scp known_hosts hadoop@hadoop004:/home/hadoop/.ssh/</span><br></pre></td></tr></table></figure><h4 id="3-6验证SSH"><a href="#3-6验证SSH" class="headerlink" title="3.6验证SSH"></a>3.6验证SSH</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 .ssh]<span class="variable">$ssh</span> hadoop001 date   <span class="comment">#不需要回车输入yes,即SSH配置成功</span></span><br><span class="line">Sat Sep  9 20:14:57 CST 2017</span><br></pre></td></tr></table></figure><h2 id="4-方法二配置步骤"><a href="#4-方法二配置步骤" class="headerlink" title="4.方法二配置步骤"></a>4.方法二配置步骤</h2><p>可以使用<code>ssh-copy-id</code>命令来完成．<br><code>ssh-copy-id</code>命令会自动上传SSH公钥到其他节点的<code>.ssh/authorized_keys</code>文件中．<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 .ssh]<span class="variable">$ssh</span>-copy-id hadoop@hadoop001</span><br><span class="line">[hadoop@hadoop001 .ssh]<span class="variable">$ssh</span>-copy-id hadoop@hadoop002</span><br><span class="line">[hadoop@hadoop001 .ssh]<span class="variable">$ssh</span>-copy-id hadoop@hadoop003</span><br><span class="line">[hadoop@hadoop001 .ssh]<span class="variable">$ssh</span>-copy-id hadoop@hadoop004</span><br></pre></td></tr></table></figure></p><p>同理其他节点也使用<code>ssh-copy-id</code>分发SSH公钥,这样也能达到相互免密登录的效果</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近在搭建Hadoop环境需要设置无密码登陆，所谓无密码登陆其实是指通过证书认证的方式登陆，使用一种被称为”公私钥”认证的方式来进行ssh登录。&lt;br&gt;这篇教程介绍使用SSH Key来实现SSH无密码登录，而且使用scp复制文件时也不需要再输入密码．除了方便SSH登录，scp复制文件外，SSH无密码登录也为Linux服务器增加了又一道安全防线．&lt;br&gt;
    
    </summary>
    
      <category term="Linux" scheme="http://xiejm.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="http://xiejm.com/tags/Linux/"/>
    
      <category term="SSH" scheme="http://xiejm.com/tags/SSH/"/>
    
  </entry>
  
</feed>
