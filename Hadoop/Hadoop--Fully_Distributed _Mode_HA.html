<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    
    <title>Hadoop--集群(Fully Distributed Mode)模式部署+HA | 云筑小站</title>
    <meta name="renderer" content="webkit">
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

    <meta name="description" content="脚踏实地、仰望星空;低头走路，抬头看天;既练轻功，也练内功。">

    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Hadoop--集群(Fully Distributed Mode)模式部署+HA | 云筑小站">
    <meta name="twitter:description" content="脚踏实地、仰望星空;低头走路，抬头看天;既练轻功，也练内功。">

    <meta property="og:type" content="article">
    <meta property="og:title" content="Hadoop--集群(Fully Distributed Mode)模式部署+HA | 云筑小站">
    <meta property="og:description" content="脚踏实地、仰望星空;低头走路，抬头看天;既练轻功，也练内功。">

    
    <meta name="author" content="XieJM">
    
    <link rel="stylesheet" href="/css/vno.css">
    <link rel="stylesheet" href="//netdna.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css">

    
    <link rel="icon" href="/images/favicon.png">
    

    
    <link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
    
    
    <meta name="generator" content="hexo"/>
    
    <link rel="alternate" type="application/rss+xml" title="云筑小站" href="/atom.xml">
    

    <link rel="canonical" href="http://xiejm.com/Hadoop/Hadoop--Fully_Distributed _Mode_HA.html"/>

    
</head>

<body class="home-template no-js">

    <span class="mobile btn-mobile-menu">
        <i class="fa fa-list btn-mobile-menu__icon"></i>
        <i class="fa fa-angle-up btn-mobile-close__icon hidden"></i>
    </span>

    
<header class="panel-cover panel-cover--collapsed" style="background-image: url(/images/background-cover.jpg)">
  <div class="panel-main">
    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">

        <a href="/" title="前往 云筑小站 的主页"><img src="/images/logo.png" width="80" alt="云筑小站 logo" class="panel-cover__logo logo" /></a>
        <h1 class="panel-cover__title panel-title"><a href="/" title="link to homepage for 云筑小站">云筑小站</a></h1>
        
        <span class="panel-cover__subtitle panel-subtitle">分享技术 记录生活</span>
        
        <hr class="panel-cover__divider" />
        <p class="panel-cover__description">脚踏实地、仰望星空;低头走路，抬头看天;既练轻功，也练内功。</p>
        <hr class="panel-cover__divider panel-cover__divider--secondary" />
        
        <div class="navigation-wrapper">
          <div>
          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">
              <li class="navigation__item"><a href="/#blog" title="访问博客" class="blog-button">文章</a></li>
            
              <li class="navigation__item"><a href="/archives">归档</a></li>
            
            </ul>
          </nav>
          </div>
          <div>
          <nav class="cover-navigation navigation--social">
  <ul class="navigation">

  <!-- Weibo-->
  

  <!-- Github -->
  
  <li class="navigation__item">
    <a href="https://github.com/xjmhz" title="查看我的GitHub主页" target="_blank">
      <i class='social fa fa-github'></i>
      <span class="label">Github</span>
    </a>
  </li>


<!-- Stack Overflow -->
        

  <!-- Google Plus -->
  

<!-- Facebook -->


<!-- Twitter -->

<!-- instagram -->


  <li class="navigation__item">
    <a href="/atom.xml" title="RSS" target="_blank">
      <i class='social fa fa-rss'></i>
      <span class="label">RSS</span>
    </a>
  </li>



  </ul>
</nav>

          </div>
        </div>

      </div>

    </div>

    <div class="panel-cover--overlay cover-disabled"></div>
  </div>
</header>


    <div class="content-wrapper">
        <div class="content-wrapper__inner">
            <article class="post-container post-container--single">

  <header class="post-header">
    <div class="post-meta">
      <time datetime="2017-10-03T13:06:38.000Z" class="post-list__meta--date date">2017-10-03</time>
 &#8226; <span class="post-meta__tags tags">分类&nbsp;
  <a class="tag-link" href="/tags/Hadoop/">Hadoop</a>

</span>
    </div>
    <h1 class="post-title">Hadoop--集群(Fully Distributed Mode)模式部署+HA</h1>
  </header>

  <section class="post">
    <p>本文主要记录Hadoop全分布模式（Fully Distributed Mode）部署的详细步骤：</p>
<p>Hadoop全分布模式（Fully Distributed Mode）是一种集群部署方式，它的守护进程运行在一个集群上。</p>
<a id="more"></a>
<ul>
<li>我们一般会在master节点上部署<code>NameNode</code>,<code>ResourceManager</code>；</li>
<li><code>SecondaryNameNode</code>可以安装在master节点，也可以单独安装。</li>
<li>slave节点能看到<code>DataNode</code>和<code>NodeManager</code>。</li>
</ul>
<h2 id="1-基本环境"><a href="#1-基本环境" class="headerlink" title="1.基本环境"></a>1.基本环境</h2><ul>
<li>CentOS 6.7</li>
<li>JDK1.7+</li>
<li>Hadoop 2.8.1</li>
<li>ZooKeeper 3.4.6</li>
</ul>
<blockquote>
<p>系统原始环境特别说明：<br>系统选择minimal最小系统安装，并自定义选装以下包：<br>Base System中的Base、Compatbillty libraries、Debugging Tools; Development中的 Development tools</p>
</blockquote>
<h3 id="1-1组件说明："><a href="#1-1组件说明：" class="headerlink" title="1.1组件说明："></a>1.1组件说明：</h3><table>
<thead>
<tr>
<th>组件</th>
<th>备注说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>CentOS</td>
<td>使用下列命令查看系统版本和系统位数<br>lsb_release -a #系统版本<br>file /bin/ls #系统位数</td>
</tr>
<tr>
<td>JDK</td>
<td>本文采用JDK1.8.45</td>
</tr>
<tr>
<td>Hadoop</td>
<td>使用官网最新release版本2.8.1</td>
</tr>
<tr>
<td>ZooKeeper</td>
<td>用于HA热切,存储数据使用的协调服务<br><a href="http://mirrors.hust.edu.cn/apache/zookeeper/zookeeper-3.4.6/zookeeper-3.4.6.tar.gz" target="_blank" rel="noopener">下载地址：</a></td>
</tr>
</tbody>
</table>
<h2 id="2-主机节点规划"><a href="#2-主机节点规划" class="headerlink" title="2.主机节点规划"></a>2.主机节点规划</h2><table>
<thead>
<tr>
<th>HostName</th>
<th>IP Address</th>
<th>Software</th>
<th>Course</th>
</tr>
</thead>
<tbody>
<tr>
<td>hadoop001</td>
<td>192.168.194.111</td>
<td>Hadoop、ZooKeeper</td>
<td>NameNode<br>DFSZKFailoverController <br>ResourceManager<br>JobHistoryServer<br>QuorumPeerMain</td>
</tr>
<tr>
<td>hadoop002</td>
<td>192.168.194.112</td>
<td>Hadoop、ZooKeeper</td>
<td>NameNode<br>DFSZKFailoverController <br>ResourceManager<br>QuorumPeerMain</td>
</tr>
<tr>
<td>hadoop003</td>
<td>192.168.194.113</td>
<td>Hadoop、ZooKeeper</td>
<td>DataNode<br> NodeManager<br>JournalNode<br>QuorumPeerMain</td>
</tr>
<tr>
<td>hadoop004</td>
<td>192.168.194.114</td>
<td>Hadoop、ZooKeeper</td>
<td>DataNode<br> NodeManager<br>JournalNode<br>QuorumPeerMain</td>
</tr>
<tr>
<td>hadoop005</td>
<td>192.168.194.115</td>
<td>Hadoop、ZooKeeper</td>
<td>DataNode<br> NodeManager<br>JournalNode<br>QuorumPeerMain</td>
</tr>
</tbody>
</table>
<h2 id="3-目录规划"><a href="#3-目录规划" class="headerlink" title="3.目录规划"></a>3.目录规划</h2><table>
<thead>
<tr>
<th>名称</th>
<th>路径</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>$HADOOP_HOME</td>
<td>/opt/software/hadoop</td>
<td></td>
</tr>
<tr>
<td>Data</td>
<td>$HADOOP_HOME/data</td>
<td></td>
</tr>
<tr>
<td>Log</td>
<td>$HADOOP_HOME/logs</td>
<td></td>
</tr>
<tr>
<td>hadoop.tmp.dir</td>
<td>$HADOOP_HOME/tmp</td>
<td>需要手工创建,权限777,root:root</td>
<td></td>
</tr>
<tr>
<td>$ZOOKEEPER_HOME</td>
<td>/opt/software/zookeeper</td>
<td></td>
</tr>
</tbody>
</table>
<h2 id="4-系统环境配置"><a href="#4-系统环境配置" class="headerlink" title="4.系统环境配置"></a>4.系统环境配置</h2><p>本节内容所有节点都要配置，以下操作用hadoop001节点做示例</p>
<h3 id="4-1-配置网络和主机名"><a href="#4-1-配置网络和主机名" class="headerlink" title="4.1 配置网络和主机名"></a>4.1 配置网络和主机名</h3><h4 id="网络IP设置"><a href="#网络IP设置" class="headerlink" title="网络IP设置"></a>网络IP设置</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 ~]# vi /etc/sysconfig/network-scripts/ifcfg-eth0</span><br><span class="line"><span class="meta">#</span>按以下格式修改，没有的字段需要自行添加</span><br><span class="line">DEVICE=eth0                                 #网卡名称</span><br><span class="line">HWADDR=00:0C:29:8F:61:EB                    #MAC地址</span><br><span class="line">TYPE=Ethernet                               #网卡类型</span><br><span class="line">UUID=039afc77-da3e-4e17-bcaa-062148464812   #网卡ID</span><br><span class="line">ONBOOT=yes                                  #是否开机启动</span><br><span class="line">NM_CONTROLLED=yes                           #Network Manager</span><br><span class="line">BOOTPROTO=static                            #静态IP模式</span><br><span class="line">IPADDR=192.168.194.111                      #IP地址</span><br><span class="line">NETMASK=255.255.255.0                       #子网掩码</span><br><span class="line">GATEWAY=192.168.194.2                       #网关地址</span><br><span class="line">DNS1=223.5.5.5                              #DNS</span><br></pre></td></tr></table></figure>
<p>保存退出后，重启网卡</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 ~]# service network restart</span><br></pre></td></tr></table></figure>
<h4 id="主机名配置"><a href="#主机名配置" class="headerlink" title="主机名配置"></a>主机名配置</h4><p>如果在安装系统的时候未进行主机名设置，那么请根据以下命令对主机名进行配置</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 ~]# vi /etc/sysconfig/network</span><br><span class="line"><span class="meta">#</span>修改下列信息，其中HOSTNAME项修改成我们需要的主机名</span><br><span class="line">NETWORKING=yes #启动网络</span><br><span class="line">HOSTNAME=hadoop001 #主机名</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>修改完成后保存并退出，使用下列命令更新当前bash的主机名</span><br><span class="line">[root@hadoop001 ~]# hostname hadoop001</span><br></pre></td></tr></table></figure>
<h3 id="4-2-添加集群用户"><a href="#4-2-添加集群用户" class="headerlink" title="4.2 添加集群用户"></a>4.2 添加集群用户</h3><p>增加ID号为1000的hadoopGroup,增加ID为2000的用户并加入hadoopGroup组</p>
<p>固定用户和组的ID号是为了用户和组的权限一致，我遇到过一次在两台机器上用户名和组名相同，ID号不同结果导致权限有问题</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># groupadd -g 1000 hadoopGroup</span></span><br><span class="line">[root@hadoop001 ~]<span class="comment"># useradd -u 2000 -g hadoopGroup hadoop</span></span><br></pre></td></tr></table></figure>
<p>设置hadoop用户的密码为hadoop，下面的这种方法免去了修改密码时的密码确认，在生产环境中使用此方式改密码需要清空history</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># echo "hadoop"|passwd --stdin hadoop</span></span><br></pre></td></tr></table></figure>
<h3 id="4-3-关闭SELINUX"><a href="#4-3-关闭SELINUX" class="headerlink" title="4.3 关闭SELINUX"></a>4.3 关闭SELINUX</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># sed -i 's#SELINUX=enforcing#SELINUX=disabled#g' /etc/selinux/config</span></span><br><span class="line"><span class="comment">#关闭当前的SELINUX状态,如果不用setenforce那么需要重启系统</span></span><br><span class="line">[root@hadoop001 ~]<span class="comment"># setenforce 0</span></span><br><span class="line">[root@hadoop001 ~]<span class="comment"># grep SELINUX=disabled /etc/selinux/config</span></span><br><span class="line">SELINUX=disabled</span><br></pre></td></tr></table></figure>
<h3 id="4-4-关闭防火墙"><a href="#4-4-关闭防火墙" class="headerlink" title="4.4 关闭防火墙"></a>4.4 关闭防火墙</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># service iptables stop</span></span><br><span class="line">iptables: Setting chains to policy ACCEPT: filter          [  OK  ]</span><br><span class="line">iptables: Flushing firewall rules:                         [  OK  ]</span><br><span class="line">iptables: Unloading modules:                               [  OK  ]</span><br><span class="line">[root@hadoop001 ~]<span class="comment"># chkconfig iptables off</span></span><br></pre></td></tr></table></figure>
<h3 id="4-5-优化开机自启动服务-可选"><a href="#4-5-优化开机自启动服务-可选" class="headerlink" title="4.5 优化开机自启动服务(可选)"></a>4.5 优化开机自启动服务(可选)</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># chkconfig --list|grep 3:on |wc -l</span></span><br><span class="line">[root@hadoop001 ~]<span class="comment"># chkconfig --list|grep 3:on |cut -d "" -f1</span></span><br><span class="line"><span class="comment">#关闭所有3级别的服务</span></span><br><span class="line">[root@hadoop001 ~]<span class="comment"># for name in `chkconfig --list|grep 3:on | cut -d " " -f1` ;do chkconfig $name off;done</span></span><br><span class="line"><span class="comment">#开启需要在3级别启动的服务，这里可以根据需要增加和修改</span></span><br><span class="line">[root@hadoop001 ~]<span class="comment"># for name in crond ntpd network rsyslog sshd;do chkconfig $name on;done</span></span><br><span class="line">[root@hadoop001 ~]<span class="comment"># chkconfig --list|grep 3:on</span></span><br></pre></td></tr></table></figure>
<h3 id="4-6-修改文件描述符"><a href="#4-6-修改文件描述符" class="headerlink" title="4.6 修改文件描述符"></a>4.6 修改文件描述符</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#修改系统limits.conf</span></span><br><span class="line">[root@hadoop001 ~]<span class="comment"># cat &gt;&gt; /etc/security/limits.conf &lt;&lt; EOF</span></span><br><span class="line">*           soft    nofile      262144</span><br><span class="line">*           hard    nofile      262144</span><br><span class="line">*           soft    nproc       262144</span><br><span class="line">*           hard    nproc       262144</span><br><span class="line">EOF</span><br><span class="line"><span class="comment">#查看系统ulimit文件句柄数</span></span><br><span class="line">[root@hadoop001 ~]<span class="comment"># ulimit -n</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#修改90-nproc.conf</span></span><br><span class="line">[root@hadoop001 ~]<span class="comment"># vi /etc/security/limits.d/90-nproc.conf</span></span><br><span class="line">*  soft nproc 262144</span><br></pre></td></tr></table></figure>
<h3 id="4-7-增加集群用户sudo权限"><a href="#4-7-增加集群用户sudo权限" class="headerlink" title="4.7 增加集群用户sudo权限"></a>4.7 增加集群用户sudo权限</h3><p>增加hadoop用户的sudo权限</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># vi /etc/sudoers</span></span><br><span class="line">hadoop ALL=(ALL)       ALL</span><br></pre></td></tr></table></figure>
<h3 id="4-8-配置主机IP映射"><a href="#4-8-配置主机IP映射" class="headerlink" title="4.8 配置主机IP映射"></a>4.8 配置主机IP映射</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># vi /etc/hosts</span></span><br><span class="line"><span class="comment">#在hosts文件末尾追加</span></span><br><span class="line"></span><br><span class="line">192.168.194.111    hadoop001</span><br><span class="line">192.168.194.112    hadoop002</span><br><span class="line">192.168.194.113    hadoop003</span><br><span class="line">192.168.194.114    hadoop004</span><br><span class="line">192.168.194.115    hadoop005</span><br></pre></td></tr></table></figure>
<h3 id="4-9-配置SSH免密登录"><a href="#4-9-配置SSH免密登录" class="headerlink" title="4.9 配置SSH免密登录"></a>4.9 配置SSH免密登录</h3><p>SSH免密码登录配置的详细配置步骤：<a href="http://blog.xiejm.com/41.html" target="_blank" rel="noopener">SSH免密登录</a></p>
<h3 id="4-10-配置NTP时钟同步"><a href="#4-10-配置NTP时钟同步" class="headerlink" title="4.10 配置NTP时钟同步"></a>4.10 配置NTP时钟同步</h3><ul>
<li>第一种方法 ：（所有节点都要操作）都从公共NTP服务器同步，执行如下：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime</span><br><span class="line">$ ntpdate us.pool.ntp.org</span><br><span class="line">$ crontab -e</span><br><span class="line">0-59/10 * * * * /usr/sbin/ntpdate us.pool.ntp.org | logger -t NTP</span><br></pre></td></tr></table></figure>
<p>以上crontab的配置说明详见 <a href="http://note.youdao.com/" target="_blank" rel="noopener">Linux-crontab命令</a></p>
<ul>
<li>第二种方法：选一个节点搭建一个NTP服务，其他节点从该NTP服务器同步，<a href="http://note.youdao.com/" target="_blank" rel="noopener">具体步骤参考本博客的NTP 配置</a></li>
</ul>
<h3 id="4-11-关闭交换分区"><a href="#4-11-关闭交换分区" class="headerlink" title="4.11 关闭交换分区"></a>4.11 关闭交换分区</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># echo '0' &gt; /proc/sys/vm/swappiness</span></span><br><span class="line">[root@hadoop001 ~]<span class="comment"># sysctl vm.swappiness=0</span></span><br><span class="line">[root@hadoop001 ~]<span class="comment"># echo 'vm.swappiness=0'&gt;&gt; /etc/sysctl.conf</span></span><br></pre></td></tr></table></figure>
<h3 id="4-12-禁用透明大页"><a href="#4-12-禁用透明大页" class="headerlink" title="4.12 禁用透明大页"></a>4.12 禁用透明大页</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># echo never &gt; /sys/kernel/mm/redhat_transparent_hugepage/defrag</span></span><br><span class="line">[root@hadoop001 ~]<span class="comment"># cat /sys/kernel/mm/redhat_transparent_hugepage/defrag</span></span><br><span class="line">always [never]</span><br><span class="line">[root@hadoop001 ~]<span class="comment"># echo 'echo never &gt; /sys/kernel/mm/redhat_transparent_hugepage/defrag ' &gt;&gt; /etc/rc.local</span></span><br></pre></td></tr></table></figure>
<h2 id="5-JDK安装"><a href="#5-JDK安装" class="headerlink" title="5.JDK安装"></a>5.JDK安装</h2><p><strong>【注意】所有节点配置</strong><br>详细安装步骤参考本博客的JDK安装的文章:<a href="http://blog.xiejm.com/8.html" target="_blank" rel="noopener">JDK安装步骤</a></p>
<h2 id="6-配置环境变量"><a href="#6-配置环境变量" class="headerlink" title="6.配置环境变量"></a>6.配置环境变量</h2><ul>
<li><strong>【注意】所有节点配置</strong></li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 software]<span class="comment"># vim /etc/profile</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#按组合键shift+g 或大写G 到最后一行，按字母o  插入下列内容</span></span><br><span class="line">    <span class="comment">#SET HADOOP</span></span><br><span class="line">    exprot HADOOP_HOME=/opt/software/hadoop</span><br><span class="line"></span><br><span class="line">    <span class="comment">#SET ZOOKEEPER</span></span><br><span class="line">    <span class="built_in">export</span> ZOOKEEPER_HOME=/opt/software/zookeeper</span><br><span class="line"><span class="comment">#如果JDK环境变量没有配置，请加入下列内容</span></span><br><span class="line">    <span class="comment">#SET JDK</span></span><br><span class="line">    <span class="built_in">export</span> JAVA_HOME=/usr/java/jdk1.7.0_79</span><br><span class="line">    <span class="built_in">export</span> JRE_HOME=<span class="variable">$JAVA_HOME</span>/jre</span><br><span class="line">    <span class="built_in">export</span> CLASS_PATH=.:<span class="variable">$JAVA_HOME</span>/lib:<span class="variable">$JAVA_HOME</span>/lib/tools.jar</span><br><span class="line"></span><br><span class="line"><span class="comment">#设置PATH</span></span><br><span class="line">    <span class="built_in">export</span> PATH=<span class="variable">$ZOOKEEPER_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/sbin:<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$JRE_HOME</span>/bin:<span class="variable">$PATH</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#添加完成后  :wq 保存</span></span><br><span class="line"><span class="comment">#使修改生效：</span></span><br><span class="line">[root@hadoop001 software]<span class="comment">#source !$</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#验证JDK有效性</span></span><br><span class="line">[root@hadoop001 software]<span class="comment"># java -version</span></span><br><span class="line">java version <span class="string">"1.7.0_79"</span></span><br><span class="line">Java(TM) SE Runtime Environment (build 1.7.0_79-b15)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 24.79-b02, mixed mode)</span><br></pre></td></tr></table></figure>
<h2 id="7-安装配置ZooKeeper"><a href="#7-安装配置ZooKeeper" class="headerlink" title="7.安装配置ZooKeeper"></a>7.安装配置ZooKeeper</h2><h3 id="7-1-下载ZooKeeper"><a href="#7-1-下载ZooKeeper" class="headerlink" title="7.1 下载ZooKeeper"></a>7.1 下载ZooKeeper</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 software]<span class="comment"># pwd</span></span><br><span class="line">/opt/software</span><br><span class="line">[root@hadoop001 software]<span class="comment"># wget http://mirrors.hust.edu.cn/apache/zookeeper/zookeeper-3.4.6/zookeeper-3.4.6.tar.gz</span></span><br><span class="line">[root@hadoop001 software]<span class="comment"># tar -zxvf zookeeper-3.4.6.tar.gz</span></span><br><span class="line">[root@hadoop001 software]<span class="comment"># mv zookeeper-3.4.6 zookeeper</span></span><br><span class="line">[root@hadoop001 software]<span class="comment"># chown -R hadoop:hadoopGroup zookeeper</span></span><br></pre></td></tr></table></figure>
<h3 id="7-2-修改配置"><a href="#7-2-修改配置" class="headerlink" title="7.2 修改配置"></a>7.2 修改配置</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 software]<span class="comment"># cd /opt/software/zookeeper/conf</span></span><br><span class="line">[root@hadoop001 conf]<span class="comment"># cp zoo_sample.cfg zoo.cfg</span></span><br><span class="line"></span><br><span class="line">[root@hadoop001 conf]<span class="comment"># vi zoo.cfg</span></span><br><span class="line">修改dataDir</span><br><span class="line">dataDir=/opt/software/zookeeper/data</span><br><span class="line">添加下面三行</span><br><span class="line">server.1=hadoop001:2888:3888</span><br><span class="line">server.2=hadoop002:2888:3888</span><br><span class="line">server.3=hadoop003:2888:3888</span><br><span class="line">server.2=hadoop004:2888:3888</span><br><span class="line">server.3=hadoop005:2888:3888</span><br><span class="line"></span><br><span class="line">[root@hadoop001 conf]<span class="comment"># cd ../</span></span><br><span class="line">[root@hadoop001 zookeeper]<span class="comment"># mkdir data</span></span><br><span class="line">[root@hadoop001 zookeeper]<span class="comment"># touch data/myid</span></span><br><span class="line">[root@hadoop001 zookeeper]<span class="comment"># echo 1 &gt; data/myid</span></span><br><span class="line">[root@hadoop001 zookeeper]<span class="comment"># cat data/myid</span></span><br><span class="line">1</span><br></pre></td></tr></table></figure>
<h3 id="7-3-文件分发"><a href="#7-3-文件分发" class="headerlink" title="7.3 文件分发"></a>7.3 文件分发</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 zookeeper]<span class="comment"># scp -r /opt/software/zookeeper root@hadoop002:/opt/software</span></span><br><span class="line">[root@hadoop001 zookeeper]<span class="comment"># scp -r /opt/software/zookeeper root@hadoop003:/opt/software</span></span><br><span class="line">[root@hadoop001 zookeeper]<span class="comment"># scp -r /opt/software/zookeeper root@hadoop004:/opt/software</span></span><br><span class="line">[root@hadoop001 zookeeper]<span class="comment"># scp -r /opt/software/zookeeper root@hadoop005:/opt/software</span></span><br></pre></td></tr></table></figure>
<h3 id="7-4-其他节点myid修改"><a href="#7-4-其他节点myid修改" class="headerlink" title="7.4 其他节点myid修改"></a>7.4 其他节点myid修改</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#不同zk节点配置不同的myid</span></span><br><span class="line"><span class="comment">#myid中的数值需要和 zoo.cfg中的配置一致。</span></span><br><span class="line"><span class="comment"># hadoop002/003/004/005,也修改配置,就如下不同</span></span><br><span class="line">[root@hadoop002 zookeeper]<span class="comment"># echo 2 &gt; data/myid</span></span><br><span class="line">[root@hadoop003 zookeeper]<span class="comment"># echo 3 &gt; data/myid</span></span><br><span class="line">[root@hadoop004 zookeeper]<span class="comment"># echo 4 &gt; data/myid</span></span><br><span class="line">[root@hadoop005 zookeeper]<span class="comment"># echo 5 &gt; data/myid</span></span><br><span class="line"><span class="comment">####注意！！！不能把 echo 3&gt;data/myid,将&gt;前后空格保留,否则无法将 3 写入myid文件</span></span><br></pre></td></tr></table></figure>
<h2 id="8-安装Hadoop"><a href="#8-安装Hadoop" class="headerlink" title="8.安装Hadoop"></a>8.安装Hadoop</h2><h3 id="8-1-解压"><a href="#8-1-解压" class="headerlink" title="8.1 解压"></a>8.1 解压</h3><p>之前编译好了hadoop-2.8.1-snappy.tar.gz，上传这个包并解压。<br>具体编译过程，参考本博客<a href="http://note.youdao.com/" target="_blank" rel="noopener"></a></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># mkdir -p /opt/software</span></span><br><span class="line"><span class="comment">#将hadoop安装包放到/opt/software</span></span><br><span class="line">[root@hadoop001 ~]<span class="comment"># cd /opt/software</span></span><br><span class="line">[root@hadoop001 software]<span class="comment"># tar -zxvf hadoop-2.8.1.tar.gz</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#设置软连接</span></span><br><span class="line">[root@hadoop001 software]<span class="comment"># ln -s /opt/software/hadoop-2.8.1 /opt/software/hadoop</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 当我们使用hadoop用户才需要下面命令</span></span><br><span class="line"><span class="comment">#设置hadoop目录所有者、所属组</span></span><br><span class="line"><span class="comment">#[root@hadoop001 software]# chown -R hadoop:hadoopGroup hadoop/*</span></span><br><span class="line"><span class="comment">#[root@hadoop001 software]# chown -R hadoop:hadoopGroup hadoop-2.8.1</span></span><br><span class="line"><span class="comment">#[root@hadoop001 software]# chown -R hadoop:hadoopGroup hadoop-2.8.1/*</span></span><br></pre></td></tr></table></figure>
<h3 id="8-2-配置修改"><a href="#8-2-配置修改" class="headerlink" title="8.2 配置修改"></a>8.2 配置修改</h3><p>使用命令<code>cd $HADOOP_HOME/etc/hadoop</code>进入配置文件目录</p>
<ul>
<li>8.2.1 修改$HADOOP_HOME/etc/hadoop/hadoop-env.sh</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=<span class="string">"/usr/java/jdk1.7.0_67"</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_OPTS=<span class="string">"<span class="variable">$HADOOP_OPTS</span> -Djava.library.path=<span class="variable">$HADOOP_HOME</span>/lib:<span class="variable">$HADOOP_HOME</span>/lib/native"</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>8.2.2 修改$HADOOP_HOME/etc/hadoop/core-site.xml</p>
</li>
<li><p>8.2.3 修改$HADOOP_HOME/etc/hadoop/hdfs-site.xml</p>
</li>
<li><p>8.2.4 修改$HADOOP_HOEM/etc/hadoop/mapred-site.xml</p>
<p>  <em>首先复制mapred-site.xml的模板文件</em>，<br>然后修改mapred-site.xml</p>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@sht-sgmhadoopnn-01 hadoop]<span class="comment"># cp  mapred-site.xml.template  mapred-site.xml</span></span><br><span class="line">[root@sht-sgmhadoopnn-01 hadoop]<span class="comment"># vi  mapred-site.xml</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>8.2.5 修改$HADOOP_HOME/etc/hadoop/yarn-site.xml</p>
</li>
<li><p>8.2.6 修改slaves</p>
</li>
</ul>
<p><a href="http://wpxiejm.oss-cn-beijing.aliyuncs.com/Note/hadoop/Hadoop_HA_conf.rar" target="_blank" rel="noopener">点击下载配置文件</a>，解压后复制到$HADOOP_HOME/etc/hadoop目录下</p>
<h3 id="8-3-创建临时文件夹和分发文件夹"><a href="#8-3-创建临时文件夹和分发文件夹" class="headerlink" title="8.3 创建临时文件夹和分发文件夹"></a>8.3 创建临时文件夹和分发文件夹</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 hadoop]<span class="comment"># mkdir -p  /opt/software/hadoop/tmp</span></span><br><span class="line">[root@hadoop001 hadoop]<span class="comment"># chmod -R 777  /opt/software/hadoop/tmp</span></span><br><span class="line">[root@hadoop001 hadoop]<span class="comment"># chown -R root:root  /opt/software/hadoop/tmp</span></span><br><span class="line"></span><br><span class="line">[root@hadoop001 hadoop]<span class="comment"># scp -r  /opt/software/hadoop root@hadoop002:/hadoop</span></span><br><span class="line">[root@hadoop001 hadoop]<span class="comment"># scp -r  /opt/software/hadoop root@hadoop003:/hadoop</span></span><br><span class="line">[root@hadoop001 hadoop]<span class="comment"># scp -r  /opt/software/hadoop root@hadoop004:/hadoop</span></span><br><span class="line">[root@hadoop001 hadoop]<span class="comment"># scp -r  /opt/software/hadoop root@hadoop005:/hadoop</span></span><br></pre></td></tr></table></figure>
<h2 id="9-启动步骤和详细过程："><a href="#9-启动步骤和详细过程：" class="headerlink" title="9.启动步骤和详细过程："></a>9.启动步骤和详细过程：</h2><h3 id="9-1-启动ZK"><a href="#9-1-启动ZK" class="headerlink" title="9.1 启动ZK"></a>9.1 启动ZK</h3><p>在所有的ZK节点执行命令： <code>zkServer.sh start</code></p>
<p>可借助命令 <code>zkServer.sh status</code>  查看各个ZK的从属关系</p>
<h3 id="9-2-启动-hadoop-HDFS-YARN"><a href="#9-2-启动-hadoop-HDFS-YARN" class="headerlink" title="9.2 启动 hadoop(HDFS+YARN)"></a>9.2 启动 hadoop(HDFS+YARN)</h3><h4 id="9-2-1-格式化前-先在-journalnode-节点机器上先启动-JournalNode-进程"><a href="#9-2-1-格式化前-先在-journalnode-节点机器上先启动-JournalNode-进程" class="headerlink" title="9.2.1. 格式化前,先在 journalnode 节点机器上先启动 JournalNode 进程"></a>9.2.1. 格式化前,先在 journalnode 节点机器上先启动 JournalNode 进程</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@hadoop003 ~]<span class="comment"># cd /opt/software/hadoop/sbin</span></span><br><span class="line">[root@hadoop003 sbin]<span class="comment"># hadoop-daemon.sh start journalnode</span></span><br><span class="line">starting journalnode, logging to /opt/software/hadoop/logs/hadoop-root-journalnode-hadoop001.out</span><br><span class="line">[root@hadoop003 sbin]<span class="comment"># jps</span></span><br><span class="line">4016 Jps</span><br><span class="line">3683 QuorumPeerMain</span><br><span class="line">3981 JournalNode</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@hadoop004 hadoop]<span class="comment"># cd /opt/software/hadoop/sbin</span></span><br><span class="line">[root@hadoop004 sbin]<span class="comment"># hadoop-daemon.sh start journalnode</span></span><br><span class="line">starting journalnode, logging to /opt/software/hadoop/logs/hadoop-root-journalnode-hadoop002.out</span><br><span class="line">[root@hadoop004 sbin]<span class="comment"># jps</span></span><br><span class="line">9891 Jps</span><br><span class="line">9609 QuorumPeerMain</span><br><span class="line">9852 JournalNode</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@hadoop005 hadoop]<span class="comment"># cd /opt/software/hadoop/sbin</span></span><br><span class="line">[root@hadoop005 sbin]<span class="comment"># hadoop-daemon.sh start journalnode</span></span><br><span class="line">starting journalnode, logging to /opt/software/hadoop/logs/hadoop-root-journalnode-hadoop003.out</span><br><span class="line">[root@hadoop005 sbin]<span class="comment"># jps</span></span><br><span class="line">4425 JournalNode</span><br><span class="line">4460 Jps</span><br><span class="line">4191 QuorumPeerMain</span><br></pre></td></tr></table></figure>
<h4 id="9-2-2-NameNode-格式化"><a href="#9-2-2-NameNode-格式化" class="headerlink" title="9.2.2.NameNode 格式化"></a>9.2.2.NameNode 格式化</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 sbin]<span class="comment"># cd ../</span></span><br><span class="line">[root@hadoop001 hadoop]<span class="comment"># hadoop namenode -format</span></span><br></pre></td></tr></table></figure>
<h4 id="9-2-3-同步-NameNode-元数据"><a href="#9-2-3-同步-NameNode-元数据" class="headerlink" title="9.2.3.同步 NameNode 元数据"></a>9.2.3.同步 NameNode 元数据</h4><p>同步 hadoop001 元数据到 hadoop002<br>主要是： dfs.namenode.name.dir， dfs.namenode.edits.dir 还应该确保共享存储目录下<br>(dfs.namenode.shared.edits.dir ) 包含 NameNode 所有的元数据。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 hadoop]<span class="comment"># pwd</span></span><br><span class="line">/opt/software/hadoop</span><br><span class="line">[root@hadoop001 hadoop]<span class="comment"># scp -r data/ root@hadoop002:/opt/software/hadoop</span></span><br><span class="line">in_use.lock 100% 14 0.0KB/s 00:00</span><br><span class="line">VERSION 100% 167 0.2KB/s 00:00</span><br><span class="line">seen_txid 100% 2 0.0KB/s 00:00</span><br><span class="line">VERSION 100% 220 0.2KB/s 00:00</span><br><span class="line">fsimage_0000000000000000000.md5 100% 62 0.1KB/s</span><br><span class="line">00:00</span><br><span class="line">fsimage_0000000000000000000 100% 306 0.3KB/s</span><br><span class="line">00:00</span><br><span class="line">[root@hadoop001 hadoop]<span class="comment">#</span></span><br></pre></td></tr></table></figure>
<h4 id="9-2-4-初始化-ZFCK"><a href="#9-2-4-初始化-ZFCK" class="headerlink" title="9.2.4.初始化 ZFCK"></a>9.2.4.初始化 ZFCK</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 bin]<span class="comment"># hdfs zkfc -formatZK</span></span><br></pre></td></tr></table></figure>
<h4 id="9-2-5-启动-HDFS-分布式存储系统"><a href="#9-2-5-启动-HDFS-分布式存储系统" class="headerlink" title="9.2.5.启动 HDFS 分布式存储系统"></a>9.2.5.启动 HDFS 分布式存储系统</h4><p>集群启动,在 hadoop001 执行 start-dfs.sh<br>集群关闭,在 hadoop001 执行 stop-dfs.sh</p>
<ul>
<li><strong>集群启动</strong></li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 sbin]<span class="comment"># start-dfs.sh</span></span><br><span class="line">[root@hadoop001 hadoop]<span class="comment"># start-dfs.sh</span></span><br><span class="line"><span class="comment">#补充集群启动信息</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>如果需要使用单独启动某个进程，请使用以下命令</strong></li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">NameNode(hadoop001, hadoop002):</span><br><span class="line">hadoop-daemon.sh start namenode</span><br><span class="line">DataNode(hadoop003, hadoop004, hadoop005):</span><br><span class="line">hadoop-daemon.sh start datanode</span><br><span class="line">JournamNode(hadoop003, hadoop003, hadoop005):</span><br><span class="line">hadoop-daemon.sh start journalnode</span><br><span class="line">ZKFC(hadoop001, hadoop002):</span><br><span class="line">hadoop-daemon.sh start zkfc</span><br></pre></td></tr></table></figure>
<h4 id="9-2-6-验证-namenode-datanode-zkfc"><a href="#9-2-6-验证-namenode-datanode-zkfc" class="headerlink" title="9.2.6.验证 namenode,datanode,zkfc"></a>9.2.6.验证 namenode,datanode,zkfc</h4><ul>
<li>HDFS进程检查</li>
</ul>
<p><code>hadoop001</code>和<code>hadoop002</code>节点的进程相同，<code>hadoop003</code>、<code>hadoop004</code>、<code>hadoop005</code>节点的进程相同,下面用hadoop001、hadoop003的jps 命令输出为示例</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 sbin]<span class="comment"># jps</span></span><br><span class="line">12712 Jps</span><br><span class="line">12593 DFSZKFailoverController</span><br><span class="line">12278 NameNode</span><br><span class="line"></span><br><span class="line">[root@hadoop003 ~]<span class="comment"># jps</span></span><br><span class="line">6348 JournalNode</span><br><span class="line">8775 Jps</span><br><span class="line">559 QuorumPeerMain</span><br><span class="line">8509 DataNode</span><br></pre></td></tr></table></figure>
<ul>
<li>webUI页面检查NN、DN节点情况</li>
</ul>
<p>hadoop001:<br><a href="http://hadoop001:50070/" target="_blank" rel="noopener">http://hadoop001:50070/</a></p>
<p>hadoop002:<br><a href="http://hadoop002:50070/" target="_blank" rel="noopener">http://hadoop002:50070/</a></p>
<h4 id="9-2-7-启动-YARN-框架"><a href="#9-2-7-启动-YARN-框架" class="headerlink" title="9.2.7.启动 YARN 框架"></a>9.2.7.启动 YARN 框架</h4><ul>
<li><strong>集群启动</strong><br>在hadoop001 启动 Yarn，命令所在目录： $HADOOP_HOME/sbin</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 hadoop]<span class="comment"># start-yarn.sh</span></span><br><span class="line">starting yarn daemons</span><br><span class="line">starting resourcemanager, logging to /opt/software/hadoop/logs/yarn-root-resourcemanagerhadoop001.out</span><br><span class="line">hadoop002: starting nodemanager, logging to /opt/software/hadoop/logs/yarn-rootnodemanager-hadoop002.out</span><br><span class="line">hadoop003: starting nodemanager, logging to /opt/software/hadoop/logs/yarn-rootnodemanager-hadoop003.out</span><br><span class="line">hadoop001: starting nodemanager, logging to /opt/software/hadoop/logs/yarn-rootnodemanager-hadoop001.out</span><br><span class="line">[root@hadoop001 hadoop]<span class="comment">#</span></span><br></pre></td></tr></table></figure>
<ul>
<li>在hadoop002 备机启动 RM</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@hadoop002 ~]<span class="comment"># yarn-daemon.sh start resourcemanager</span></span><br><span class="line">starting resourcemanager, logging to /opt/software/hadoop/logs/yarn-root-resourcemanagerhadoop002.out</span><br><span class="line">[root@hadoop002 ~]<span class="comment">#</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>如果需要单独启动，请使用下面命令</strong></li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">1) ResourceManager(hadoop001, hadoop002)</span><br><span class="line">yarn-daemon.sh start resourcemanager</span><br><span class="line">2) NodeManager(hadoop001, hadoop002, hadoop003)</span><br><span class="line">yarn-daemon.sh start nodemanager</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">- **如果需要关闭YARN集群可以使用下面命令**</span><br><span class="line">[root@hadoop001 sbin]<span class="comment"># stop-yarn.sh</span></span><br><span class="line"><span class="comment">#包含 namenode 的 resourcemanager 进程， datanode 的 nodemanager 进程</span></span><br><span class="line">[root@hadoop002 sbin]<span class="comment"># yarn-daemon.sh stop resourcemanager</span></span><br></pre></td></tr></table></figure>
<h4 id="9-2-7-验证-resourcemanager-nodemanager"><a href="#9-2-7-验证-resourcemanager-nodemanager" class="headerlink" title="9.2.7.验证 resourcemanager,nodemanager"></a>9.2.7.验证 resourcemanager,nodemanager</h4><ul>
<li>YARN进程验证</li>
</ul>
<p><code>hadoop001</code>和<code>hadoop002</code>节点的进程相同，<code>hadoop003</code>、<code>hadoop004</code>、<code>hadoop005</code>节点的进程相同,下面用hadoop001、hadoop003的jps 命令输出为示例</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># jps</span></span><br><span class="line">5043 NodeManager</span><br><span class="line">3683 QuorumPeerMain</span><br><span class="line">4679 DFSZKFailoverController</span><br><span class="line">5355 Jps</span><br><span class="line">4348 DataNode</span><br><span class="line">3981 JournalNode</span><br><span class="line">4943 ResourceManager</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@hadoop003 ~]<span class="comment"># jps</span></span><br><span class="line">4518 DataNode</span><br><span class="line">4679 NodeManager</span><br><span class="line">4425 JournalNode</span><br><span class="line">4794 Jps</span><br><span class="line">4191 QuorumPeerMain</span><br></pre></td></tr></table></figure>
<ul>
<li>页面验证</li>
</ul>
<p>ResourceManger（ Active） ： <a href="http://192.168.194.111:8088" target="_blank" rel="noopener">http://192.168.194.111:8088</a></p>
<p>ResourceManger（ Standby）： <a href="http://192.168.194.112:8088/cluster/cluster" target="_blank" rel="noopener">http://192.168.194.112:8088/cluster/cluster</a></p>

  </section>

</article>

<section class="read-more">
     
        
            <div class="read-more-item">
                <span class="read-more-item-dim">最近的文章</span>
                <h2 class="post-list__post-title post-title"><a href="/Hive/Hive--DataType.html" title="Hive的数据类型和分隔符">Hive的数据类型和分隔符</a></h2>
                <p class="excerpt">
                
                本文记录了Hive的数据类型和分隔符的最佳实践
                &hellip;
                </p>
                <div class="post-list__meta"><time datetime="2017-10-03T13:37:16.000Z" class="post-list__meta--date date">2017-10-03</time>
 &#8226; <span class="post-list__meta--tags tags">于&nbsp;
  <a class="tag-link" href="/tags/Hive/">Hive</a>

</span><a class="btn-border-small" href="/Hive/Hive--DataType.html">继续阅读</a></div>

            </div>
        

        
            <div class="read-more-item">
                <span class="read-more-item-dim">更早的文章</span>
                <h2 class="post-list__post-title post-title"><a href="/Hadoop/Hadoop--HDFS_Shell.html" title="Hadoop--HDFS的shell(命令行客户端)操作">Hadoop--HDFS的shell(命令行客户端)操作</a></h2>
                <p class="excerpt">
                
                Hadoop有两个常用命令  hadoop 和 hdfs
hadoop fs &amp;lt;====&amp;gt; hdfs dfs   这两个命令时相等的
1.hadoop 命令使用
命令行客户端支持的命令参数：
[root@hadoop001 bin]# hadoop fsUsage: hadoop fs 
                &hellip;
                </p>
                <div class="post-list__meta"><time datetime="2017-10-03T13:01:46.000Z" class="post-list__meta--date date">2017-10-03</time>
 &#8226; <span class="post-list__meta--tags tags">于&nbsp;
  <a class="tag-link" href="/tags/Hadoop/">Hadoop</a>

</span><a class="btn-border-small" href="/Hadoop/Hadoop--HDFS_Shell.html">继续阅读</a></div>

            </div>
        
   
</section>



<section class="post-comments">
  
<div id="cloud-tie-wrapper" class="cloud-tie-wrapper"></div>
<script src="https://img1.cache.netease.com/f2e/tie/yun/sdk/loader.js"></script>
<script>
var cloudTieConfig = {
  url: document.location.href, 
  sourceId: "",
  productKey: "7e456de65fb945698c448c72191454ce",
  target: "cloud-tie-wrapper"
};
var yunManualLoad = true;
Tie.loader("aHR0cHM6Ly9hcGkuZ2VudGllLjE2My5jb20vcGMvbGl2ZXNjcmlwdC5odG1s", true);
</script>

</section>





            <footer class="footer">
    <span class="footer__copyright">
        本站点采用<a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议</a>
    </span>
    <span class="footer__copyright">
        基于 <a href="http://hexo.io">Hexo</a> 搭建，感谢 <a href="https://pages.github.com/">GitHub Pages</a> 提供免费的托管服务
    </span>
    <span class="footer__copyright">
        &copy; 2018 - 本站由 <a href="/">XieJM</a> 创建,
        使用<a href="https://github.com/skx926/hexo-theme-vno">hexo-theme-vno</a>主题
    </span>
</footer>

        </div>
    </div>

    <script src="https://code.jquery.com/jquery-2.1.4.min.js"></script>
    <script src="/js/main.js"></script>

     
<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	ga('create', 'UA-73055515-1', 'auto');
	ga('send', 'pageview');
</script>

</body>
</html>
