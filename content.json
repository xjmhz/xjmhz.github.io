[{"title":"Hive的JDBC使用Java API编程","date":"2017-10-11T03:02:00.000Z","path":"Hive/HIve--JDBC API.html","text":"在之前的UDF函数编程的项目中再新建一个class下面只是一个简单的模板，实际应用的话还可以再加工下，比如执行语句作为一个参数传入进来，封装成一个类。 1234567891011121314151617181920212223242526272829303132333435363738394041package com.xiejm.bigdata.hive;import java.sql.SQLException;import java.sql.Connection;import java.sql.ResultSet;import java.sql.Statement;import java.sql.DriverManager;public class HiveJDBC &#123; public static String driverName=\"org.apache.hive.jdbc.HiveDriver\"; public static String url=\"jdbc:hive2://192.168.2.10:10000/default\"; public static String username=\"hadoop\"; public static String password=\"\"; public static void main(String[] args) throws Exception &#123; try&#123; Class.forName(driverName); Connection connection = DriverManager.getConnection(url,username,password); Statement stms = connection.createStatement(); ResultSet rs = stms.executeQuery(\"select * from emp\"); while(rs.next()) &#123; System.out.println(rs.getInt(1) + \" , \" + rs.getString(2)); &#125; System.out.println(\"Table emp select successfully.\"); &#125;catch(Exception e) &#123; //将可能抛出的Exception异常捕获 System.out.println(\"Got a Exception：\" + e.getMessage()); //打印捕获的异常的堆栈信息，从堆栈信息中可以发现异常发生的位置和原因 e.printStackTrace(); throw e; //不做进一步处理，将异常向外抛出 &#125;finally&#123; rs.close(); stms.close(); connection.close(); &#125; &#125;&#125;","tags":[{"name":"Hive","slug":"Hive","permalink":"http://xiejm.com/tags/Hive/"}]},{"title":"Hive的元数据管理","date":"2017-10-09T09:38:12.000Z","path":"Hive/Hive--metastore.html","text":"我们知道，Hive的元数据并不存放在HDFS上，而是存放在传统的RDBMS中，典型的如MySQL，这里我们以MySQL为元数据库，结合1.1.0版本的Hive为例进行说明。连接上MySQL后可以看到Hive元数据对应的表有36个，以下是部分主要表的简要说明。 表名 说明 关联键 BUCKETING_COLS 存储bucket字段信息，通过SD_ID与其他表关联 CDS 一个字段CD_ID，与SDS表关联 COLUMNS_V2 Hive表字段信息(字段注释，字段名，字段类型，字段序号) DBS Hive中所有数据库的基本信息 GLOBAL_PRIVS 全局变量，与表无关 PARTITIONS Hive表分区信息(创建时间，具体的分区) PARTITION_KEYS Hive分区表分区字段(名称，类型，comment，序号) PARTITION_KEY_VALS Hive表分区名(键值，序号) PARTITION_PARAMS 存储某分区相关信息，包括文件数，文件大小，记录条数等。通过PART_ID关联 ROLES 角色表，和GLOBAL_PRIVS配合，与表无关 SDS 所有hive表、表分区所对应的hdfs数据目录和数据格式 SD_ID,SERDE_ID SEQUENCE_TABLE 存储sqeuence相关信息，与表无关 SERDES Hive表序列化反序列化使用的类库信息 SERDE_PARAMS 序列化反序列化信息，如行分隔符、列分隔符、NULL的表示字符等 SORT_COLS Hive表SORTED BY字段信息(字段名，sort类型，字段序号) TABLE_PARAMS 表级属性，如是否外部表，表注释等 TBLS 所有hive表的基本信息 TBL_PRIVS 表赋权限相关信息，通过TBL_ID关联 VERSION 存储Hive版本的元数据表","tags":[{"name":"Hive","slug":"Hive","permalink":"http://xiejm.com/tags/Hive/"}]},{"title":"Hive的分区表","date":"2017-10-09T07:53:44.000Z","path":"Hive/Hive--partition.html","text":"本文主要是对Hive分区表的总结，Hive分区表有三种类型：静态单级分区表、静态多级分区表、动态分区表在实际工作中Hive的分区表应用非常多，特别是动态分区表。 直接上干货！ 静态分区表的使用单级分区表创建新建一个分区表，并导入数据。 12345678910111213create table order_partition(order_number string,event_time string)PARTITIONED BY(event_month string,day STRING)row format delimited fields terminated by '\\t';load data local inpath '/root/order_created.txt' overwrite into table order_partition PARTITION(event_month='201709');Loading data to table default.order_partition partition (event_month=201709)Partition default.order_partition&#123;event_month=201709&#125; stats: [numFiles=1, numRows=0, totalSize=208, rawDataSize=0]OKTime taken: 1.121 seconds 从上面的语句可以看出与创建普通表的区别是多了PARTITIONED BY 在创建表时候，使用PARTITIONED BY关键字来指定该表为分区表，后面括号中指定了分区的字段和类型，分区字段可以有多个，在HDFS中对应多级目录。 以上语句如果报错显示乱码，解决办法：在MySQL中修改Hive的元数据 数据库中的分区表的编码 1234alter database hive3 character set latin1;use hive3;alter table PARTITIONS convert to character set latin1;alter table PARTITION_KEYS convert to character set latin1; 用beeline查询分区表记录 123456789101112130: jdbc:hive2://hadoop001:10000/default&gt; select * from order_partition;INFO : OK+-------------------------------+-----------------------------+------------------------------+--+| order_partition.order_number | order_partition.event_time | order_partition.event_month |+-------------------------------+-----------------------------+------------------------------+--+| 10703007267488 | 2014-05-01 06:01:12.334+01 | 201709 || 10101043505096 | 2014-05-01 07:28:12.342+01 | 201709 || 10103043509747 | 2014-05-01 07:50:12.33+01 | 201709 || 10103043501575 | 2014-05-01 09:27:12.33+01 | 201709 || 10104043514061 | 2014-05-01 09:03:12.324+01 | 201709 |+-------------------------------+-----------------------------+------------------------------+--+5 rows selected (0.141 seconds) 查看下order_partition.txt ，用来与表做对比 123456[hadoop@hadoop001 ~]$ cat data/order_created.txt10703007267488 2014-05-01 06:01:12.334+0110101043505096 2014-05-01 07:28:12.342+0110103043509747 2014-05-01 07:50:12.33+0110103043501575 2014-05-01 09:27:12.33+0110104043514061 2014-05-01 09:03:12.324+01 从select * from order_partition; 和 cat data/order_created.txt的结果对比得出一个结果：分区列不是表中的一个实际的列，其实是一个伪列 使用ALTER TABLE修改分区如果我们在HDFS中创建了分区表目录，并通过HDFS导入数据到该目录，那么在Hive端是查询不到该分区表的数据的我们需要关联该数据 使用下列语句,具体语法看官网wiki 方法一： 1ALTER TABLE order_partition ADD IF NOT EXISTS PARTITION(event_month='201709') 方法二： 作用：将没有metastore中的分区表信息，添加到metastore中 1MSCK REPAIR order_partition; 不推荐使用，太暴力了 为什么暴力：MSCK的操作是表级别的，假设metastore数据存了一年，一天1个分区，一年就是365分区，这样MSCK一执行很可能把其他不需要刷新元数据信息的表也给刷新了，所以存在风险。 使用INSERT添加分区往分区中追加数据： 1INSERT INTO TABLE order_partition PARTITION (month = '2017-10',day = '2017-10-01') 覆盖分区数据： 1INSERT overwrite TABLE order_partition PARTITION (month = '2017-10',day = '2017-10-01') 查询分区表信息查看一个分区对应的HDFS路径信息 12345show partitions order_partition;OKevent_month=201709Time taken: 0.041 seconds, Fetched: 1 row(s) 静态多级分区表上面已经说了静态分区表的一些使用方法，这里在补充一个多级分区表的创建 12345678create table order_mulit_partition(order_number string,event_time string)PARTITIONED BY(event_month string,day STRING)row format delimited fields terminated by '\\t';load data local inpath '/root/order_created.txt' overwrite into table order_partition PARTITION(event_month='201709',day='2017-09-15'); 创建分区表返回信息 12345Loading data to table default.order_mulit_partition partition (event_month=201709, day=2017-09-15)Partition default.order_mulit_partition&#123;event_month=201709, day=2017-09-15&#125; stats: [numFiles=1, numRows=0, totalSize=208, rawDataSize=0]OKTime taken: 1.121 seconds` 说明：上面的表order_mulit_partition分区event_month=’201709’,day=’2017-09-15’对应HDFS上的路径为：/user/hive/warehouse/default.db/order_partition/event_month=201709/day=2017-09-15/，当查询中指定了event_month=’201709’ AND dday=’2017-09-15’,MapReduce直接从该目录中读取数据，如果只指定了event_month=’201709’，那么MapReduce将/month=2015-06/下所有的子目录都作为Input。 查看表 12345678hive (default)&gt; select * from order_mulit_partition;OK10703007267488 2014-05-01 06:01:12.334+01 201709 2017-09-1510101043505096 2014-05-01 07:28:12.342+01 201709 2017-09-1510103043509747 2014-05-01 07:50:12.33+01 201709 2017-09-1510103043501575 2014-05-01 09:27:12.33+01 201709 2017-09-1510104043514061 2014-05-01 09:03:12.324+01 201709 2017-09-15Time taken: 0.325 seconds, Fetched: 5 row(s) 可以看到有两个字段,再执行一次载入数据 1load data local inpath '/root/order_created.txt' overwrite into table order_mulit_partition PARTITION(event_month='201709',day='2017-09-16'); 查看表 123456789101112hive (default)&gt; select * from order_mulit_partition;OK10703007267488 2014-05-01 06:01:12.334+01 201709 2017-09-1510101043505096 2014-05-01 07:28:12.342+01 201709 2017-09-1510103043509747 2014-05-01 07:50:12.33+01 201709 2017-09-1510103043501575 2014-05-01 09:27:12.33+01 201709 2017-09-1510104043514061 2014-05-01 09:03:12.324+01 201709 2017-09-1510703007267488 2014-05-01 06:01:12.334+01 201709 2017-09-1610101043505096 2014-05-01 07:28:12.342+01 201709 2017-09-1610103043509747 2014-05-01 07:50:12.33+01 201709 2017-09-1610103043501575 2014-05-01 09:27:12.33+01 201709 2017-09-1610104043514061 2014-05-01 09:03:12.324+01 201709 2017-09-16 再来查下order_mulit_partition分区表信息 12345show partitions order_mulit_partition;OKevent_month=201709/day=2017-09-15event_month=201709/day=2017-09-16Time taken: 0.033 seconds, Fetched: 2 row(s) 在多分区使用的时候注意：每个分区字段要写明，顺序等等 动态分区表这种方式在工作中才是常用的,动态分区无需手工指定数据导入的具体分区，而由select语句中的字段自行决定导出到哪一个分区中，并自动创建相应的分区，使用上更加方便便捷。 假设有一个需求：按照不同部门作为分区导入数据到目标表 如果这张表有很多列，全部这样写SQL 语句效率太差,那么我们就可以使用动态分区表 创建分区表12345678910create table emp_dynamic_partition(empno int, ename string, job string, mgr int, hiredate string, sal double, comm double)PARTITIONED BY(deptno int)row format delimited fields terminated by '\\t'; 使用动态方式导入数据导入数据前先设置动态分区的模式： 1hive (default)&gt; set hive.exec.dynamic.partition.mode=nonstrict; 开始导入数据 12345678910111213141516171819202122232425262728293031323334hive (default)&gt; insert into table emp_dynamic_partition partition(deptno) &gt; select empno , ename , job , mgr , hiredate , sal , comm, deptno from emp;Query ID = hadoop_20170925072626_d98526f6-67aa-4593-b723-5a5bd4e422a8Total jobs = 3Launching Job 1 out of 3Number of reduce tasks is set to 0 since there's no reduce operatorStarting Job = job_1506263819287_0002, Tracking URL = http://hadoop001:8088/proxy/application_1506263819287_0002/Kill Command = /home/hadoop/app/hadoop//bin/hadoop job -kill job_1506263819287_0002Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 02017-09-25 07:28:03,866 Stage-1 map = 0%, reduce = 0%2017-09-25 07:28:10,214 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 2.37 secMapReduce Total cumulative CPU time: 2 seconds 370 msecEnded Job = job_1506263819287_0002Stage-4 is selected by condition resolver.Stage-3 is filtered out by condition resolver.Stage-5 is filtered out by condition resolver.Moving data to: hdfs://hadoop001:9000/user/hive/warehouse/emp_dynamic_partition/.hive-staging_hive_2017-09-25_07-27-57_012_6944175324299480316-1/-ext-10000Loading data to table default.emp_dynamic_partition partition (deptno=null) Time taken for load dynamic partitions : 542 Loading partition &#123;deptno=20&#125; Loading partition &#123;deptno=__HIVE_DEFAULT_PARTITION__&#125; Loading partition &#123;deptno=10&#125; Loading partition &#123;deptno=30&#125; Time taken for adding to write entity : 3Partition default.emp_dynamic_partition&#123;deptno=10&#125; stats: [numFiles=1, numRows=3, totalSize=130, rawDataSize=127]Partition default.emp_dynamic_partition&#123;deptno=20&#125; stats: [numFiles=1, numRows=5, totalSize=214, rawDataSize=209]Partition default.emp_dynamic_partition&#123;deptno=30&#125; stats: [numFiles=1, numRows=6, totalSize=275, rawDataSize=269]Partition default.emp_dynamic_partition&#123;deptno=__HIVE_DEFAULT_PARTITION__&#125; stats: [numFiles=1, numRows=1, totalSize=44, rawDataSize=43]MapReduce Jobs Launched: Stage-Stage-1: Map: 1 Cumulative CPU: 2.37 sec HDFS Read: 4705 HDFS Write: 961 SUCCESSTotal MapReduce CPU Time Spent: 2 seconds 370 msecOKempno ename job mgr hiredate sal comm deptnoTime taken: 16.388 seconds 从上面的mapreduce运行输出结果可以看出，已经根据deptno 动态分区了 查询已导入数据1234567891011121314151617181920210: jdbc:hive2://hadoop001:10000/default&gt; select * from emp_dynamic_partition ;+------------------------------+------------------------------+----------------------------+----------------------------+---------------------------------+----------------------------+-----------------------------+-------------------------------+--+| emp_dynamic_partition.empno | emp_dynamic_partition.ename | emp_dynamic_partition.job | emp_dynamic_partition.mgr | emp_dynamic_partition.hiredate | emp_dynamic_partition.sal | emp_dynamic_partition.comm | emp_dynamic_partition.deptno |+------------------------------+------------------------------+----------------------------+----------------------------+---------------------------------+----------------------------+-----------------------------+-------------------------------+--+| 7782 | CLARK | MANAGER | 7839 | 1981-6-9 | 2450.0 | NULL | 10 || 7839 | KING | PRESIDENT | NULL | 1981-11-17 | 5000.0 | NULL | 10 || 7934 | MILLER | CLERK | 7782 | 1982-1-23 | 1300.0 | NULL | 10 || 7369 | SMITH | CLERK | 7902 | 1980-12-17 | 800.0 | NULL | 20 || 7566 | JONES | MANAGER | 7839 | 1981-4-2 | 2975.0 | NULL | 20 || 7788 | SCOTT | ANALYST | 7566 | 1987-4-19 | 3000.0 | NULL | 20 || 7876 | ADAMS | CLERK | 7788 | 1987-5-23 | 1100.0 | NULL | 20 || 7902 | FORD | ANALYST | 7566 | 1981-12-3 | 3000.0 | NULL | 20 || 7499 | ALLEN | SALESMAN | 7698 | 1981-2-20 | 1600.0 | 300.0 | 30 || 7521 | WARD | SALESMAN | 7698 | 1981-2-22 | 1250.0 | 500.0 | 30 || 7654 | MARTIN | SALESMAN | 7698 | 1981-9-28 | 1250.0 | 1400.0 | 30 || 7698 | BLAKE | MANAGER | 7839 | 1981-5-1 | 2850.0 | NULL | 30 || 7844 | TURNER | SALESMAN | 7698 | 1981-9-8 | 1500.0 | 0.0 | 30 || 7900 | JAMES | CLERK | 7698 | 1981-12-3 | 950.0 | NULL | 30 || 8888 | HIVE | PROGRAM | 7839 | 1988-1-23 | 10300.0 | NULL | NULL |+------------------------------+------------------------------+----------------------------+----------------------------+---------------------------------+----------------------------+-----------------------------+-------------------------------+--+ 再来看下HDFS上的文件，就更加清楚了 123456789[hadoop@hadoop001 ~]$ hadoop dfs -ls /user/hive/warehouse/emp_dynamic_partitionDEPRECATED: Use of this script to execute hdfs command is deprecated.Instead use the hdfs command for it.Found 4 itemsdrwxr-xr-x - hadoop supergroup 0 2017-09-25 07:28 /user/hive/warehouse/emp_dynamic_partition/deptno=10drwxr-xr-x - hadoop supergroup 0 2017-09-25 07:28 /user/hive/warehouse/emp_dynamic_partition/deptno=20drwxr-xr-x - hadoop supergroup 0 2017-09-25 07:28 /user/hive/warehouse/emp_dynamic_partition/deptno=30drwxr-xr-x - hadoop supergroup 0 2017-09-25 07:28 /user/hive/warehouse/emp_dynamic_partition/deptno=__HIVE_DEFAULT_PARTITION__ 到此，我们看到了动态分区的好处了 更多关于分区的介绍，可参考官方文档：https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-AddPartitions","tags":[{"name":"Hive","slug":"Hive","permalink":"http://xiejm.com/tags/Hive/"}]},{"title":"Hive--数据库和表","date":"2017-10-08T10:23:30.000Z","path":"Hive/Hive--DDL.html","text":"本文介绍Hive的数据存储和一些常用DDL、DML操作 数据存储首先需要清楚Hive中的数据存储的位置，一般生产环境中元数据是存放在MySQL等数据库中，因为这些数据要不断更新、修改，不适合存储在HDFS中。而真正的数据是存在HDFS中，这样更有利于对数据做分布式运算。 Hive中主要包括四类数据模型：Hive中主要包含以下几种数据模型：Table(表)，External Table(外部表)，Partition(分区)，Bucket(桶) Table(表)：Hive中的表和关系型数据库中的表在概念上很相似，每个表在HDFS中都有对应的目录来存储表数据，这个目录可以通过${HIVE_HOME}/conf/hive-site.xml配置文件中的hive.metastore.warehouse.dir属性来配置，这个属性默认的值是/user/hive/warehouse(这个目录在 HDFS上)，可以根据实际需要修改这个路径。 External Table(外部表)：Hive中外部表与表相似，但是外部表的数据不是存储在自己所属的目录中，而是存在其他地方。当你要删除外部表时，这个外部表所对应的数据文件是不会贝删除的，删除的仅仅是与之对应的元数据信息。 Partition(分区)：在Hive中，所有分区数据都存储在表路径的对应子目录中。 Bucket(桶)：对指定的列计算Hash，根据Hash切分数据，目的是为了并行，每个桶对应一个文件。 Hive的数据默认存储有一个根目录，在hive-site.xml中，由参数hive.metastore.warehouse.dir指定。默认值为/user/hive/warehouse. For example ： 如果创建一个表：emp，当创建成功后在HDFS中会创建/user/hive/warehouse/emp目录(这里假定hive.metastore.warehouse.dir配置为/user/hive/warehouse),emp表的所有数据都存在这个目录中。 如果创建分区表order_partition，指定分区列event_month=201405 ，当创建成功后在HDFS中对应的目录是/user/hive/warehouse/order_partition/event_month=201405 如果将emp表分散到8个桶中，首先对ID列的值计算Hash，对应的Hash值为0-8的HDFS存储路径：/user/hive/warehouse/emp/part-00000;而hash值为2的数据存储的HDFS 目录为：/user/hive/warehouse/emp/part-00002。 Hive表结构 Hive的DDL和DML操作名词解释 DDL：Data Definition Language，数据定义语言 在关系型数据库中的CREATE、DELETE、ALTER，这些都是DDL 打开官网的Hive wiki。在User Documentation有一个DDL文档。 DDL概述常用的HiveQL DDL includeing: CREATE DATABASE,TABLE,FUNCTION DROP DATABASE,TABLE ALTER DATABASE,TABLE MSCK PEPAIR TABLE SHOW DATABASES,TABLES,FUNCTIONS DDL示例以下是一些常用的DDL操作示例。都来自于官网文档，所有官网文档是第一手资料，一定要会查官网文档。 【重点】当使用hive中的QL语句时，一定要知道这条语句对应的元数据信息时怎么存储的 数据库DDL create database 基本语法 123456789CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name [COMMENT database_comment] [LOCATION hdfs_path] [WITH DBPROPERTIES (property_name=property_value, ...)]; 说明： - IF NOT EXISTS：如果不存在则创建 - COMMENT：注释 - LOCATION：数据库存放目录 - WITH DBPROPERTIES：拓展信息，key/value 我们知道hive的数据存储在HDFS件中，默认情况下，其存储在hive配置文件hive-site.xml参数hive.metastore.warehouse.dir指定的目录下，默认值为/user/hive/warehouse，当然我们也可以在创建数据库时，指定LOCATION值来修改默认的路径 创建数据库 123hive&gt; CREATE DATABASE IF NOT EXISTS tsetdb &gt; COMMENT 'add comment:it is my database' &gt; WITH DBPROPERTIES ('create_time'='2017-09-19','creator'='xiejm'); 通过命令dfs -ls /user/hive/warehouse查看文件是否存在 12345hive&gt; dfs -ls /user/hive/warehouse;Found 1 itemsdrwxr-xr-x - root supergroup 0 2017-09-19 13:39 /user/hive/warehouse/testdb.dbhive&gt; 创建数据库指定的存放目录（LOCATION参数) 1234hive&gt; CREATE DATABASE IF NOT EXISTS tsetdb_local &gt; COMMENT 'tsetdb_local ' &gt; LOCATION '/user/hadoop' &gt; WITH DBPROPERTIES ('create_time'='2017-09-19'); 通过命令dfs -ls /user/hadoop查看，发现并没有生产tsetdb_local.db目录 再使用命令DESC database database_name查看数据库的创建脚本，对比添加LOCATION参数不同之处 12345678hive&gt; desc database testdb;OKtestdb hdfs://hadoop001:9000/user/hive/warehouse/testdb.db root USERTime taken: 0.009 seconds, Fetched: 1 row(s)hive&gt; desc database tsetdb_local;OKtsetdb_local tsetdb_local hdfs://hadoop001:9000/user/hadoop root USERTime taken: 0.026 seconds, Fetched: 1 row(s) 我们还可以通过到MySQL命令行查hive的元数据信息，查看刚创建的数据库 12345678910111213141516171819202122232425262728293031mysql&gt; select * from DBS \\G;*************************** 1. row *************************** DB_ID: 1 DESC: Default Hive databaseDB_LOCATION_URI: hdfs://hadoop001:9000/user/hive/warehouse NAME: default OWNER_NAME: public OWNER_TYPE: ROLE*************************** 2. row *************************** DB_ID: 6 DESC: NULLDB_LOCATION_URI: hdfs://hadoop001:9000/user/hive/warehouse/hive.db NAME: hive OWNER_NAME: root OWNER_TYPE: USER*************************** 3. row *************************** DB_ID: 17 DESC: NULLDB_LOCATION_URI: hdfs://hadoop001:9000/user/hive/warehouse/testdb.db NAME: testdb OWNER_NAME: root OWNER_TYPE: USER*************************** 4. row *************************** DB_ID: 21 DESC: tsetdb_local DB_LOCATION_URI: hdfs://hadoop001:9000/user/hadoop NAME: tsetdb_local OWNER_NAME: root OWNER_TYPE: USER9 rows in set (0.00 sec) 显示现有数据库 1hive&gt; show databases; 通过上述操作我们发现，default为默认数据库，它在HDFS中的存储路径是/user/hive/warehouse,也就是我们配置文件中指定的默认路径。 根据条件查询现有数据库 1234567hive&gt; show databases like 'test*';OKtestdbtestdb_anthinfotestdb_commenttestdb_localTime taken: 0.005 seconds, Fetched: 4 row(s) 修改数据库 12ALTER (DATABASE|SCHEMA) database_name SET DBPROPERTIES (property_name=property_value, ...);ALTER (DATABASE|SCHEMA) database_name SET OWNER [USER|ROLE] user_or_role; 修改数据的create_time属性 123hive&gt; alter database testdb set dbproperties ('create_time' ='2017-9-11');OKTime taken: 0.046 seconds 7.删除数据库 1DROP (DATABASE|SCHEMA) [IF EXISTS] database_name [RESTRICT|CASCADE]; 当删除database_name数据库，默认情况下只能删除空数据库，当数据库为非空时，可以使用drop database databse_name cascade 级联删除。实际生产中不建议使用cascade参数进行暴力删除 12345678910111213# 删除空数据库hive&gt; drop database tsetdb_local;OKTime taken: 0.507 seconds#删除非空表报错hive&gt; drop database user;FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. InvalidOperationException(message:Database user is not empty. One or more tables exist.)#暴力删除表 加上cascade参数。hive&gt; drop database user cascade;OKTime taken: 1.441 seconds 表DDL语法基本结构 123456789101112131415161718192021222324252627282930CREATE [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name [(col_name data_type [COMMENT col_comment], ...)] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] [ [ROW FORMAT row_format] [STORED AS file_format] ] [LOCATION hdfs_path] [AS select_statement]; 说明： - EXTERNAL：外部表 - IF NOT EXISTS：表不存在创建 - db_name：表所属数据库 - COMMENT col_comment：列注释 - COMMENT table_comment：表注释 - PARTITIONED BY：分区字段 - ROW FORMAT row_format:行的数据格式 - STORED AS file_format:文件存储格式 - LOCATION hdfs_path：存放路径 - AS select_statement：查询语句为结果集CREATE TABLE [IF NOT EXISTS] [db_name.]table_name LIKE existing_table_or_view_name [LOCATION hdfs_path]; 说明： - IF NOT EXISTS：表不存在创建 - db_name：表所属数据库 - existing_table_or_view_name：结果集为存在的表或者师徒 - LOCATION hdfs_path：存放路径","tags":[{"name":"Hive","slug":"Hive","permalink":"http://xiejm.com/tags/Hive/"}]},{"title":"hive内部表与外部表区别详细介绍","date":"2017-10-08T06:42:30.000Z","path":"Hive/Hive--ManagerTableAndExtendTable.html","text":"本文记录了Hive的内部表与外部表区别 内部表 首先我们用下面语句在Hive中创建一个表： 123456create table emp(empno int,ename string,job string,mgr int,hiredate string,sal double,deptno int)row format delimited fields terminated by '\\t';OKTime taken: 0.359 seconds 接着我们就在Hive里面创建了一张普通的表，现在我们给这个表导入数据： 12345678910111213141516171819202122232425load data local inpath '/root/emp.txt' overwrite into table emp;Loading data to table default.empTable default.emp stats: [numFiles=1, numRows=0, totalSize=700, rawDataSize=0]OKTime taken: 0.82 secondshive&gt; select * from emp;OK7369 SMITH CLERK 7902 1980-12-17 800.0 NULL7499 ALLEN SALESMAN 7698 1981-2-20 1600.0 3007521 WARD SALESMAN 7698 1981-2-22 1250.0 5007566 JONES MANAGER 7839 1981-4-2 2975.0 NULL7654 MARTIN SALESMAN 7698 1981-9-28 1250.0 14007698 BLAKE MANAGER 7839 1981-5-1 2850.0 NULL7782 CLARK MANAGER 7839 1981-6-9 2450.0 NULL7788 SCOTT ANALYST 7566 1987-4-19 3000.0 NULL7839 KING PRESIDENT NULL 1981-11-17 5000.0 NULL7844 TURNER SALESMAN 7698 1981-9-8 1500.0 07876 ADAMS CLERK 7788 1987-5-23 1100.0 NULL7900 JAMES CLERK 7698 1981-12-3 950.0 NULL7902 FORD ANALYST 7566 1981-12-3 3000.0 NULL7934 MILLER CLERK 7782 1982-1-23 1300.0 NULL8888 HIVE PROGRAM 7839 1988-1-23 10300.0 NULLTime taken: 1.027 seconds, Fetched: 15 row(s) 查看emp表存储的HDFS文件 123hive&gt; dfs -ls /user/hive/warehouse/emp;Found 1 items-rwxr-xr-x 1 root supergroup 700 2017-09-20 17:13 /user/hive/warehouse/emp/emp.txt 接着我们来删除emp 12345678910##############删除表hive&gt; drop table emp;OKTime taken: 0.239 seconds################查emo表的文件提示没有这个目录hive&gt; dfs -ls /user/hive/warehouse/emp;ls: `/user/hive/warehouse/emp': No such file or directoryCommand failed with exit code = 1Query returned non-zero code: 1, cause: null 看到没，当我们把emp表删除后，再去查HDFS文件提示目录不存在。表数据已经被删除了。如果你的Hadoop启动垃圾箱机制，那么drop table 命令将会把表的所有移动到垃圾箱。 外部表 现在我们来创建一个外部表： 1234hive&gt; create external table emp_external( &gt; empno int,ename string,job string,mgr int,hiredate string,sal double,deptno int);OKTime taken: 0.213 seconds 通过上面的创建表命令我们发现，创建表和外部表的区别，创建外部表多了external关键字当然我们在创建外部表时还可以加一个location &#39;hdfs PATH&#39;参数来指定目录默认情况下，Hive将在HDFS上的/user/hive/warehouse/文件夹下以外部表的表名创建一个文件夹，并将属于这个表的数据存放在这里接着使用hive 中的dfs命令查下 1234hive&gt; dfs -ls /user/hive/warehouse;Found 2 itemsdrwxr-xr-x - root supergroup 0 2017-09-20 17:45 /user/hive/warehouse/emp_externaldrwxr-xr-x - root supergroup 0 2017-09-13 21:46 /user/hive/warehouse/hive.db 导入表数据 和创建表的导入数据到表一样，将本地的数据导入到外部表 12345hive&gt; load data local inpath '/root/emp.txt' overwrite into table emp_external;Loading data to table default.emp_externalTable default.emp_external stats: [numFiles=1, numRows=0, totalSize=700, rawDataSize=0]OKTime taken: 1.087 seconds 12345############再emp_external目录下多了一个数据文件hive&gt; dfs -ls /user/hive/warehouse/emp_external;Found 1 items-rwxr-xr-x 1 root supergroup 700 2017-09-20 17:56 /user/hive/warehouse/emp_external/emp.txthive&gt; 删除外部表 123456789##############删除表hive&gt; drop table emp_external;OKTime taken: 0.326 secondshive&gt; dfs -ls /user/hive/warehouse/emp_external;Found 1 items-rwxr-xr-x 1 root supergroup 700 2017-09-20 17:56 /user/hive/warehouse/emp_external/emp.txt 通过上面删除表后查看HDFS文件发现表被删除了，但是表对应的数据文件并没有被删除 总结 外部表中的数据并不是由它自己来管理的！而表则不一样； 在删除表的时候，Hive将会把属于表的元数据和数据全部删掉；而删除外部表的时候，Hive仅仅删除外部表的元数据，数据是不会删除的！ 在与第三方共用数据时，可以使用外部表。","tags":[{"name":"Hive","slug":"Hive","permalink":"http://xiejm.com/tags/Hive/"}]},{"title":"Hive--UDF开发","date":"2017-10-08T06:39:22.000Z","path":"Hive/Hive--UDF.html","text":"前言Hive中，除了提供丰富的内置函数之外还可以通过实现用户定义函数（User-Defined Functions，UDF）进行扩展（事实上，大多数Hive功能都是通过扩展UDF实现的）。 开发自定义UDF函数有两种方式，一个是继承org.apache.hadoop.hive.ql.exec.UDF，另一个是继承org.apache.hadoop.hive.ql.udf.generic.GenericUDF；并重载evaluate方法。Hive API提供@Description声明，使用声明可以在代码中添加UDF的具体信息。在Hive中可以使用DESC、DESCRIBE来展现这些信息。 如果是针对简单的数据类型（比如String、Integer等）可以使用UDF，如果是针对复杂的数据类型（比如Array、Map、Struct等），可以使用GenericUDF，另外，GenericUDF还可以在函数开始之前和结束之后做一些初始化和关闭的处理操作。 Hive的源码本身就是编写UDF最好的参考资料。在Hive源代码中很容易就能找到与需求功能相似的UDF实现，只需要复制过来，并加以适当的修改就可以满足需求。 环境 CentOS6.X JDK1.8 Hadoop2.6.0-cdh5.7.0 Hive-1.1.0-cdh5.7.0 Maven3.3.9 编写UDF类1.首先在IDEA中新建一个maven项目2.pom.xml中添加相关依赖 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.xiejm.bigdata&lt;/groupId&gt; &lt;artifactId&gt;hive-train&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;hive-train&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;hadoop.version&gt;2.6.0-cdh5.7.0&lt;/hadoop.version&gt; &lt;hive.version&gt;1.1.0-cdh5.7.0&lt;/hive.version&gt; &lt;/properties&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;cloudera&lt;/id&gt; &lt;url&gt; https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;$&#123;hadoop.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!--cdh的hive依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-accumulo-handler&lt;/artifactId&gt; &lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-ant&lt;/artifactId&gt; &lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-beeline&lt;/artifactId&gt; &lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-cli&lt;/artifactId&gt; &lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-common&lt;/artifactId&gt; &lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-contrib&lt;/artifactId&gt; &lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-exec&lt;/artifactId&gt; &lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.10&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 3.新建一个Java Class，命名为ToLower 123456789101112131415161718192021222324252627package com.xiejm.bigdata.hive;import org.apache.hadoop.io.Text;import org.apache.hadoop.hive.ql.exec.Description;import org.apache.hadoop.hive.ql.exec.UDF;/** * Created by jimmy on 2017/9/29. * 这个函数是将字符串全部转化为大写字母 * add jar samplecode.jar; * create temporary function toupper as 'com.xiejm.bigdata.hive.ToUpper'; */@Description(name = \"ToUpper\", value = \"_FUNC_(str) - Converts a string to uppercase\", extended = \"Example:\\n \" + \" &gt; SELECT _FUNC_(str) FROM src LIMIT 1;\\n\")public class ToUpper extends UDF &#123; public Text evaluate(Text input) &#123; Text result = new Text(\"\"); if (input != null) &#123; result.set(input.toString().toUpperCase()); &#125; return result; &#125;&#125; 4.在IDEA中编译该项目，将编译后的jar包上传到Linux中5.hive中加载jar包 1234567891011121314151617181920## 添加UDF jar包hive (default)&gt; add jar /root/hive-train-1.0.jarAdded [/root/hive-train-1.0.jar] to class pathAdded resources: [/root/hive-train-1.0.jar]## 创建UDF函数hive (default)&gt; create temporary function toupper as 'com.xiejm.bigdata.hive.ToUpper';OKTime taken: 0.005 seconds##查看UDF函数，显示函数注释hive (default)&gt; desc function extended toupper;OKtoupper(str) - Converts a string to uppercaseExample: &gt; SELECT toupper(str) FROM src LIMIT 1;Time taken: 0.007 seconds, Fetched: 4 row(s)hive (default)&gt; exit; 上面的方法介绍了如何添加临时的UDF函数，那么如果要永久加载这个函数，我们该怎么做呢？ 步骤和上面的方法是一样的，只需把CREATE语句中的TEMPORARY关键字删除即可。 当不需要这个函数了，可以把这个函数从Hive中删除,使用下面的语法格式 1DROP [TEMPORARY] FUNCTION function_name 进阶：将自定义UDF编译到hive源码中上面的方法每次使用都要add,create一下，还是很麻烦。那么问题来了：如何把UDF集成到Hive的源码中？ 步骤如下： 1.下载hive1.1.0-cdh5.7.0的源码包2.在Linux 上解压到/root目录3.将编写好的 UDF 类文件ToUpper.java复制到Hive源码目录中,并修改 1234cp ToUpper.java /root/hive-1.1.0-cdh5.7.0/ql/src/java/org/apache/hadoop/hive/ql/udfvim /root/hive-1.1.0-cdh5.7.0/ql/src/java/org/apache/hadoop/hive/ql/udf/ToUpper.java##将 package com.xiejm.bigdata.hello; 修改为 package org.apache.hadoop.hive.ql.udf; 4.修改FunctionRegistry.java 文件 1vim /root/hive-1.1.0-cdh5.7.0/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java 添加import 1import org.apache.hadoop.hive.ql.udf.ToUpper; 12// registry for system functions private static final Registry system = new Registry(true); 找到上面的registry for system functions代码块中的static{}添加register 1system.registerUDF(\"ToUpper\", ToUpper.class, false); 5.编译Hive 123cd /root/hive-1.1.0-cdh5.7.0## 编译时间比较长mvn clean package -DskipTests -Phadoop-2 -Pdist 编译成功后会生成一个hive程序包和目录的地址 包地址：/root/hive-1.1.0-cdh5.7.0/packaging/target/apache-hive-1.1.0-cdh5.7.0-bin.tar.gz 目录地址：/root/hive-1.1.0-cdh5.7.0/packaging/target/apache-hive-1.1.0-cdh5.7.0-bin 6.使集成的UDF函数生效 方法一：替换hive-exec-1.1.0-cdh5.7.0.jar包 1234mv $HIVE_HOME/lib/hive-exec-1.1.0-cdh5.7.0.jar $HIVE_HOME/lib/hive-exec-1.1.0-cdh5.7.0.jar.bakcp /root/hive-1.1.0-cdh5.7.0/packaging/target/apache-hive-1.1.0-cdh5.7.0-bin/apache-hive-1.1.0-cdh5.7.0-bin/lib/hive-exec-1.1.0-cdh5.7.0.jar $HIVE_HOME/lib/ 方法二：复制编译生成的包重新部署。 7.测试UDF 测试先我们要结束hive的runjar进程，使用jps命令查看，如果有再用kill -9 结束进程 123456789101112## 显示自定义UDF函数toupper信息hive (default)&gt; desc function toupper;OKtoupper(str) - Converts a string to uppercaseTime taken: 0.014 seconds, Fetched: 1 row(s)## 调用toupper函数hive (default)&gt; select toupper('jimmy') from emp limit 2;OKJIMMYJIMMYTime taken: 0.087 seconds, Fetched: 2 row(s)","tags":[{"name":"Hive","slug":"Hive","permalink":"http://xiejm.com/tags/Hive/"}]},{"title":"Hive--概述","date":"2017-10-06T13:17:22.000Z","path":"Hive/Hive--Overview.html","text":"一.Hive是什么 Hive是使用一种类似于用SQL的查询语言，直接作用在分布式存储系统之上。 由Facebook开源，解决海量数据结构化的日志数据统计问题 构建在Hadoop之上的数据仓库：数据存放在HDFS,计算通过YARN和MR 引擎：Hive QL —&gt; MapReduce 重点关注HQL翻译成MR会产生几个作业 Hive底层：MapReduce、Spark（Hive on Spark）、Tez 压缩/存储格式： 在生产中如何选择合适的压缩方式，这才是关键。压缩会消耗一定的CPU资源。 生产环境慎选 Spark（Hive on Spark） 现在的至少有70%用Hive来做数据仓库的 二.产生背景 由于MapReduce的繁琐：Mapper—&gt;Reducer—&gt;Driver—&gt;package(打包) 大量数据存放在HDFS，如何快速的对HDFS上的文件进行统计和分析操作。HDFS仅仅只是一个纯文本文件而已，没有schema的概念，没有schema，那么就没办法使用sql进行查询 如何为HDFS上的文件添加Schema信息 3.Hive 发展历程Hive推出至今已经10年了，以下是Hive的重要里程碑需牢记 07/08 由facebook开源 13/05 hive-0.11 stinger Phase 1 ORC/HiveServer2 13/10 hive-0.12 stinger Phase 2 ORC/improvement 14/04 hive-0.13 stinger Phase 2 Tez/Vectorized query engine 14/11 hive-0.14 Stinger.next Phase 1 Cost-based optimaizer(CBO) 4.Hive架构Hive的优点： 简单易上手 容易扩展（基于HDFS和YARN） 统一的元数据metastore管理： image Hive主要部件 用户接口：CLI（command-line interface，命令行界面）、JDBC/ODBC、Broswer Driver：管理整个SQL作业的生命周期,接受query的组件，该组件实现session的概念，以处理和提供基于JDBC/ODBC执行。 SQL Parser：把SQL语句转换成抽象语法数，抽象语法数是不能执行的，先要转换成逻辑计划，逻辑执行计划优化以后生成物理执行计划，物理执行计划优化以后才能变成作业去运行 metastore包含：存储数据仓库所有的各种表与分区的结构化信息，包括列与列的信息，序列化器与反序列化，从而能够读写HDFS中的数据 metastore包含： database: name location owner table: name owner location column name/type/index createtime metastore和Spark、impala等SQL引擎是通用的 Hive 部署架构 image 上图是Hive的部署架构：Hive 默认元数据存放在derby里面；derby只能单session ；在测试环境也建议使用MySQL；在生产环境中MySQL必须有主备； Hive与RDBMS的关系其实Hive与关系型数据库没有直接关系，只是SQL有点像Hive的事务比较鸡肋 Hive与传统关系数据库的对比 使用Hive的好处： 90%任务由Hive编写，代码量通常1、2行，开发周期通常很短 Hive的所有执行，最终都将转化成MapReduce任务 Hive长处在于数据统计，容量、投放量的计算，group by，join 更多精力放在数据逻辑上，优化数据结构和性能","tags":[{"name":"Hive","slug":"Hive","permalink":"http://xiejm.com/tags/Hive/"}]},{"title":"MySQL 5.6基于Linux源码编译安装","date":"2017-10-04T13:43:52.000Z","path":"MySQL/MySQL--Source_Install.html","text":"MySQL可以通过YUM或其他安装包快速安装，也可以下载源代码编译安装。从源代码编译安装MySQL是有好处，比如可以指定编译生产参数、优化编译、指定安装位置等。 本文的主要内容： 获取源码 系统配置、编译和安装 配置MySQL 1. 获取源码可以在以下页面找到 http://dev.mysql.com/downloads/mysql/ MySQL的源文件被打包为.tar.gz或.zip格式。在本例中，我下载的源代码文件为’mysql-5.6.23.tar.gz’。 可以使用tar命令解压文件： 123[root@hadoop001 ~]# tar -zxvf mysql-5.6.23.tar.gz -C /usr/local/[root@hadoop001 ~]# cd /usr/local/[root@hadoop001 local]# mv mysql-5.6.23-linux-glibc2.5-x86_64 mysql 2.系统配置2.1 检查系统是否有MySQL123[root@Master local]# ps -ef|grep mysqldroot 2493 2423 0 19:48 pts/3 00:00:00 grep mysqld[root@Master local]# rpm -qa |grep -i mysql 如果有系统自带的MySQL卸载之 123[root@Master local]# rpm -qa |grep mysqlmysql-libs-5.1.73-5.el6_6.x86_64[root@Master local]# rpm -e --nodeps mysql-libs-5.1.73-5.el6_6.x86_64 2.2 创建用户和组12345678910111213Create group and user[root@Master local]# groupadd -g 101 dba[root@Master local]# useradd -u 520 -g dba -G root -md /usr/local/mysql mysqladmin[root@Master local]# id mysqladminuid=520(mysqladmin) gid=101(dba) groups=101(dba),0(root)#设置mysqladmin用户密码[root@Master local]# passwd mysqladminChanging password for user mysqladmin.New UNIX password:BAD PASSWORD: it is too simplistic/systematicRetype new UNIX password:passwd: all authentication tokens updated successfully. 复制用户环境变量的初始配置文件至mysqladmin用户的home目录中 为了解决切换用户显示-bash-4.1$的问题 1[root@Master local]# cp /etc/skel/.* /usr/local/mysql 2.3 创建MySQL配置文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102[root@Master local]# vim /etc/my.cnf#把以下配置文件内容复制到/etc/my.cnf中[client]port = 3306socket = /usr/local/mysql/data/mysql.sock[mysqld]port = 3306socket = /usr/local/mysql/data/mysql.sockexplicit_defaults_for_timestamp=trueskip-external-lockingkey_buffer_size = 256Msort_buffer_size = 2Mread_buffer_size = 2Mread_rnd_buffer_size = 4Mquery_cache_size= 32Mmax_allowed_packet = 16Mmyisam_sort_buffer_size=128Mtmp_table_size=32Mtable_open_cache = 512thread_cache_size = 8wait_timeout = 86400interactive_timeout = 86400max_connections = 600# Try number of CPU's*2 for thread_concurrency#thread_concurrency = 32#isolation level and default engine default-storage-engine = INNODBtransaction-isolation = READ-COMMITTEDserver-id = 1basedir = /usr/local/mysqldatadir = /usr/local/mysql/datapid-file = /usr/local/mysql/data/hostname.pid#open performance schemalog-warningssysdate-is-nowbinlog_format = MIXEDlog_bin_trust_function_creators=1log-error = /usr/local/mysql/data/hostname.errlog-bin=/usr/local/mysql/arch/mysql-bin#other logs#general_log =1#general_log_file = /usr/local/mysql/data/general_log.err#slow_query_log=1#slow_query_log_file=/usr/local/mysql/data/slow_log.err#for replication slave#log-slave-updates #sync_binlog = 1#for innodb options innodb_data_home_dir = /usr/local/mysql/data/innodb_data_file_path = ibdata1:500M:autoextendinnodb_log_group_home_dir = /usr/local/mysql/archinnodb_log_files_in_group = 2innodb_log_file_size = 200Minnodb_buffer_pool_size = 2048Minnodb_additional_mem_pool_size = 50Minnodb_log_buffer_size = 16Minnodb_lock_wait_timeout = 100#innodb_thread_concurrency = 0innodb_flush_log_at_trx_commit = 1innodb_locks_unsafe_for_binlog=1#innodb io features: add for mysql5.5.8performance_schemainnodb_read_io_threads=4innodb-write-io-threads=4innodb-io-capacity=200#purge threads change default(0) to 1 for purgeinnodb_purge_threads=1innodb_use_native_aio=on#case-sensitive file names and separate tablespaceinnodb_file_per_table = 1lower_case_table_names=1[mysqldump]quickmax_allowed_packet = 16M[mysql]no-auto-rehash[mysqlhotcopy]interactive-timeout[myisamchk]key_buffer_size = 256Msort_buffer_size = 256Mread_buffer = 2Mwrite_buffer = 2M 2.4 配置权限修改my.cnf文件权限 123456[root@Master local]# chown mysqladmin:dba /etc/my.cnf[root@Master local]# chmod 640 !$chmod 640 /etc/my.cnf[root@Master local]# ll !$ll /etc/my.cnf-rw-r----- 1 mysqladmin dba 2218 Sep 25 11:30 /etc/my.cnf 修改mysql目录权限 1234## 创建日志存放目录[root@Master ~]$ mkdir /usr/local/mysql/arch[root@Master local]# chown -R mysqladmin:dba /usr/local/mysql/ 2.5 安装安装glibc 1[root@Master local]# yum install -y glibc 12345678910111213[root@Master ~]$ su - mysqladmin[mysqladmin@Master ~]$ scripts/mysql_install_db --user=mysqladmin --basedir=/usr/local/mysql --datadir=/usr/local/mysql/data WARNING: The host 'Master' could not be looked up with /usr/local/mysql/bin/resolveip.This probably means that your libc libraries are not 100 % compatiblewith this binary MySQL version. The MySQL daemon, mysqld, should worknormally with the exception that host name resolving will not work.This means that you should use IP addresses instead of hostnameswhen specifying MySQL privileges !Installing MySQL system tables...OKFilling help tables...OK 退出到root用户操作 12[mysqladmin@Master ~]$ exitlogout 2.6 配置开机启动将服务文件拷贝到init.d下，并重命名为mysql 1[root@Master mysql]# cp support-files/mysql.server /etc/rc.d/init.d/mysql 修改启动文件的执行权限 1[root@Master mysql]# chmod +x /etc/rc.d/init.d/mysql 添加MySQL服务启动级别 1234[root@Master mysql]# chkconfig --del mysql[root@Master mysql]# chkconfig --add mysql[root@Master mysql]# chkconfig |grep mysqlmysql 0:off 1:off 2:on 3:on 4:on 5:on 6:off 2.7 启动MySQL首先要切换到mysqladmin用户下 1234[root@Master mysql]# su - mysqladmin[mysqladmin@Master ~]$ pwd/usr/local/mysql[mysqladmin@Master ~]$ mv my.cnf my.cnf.bak 执行启动脚本 123456mysqladmin@Master ~]$ bin/mysqld_safe &amp;[1] 8966[mysqladmin@Master ~]$ 170925 16:33:51 mysqld_safe Logging to '/usr/local/mysql/data/hostname.err'.170925 16:33:51 mysqld_safe Starting mysqld daemon with databases from /usr/local/mysql/data[mysqladmin@Master ~]$ 验证启动进程 先切换到root用户下 1234[root@Master ~]# ps -ef|grep mysqld514 4152 4122 0 13:51 pts/0 00:00:00 /bin/sh bin/mysqld_safe514 4795 4152 1 13:51 pts/0 00:00:03 /usr/local/mysql/bin/mysqld --basedir=/usr/local/mysql --datadir=/usr/local/mysql/data --plugin-dir=/usr/local/mysql/lib/plugin --log-error=/usr/local/mysql/data/hostname.err --pid-file=/usr/local/mysql/data/hostname.pid --socket=/usr/local/mysql/data/mysql.sock --port=3306root 4864 4819 0 13:55 pts/1 00:00:00 grep mysqld 验证3306端口是否被监听 12[root@Master ~]# netstat -tulnp | grep mysqltcp 0 0 :::3306 :::* LISTEN 4795/mysqld 查看mysql服务状态 12[root@Master ~]# service mysql status SUCCESS! MySQL running (4795) 至此MySQL就安装成功了。yes! give me five!~~~ haha 3.配置MySQL3.1 登录使用mysql命令登录 123456789101112131415161718192021222324252627282930313233343536[mysqladmin@Master ~]$ mysqlWelcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 1Server version: 5.6.23-log MySQL Community Server (GPL)Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.mysql&gt; use mysql;Database changedmysql&gt; update user set password=password('123456') where user='root';Query OK, 4 rows affected (0.00 sec)Rows matched: 4 Changed: 4 Warnings: 0mysql&gt; delete from user where user='';Query OK, 2 rows affected (0.00 sec)mysql&gt; flush privileges;mysql&gt; select user,host from mysql.user;+------+-----------+| user | host |+------+-----------+| root | 127.0.0.1 || root | ::1 || root | localhost || root | master |+------+-----------+4 rows in set (0.00 sec)","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://xiejm.com/tags/MySQL/"}]},{"title":"Hive的几种数据导入方式","date":"2017-10-03T14:49:38.000Z","path":"Hive/Hive--load_data.html","text":"在Hive中建好表之后，需要记载数据用于后续查询分析操作。本文介绍向Hive表中加载数据的几种方式。 建表时候直接指定如果数据已经存在HDFS中，那么可以直接在建表的时候使用location来指定数据所在HDFS路径。 123456CREATE [EXTERNAL] TABLE emp (day STRING,url STRING)ROW FORMAT DELIMITEDFIELDS TERMINATED BY '\\t'location '/tmp/emp/ 从本地文件系统导入数据到Hive表中1load data local inpath \"/path/to/dir\" into table tab_name [PARTITION] 从HDFS上导入数据到Hive表中1load data inpath \"/path/to/hdfs_dir\" into table tab_name [PARTITION] 从别的表中查询数据到Hive表中基本模式1insert overwrite table tab_name [PARTITION ] select ... from tab_name ; 多插入模式123from tab_nameinsert overwrite table tab_name1 select ....insert overwrite table tab_name2 select... Hive不支持用Insert语句一条一条地进行插入操作，也不支持UPDATE操作 在创建表的时候从别的表中查询记录并插入相应的表中在实际情况下，表的输出结果可能很多，不适于显示在控制台上，这时候，将Hive的查询输出结果直接保存在一个新的表中是合适的，我们称这种情况是CTAS 1create table tabe_name as select ...... 注意： CTAS是原子的，如果select查询失败，则新表不会创建 Hive导出数据到文件系统顺带也介绍下从Hive导出数据到HDFS和本地文件系统 先来看下语法： 123INSERT OVERWRITE [LOCAL] DIRECTORY directory1 [ROW FORMAT row_format] [STORED AS file_format] SELECT ... FROM ... 在上面的语法中，如果指定了LOCAL关键字，则为导出到本地文件系统，否则导出到HDFS中。使用ROW FORMAT关键字可以指定导出文件分隔符。 再来看下示例： 将emp表的所有数据导出到本地文件系统/tmp/emp/目录，字段间分割符为tab \\t INSERT OVERWRITE LOCAL DIRECTORY ‘/tmp/emp/‘ ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\\t’SELECT * FROM emp;","tags":[{"name":"Hive","slug":"Hive","permalink":"http://xiejm.com/tags/Hive/"}]},{"title":"Hive的数据类型和分隔符","date":"2017-10-03T13:37:16.000Z","path":"Hive/Hive--DataType.html","text":"本文记录了Hive的数据类型和分隔符的最佳实践 基本数据类型Hive的数据类型在官网用户手册中有详细介绍：https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types 这里列举一些常用的数据类型，以下按使用优先级排序 string &gt; int &gt; bigint &gt; float &gt; double &gt; boolean &gt; date &gt; timestamp 【最佳实践：】实际使用中建议都使用string类型，如果无法使用string类型再按上面的次序选择合适的数据类型 分隔符列与列之间的分隔符是换行 \\n Hive默认的字段分隔符是^A ,\\001 【最佳实践：】一般情况下，在创建表的时候直接指定分隔符：\\t或者英文逗号,","tags":[{"name":"Hive","slug":"Hive","permalink":"http://xiejm.com/tags/Hive/"}]},{"title":"Hadoop--集群(Fully Distributed Mode)模式部署+HA","date":"2017-10-03T13:06:38.000Z","path":"Hadoop/Hadoop--Fully_Distributed _Mode_HA.html","text":"本文主要记录Hadoop全分布模式（Fully Distributed Mode）部署的详细步骤： Hadoop全分布模式（Fully Distributed Mode）是一种集群部署方式，它的守护进程运行在一个集群上。 我们一般会在master节点上部署NameNode,ResourceManager； SecondaryNameNode可以安装在master节点，也可以单独安装。 slave节点能看到DataNode和NodeManager。 1.基本环境 CentOS 6.7 JDK1.7+ Hadoop 2.8.1 ZooKeeper 3.4.6 系统原始环境特别说明：系统选择minimal最小系统安装，并自定义选装以下包：Base System中的Base、Compatbillty libraries、Debugging Tools; Development中的 Development tools 1.1组件说明： 组件 备注说明 CentOS 使用下列命令查看系统版本和系统位数lsb_release -a #系统版本file /bin/ls #系统位数 JDK 本文采用JDK1.8.45 Hadoop 使用官网最新release版本2.8.1 ZooKeeper 用于HA热切,存储数据使用的协调服务下载地址： 2.主机节点规划 HostName IP Address Software Course hadoop001 192.168.194.111 Hadoop、ZooKeeper NameNodeDFSZKFailoverController ResourceManagerJobHistoryServerQuorumPeerMain hadoop002 192.168.194.112 Hadoop、ZooKeeper NameNodeDFSZKFailoverController ResourceManagerQuorumPeerMain hadoop003 192.168.194.113 Hadoop、ZooKeeper DataNode NodeManagerJournalNodeQuorumPeerMain hadoop004 192.168.194.114 Hadoop、ZooKeeper DataNode NodeManagerJournalNodeQuorumPeerMain hadoop005 192.168.194.115 Hadoop、ZooKeeper DataNode NodeManagerJournalNodeQuorumPeerMain 3.目录规划 名称 路径 备注 $HADOOP_HOME /opt/software/hadoop Data $HADOOP_HOME/data Log $HADOOP_HOME/logs hadoop.tmp.dir $HADOOP_HOME/tmp 需要手工创建,权限777,root:root $ZOOKEEPER_HOME /opt/software/zookeeper 4.系统环境配置本节内容所有节点都要配置，以下操作用hadoop001节点做示例 4.1 配置网络和主机名网络IP设置12345678910111213[root@hadoop001 ~]# vi /etc/sysconfig/network-scripts/ifcfg-eth0#按以下格式修改，没有的字段需要自行添加DEVICE=eth0 #网卡名称HWADDR=00:0C:29:8F:61:EB #MAC地址TYPE=Ethernet #网卡类型UUID=039afc77-da3e-4e17-bcaa-062148464812 #网卡IDONBOOT=yes #是否开机启动NM_CONTROLLED=yes #Network ManagerBOOTPROTO=static #静态IP模式IPADDR=192.168.194.111 #IP地址NETMASK=255.255.255.0 #子网掩码GATEWAY=192.168.194.2 #网关地址DNS1=223.5.5.5 #DNS 保存退出后，重启网卡 1[root@hadoop001 ~]# service network restart 主机名配置如果在安装系统的时候未进行主机名设置，那么请根据以下命令对主机名进行配置 1234567[root@hadoop001 ~]# vi /etc/sysconfig/network#修改下列信息，其中HOSTNAME项修改成我们需要的主机名NETWORKING=yes #启动网络HOSTNAME=hadoop001 #主机名#修改完成后保存并退出，使用下列命令更新当前bash的主机名[root@hadoop001 ~]# hostname hadoop001 4.2 添加集群用户增加ID号为1000的hadoopGroup,增加ID为2000的用户并加入hadoopGroup组 固定用户和组的ID号是为了用户和组的权限一致，我遇到过一次在两台机器上用户名和组名相同，ID号不同结果导致权限有问题 12[root@hadoop001 ~]# groupadd -g 1000 hadoopGroup[root@hadoop001 ~]# useradd -u 2000 -g hadoopGroup hadoop 设置hadoop用户的密码为hadoop，下面的这种方法免去了修改密码时的密码确认，在生产环境中使用此方式改密码需要清空history 1[root@hadoop001 ~]# echo \"hadoop\"|passwd --stdin hadoop 4.3 关闭SELINUX12345[root@hadoop001 ~]# sed -i 's#SELINUX=enforcing#SELINUX=disabled#g' /etc/selinux/config#关闭当前的SELINUX状态,如果不用setenforce那么需要重启系统[root@hadoop001 ~]# setenforce 0[root@hadoop001 ~]# grep SELINUX=disabled /etc/selinux/configSELINUX=disabled 4.4 关闭防火墙12345[root@hadoop001 ~]# service iptables stopiptables: Setting chains to policy ACCEPT: filter [ OK ]iptables: Flushing firewall rules: [ OK ]iptables: Unloading modules: [ OK ][root@hadoop001 ~]# chkconfig iptables off 4.5 优化开机自启动服务(可选)1234567[root@hadoop001 ~]# chkconfig --list|grep 3:on |wc -l[root@hadoop001 ~]# chkconfig --list|grep 3:on |cut -d \"\" -f1#关闭所有3级别的服务[root@hadoop001 ~]# for name in `chkconfig --list|grep 3:on | cut -d \" \" -f1` ;do chkconfig $name off;done#开启需要在3级别启动的服务，这里可以根据需要增加和修改[root@hadoop001 ~]# for name in crond ntpd network rsyslog sshd;do chkconfig $name on;done[root@hadoop001 ~]# chkconfig --list|grep 3:on 4.6 修改文件描述符12345678910111213#修改系统limits.conf[root@hadoop001 ~]# cat &gt;&gt; /etc/security/limits.conf &lt;&lt; EOF* soft nofile 262144* hard nofile 262144* soft nproc 262144* hard nproc 262144EOF#查看系统ulimit文件句柄数[root@hadoop001 ~]# ulimit -n#修改90-nproc.conf[root@hadoop001 ~]# vi /etc/security/limits.d/90-nproc.conf* soft nproc 262144 4.7 增加集群用户sudo权限增加hadoop用户的sudo权限 12[root@hadoop001 ~]# vi /etc/sudoershadoop ALL=(ALL) ALL 4.8 配置主机IP映射12345678[root@hadoop001 ~]# vi /etc/hosts#在hosts文件末尾追加192.168.194.111 hadoop001192.168.194.112 hadoop002192.168.194.113 hadoop003192.168.194.114 hadoop004192.168.194.115 hadoop005 4.9 配置SSH免密登录SSH免密码登录配置的详细配置步骤：SSH免密登录 4.10 配置NTP时钟同步 第一种方法 ：（所有节点都要操作）都从公共NTP服务器同步，执行如下： 1234$ cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime$ ntpdate us.pool.ntp.org$ crontab -e0-59/10 * * * * /usr/sbin/ntpdate us.pool.ntp.org | logger -t NTP 以上crontab的配置说明详见 Linux-crontab命令 第二种方法：选一个节点搭建一个NTP服务，其他节点从该NTP服务器同步，具体步骤参考本博客的NTP 配置 4.11 关闭交换分区123[root@hadoop001 ~]# echo '0' &gt; /proc/sys/vm/swappiness[root@hadoop001 ~]# sysctl vm.swappiness=0[root@hadoop001 ~]# echo 'vm.swappiness=0'&gt;&gt; /etc/sysctl.conf 4.12 禁用透明大页1234[root@hadoop001 ~]# echo never &gt; /sys/kernel/mm/redhat_transparent_hugepage/defrag[root@hadoop001 ~]# cat /sys/kernel/mm/redhat_transparent_hugepage/defragalways [never][root@hadoop001 ~]# echo 'echo never &gt; /sys/kernel/mm/redhat_transparent_hugepage/defrag ' &gt;&gt; /etc/rc.local 5.JDK安装【注意】所有节点配置详细安装步骤参考本博客的JDK安装的文章:JDK安装步骤 6.配置环境变量 【注意】所有节点配置 1234567891011121314151617181920212223242526[root@hadoop001 software]# vim /etc/profile#按组合键shift+g 或大写G 到最后一行，按字母o 插入下列内容 #SET HADOOP exprot HADOOP_HOME=/opt/software/hadoop #SET ZOOKEEPER export ZOOKEEPER_HOME=/opt/software/zookeeper#如果JDK环境变量没有配置，请加入下列内容 #SET JDK export JAVA_HOME=/usr/java/jdk1.7.0_79 export JRE_HOME=$JAVA_HOME/jre export CLASS_PATH=.:$JAVA_HOME/lib:$JAVA_HOME/lib/tools.jar#设置PATH export PATH=$ZOOKEEPER_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$JAVA_HOME/bin:$JRE_HOME/bin:$PATH#添加完成后 :wq 保存#使修改生效：[root@hadoop001 software]#source !$#验证JDK有效性[root@hadoop001 software]# java -versionjava version \"1.7.0_79\"Java(TM) SE Runtime Environment (build 1.7.0_79-b15)Java HotSpot(TM) 64-Bit Server VM (build 24.79-b02, mixed mode) 7.安装配置ZooKeeper7.1 下载ZooKeeper123456[root@hadoop001 software]# pwd/opt/software[root@hadoop001 software]# wget http://mirrors.hust.edu.cn/apache/zookeeper/zookeeper-3.4.6/zookeeper-3.4.6.tar.gz[root@hadoop001 software]# tar -zxvf zookeeper-3.4.6.tar.gz[root@hadoop001 software]# mv zookeeper-3.4.6 zookeeper[root@hadoop001 software]# chown -R hadoop:hadoopGroup zookeeper 7.2 修改配置12345678910111213141516171819[root@hadoop001 software]# cd /opt/software/zookeeper/conf[root@hadoop001 conf]# cp zoo_sample.cfg zoo.cfg[root@hadoop001 conf]# vi zoo.cfg修改dataDirdataDir=/opt/software/zookeeper/data添加下面三行server.1=hadoop001:2888:3888server.2=hadoop002:2888:3888server.3=hadoop003:2888:3888server.2=hadoop004:2888:3888server.3=hadoop005:2888:3888[root@hadoop001 conf]# cd ../[root@hadoop001 zookeeper]# mkdir data[root@hadoop001 zookeeper]# touch data/myid[root@hadoop001 zookeeper]# echo 1 &gt; data/myid[root@hadoop001 zookeeper]# cat data/myid1 7.3 文件分发1234[root@hadoop001 zookeeper]# scp -r /opt/software/zookeeper root@hadoop002:/opt/software[root@hadoop001 zookeeper]# scp -r /opt/software/zookeeper root@hadoop003:/opt/software[root@hadoop001 zookeeper]# scp -r /opt/software/zookeeper root@hadoop004:/opt/software[root@hadoop001 zookeeper]# scp -r /opt/software/zookeeper root@hadoop005:/opt/software 7.4 其他节点myid修改12345678#不同zk节点配置不同的myid#myid中的数值需要和 zoo.cfg中的配置一致。# hadoop002/003/004/005,也修改配置,就如下不同[root@hadoop002 zookeeper]# echo 2 &gt; data/myid[root@hadoop003 zookeeper]# echo 3 &gt; data/myid[root@hadoop004 zookeeper]# echo 4 &gt; data/myid[root@hadoop005 zookeeper]# echo 5 &gt; data/myid####注意！！！不能把 echo 3&gt;data/myid,将&gt;前后空格保留,否则无法将 3 写入myid文件 8.安装Hadoop8.1 解压之前编译好了hadoop-2.8.1-snappy.tar.gz，上传这个包并解压。具体编译过程，参考本博客 12345678910111213[root@hadoop001 ~]# mkdir -p /opt/software#将hadoop安装包放到/opt/software[root@hadoop001 ~]# cd /opt/software[root@hadoop001 software]# tar -zxvf hadoop-2.8.1.tar.gz#设置软连接[root@hadoop001 software]# ln -s /opt/software/hadoop-2.8.1 /opt/software/hadoop# 当我们使用hadoop用户才需要下面命令#设置hadoop目录所有者、所属组#[root@hadoop001 software]# chown -R hadoop:hadoopGroup hadoop/*#[root@hadoop001 software]# chown -R hadoop:hadoopGroup hadoop-2.8.1#[root@hadoop001 software]# chown -R hadoop:hadoopGroup hadoop-2.8.1/* 8.2 配置修改使用命令cd $HADOOP_HOME/etc/hadoop进入配置文件目录 8.2.1 修改$HADOOP_HOME/etc/hadoop/hadoop-env.sh 12export JAVA_HOME=\"/usr/java/jdk1.7.0_67\"export HADOOP_OPTS=\"$HADOOP_OPTS -Djava.library.path=$HADOOP_HOME/lib:$HADOOP_HOME/lib/native\" 8.2.2 修改$HADOOP_HOME/etc/hadoop/core-site.xml 8.2.3 修改$HADOOP_HOME/etc/hadoop/hdfs-site.xml 8.2.4 修改$HADOOP_HOEM/etc/hadoop/mapred-site.xml 首先复制mapred-site.xml的模板文件，然后修改mapred-site.xml 12[root@sht-sgmhadoopnn-01 hadoop]# cp mapred-site.xml.template mapred-site.xml[root@sht-sgmhadoopnn-01 hadoop]# vi mapred-site.xml 8.2.5 修改$HADOOP_HOME/etc/hadoop/yarn-site.xml 8.2.6 修改slaves 点击下载配置文件，解压后复制到$HADOOP_HOME/etc/hadoop目录下 8.3 创建临时文件夹和分发文件夹12345678[root@hadoop001 hadoop]# mkdir -p /opt/software/hadoop/tmp[root@hadoop001 hadoop]# chmod -R 777 /opt/software/hadoop/tmp[root@hadoop001 hadoop]# chown -R root:root /opt/software/hadoop/tmp[root@hadoop001 hadoop]# scp -r /opt/software/hadoop root@hadoop002:/hadoop[root@hadoop001 hadoop]# scp -r /opt/software/hadoop root@hadoop003:/hadoop[root@hadoop001 hadoop]# scp -r /opt/software/hadoop root@hadoop004:/hadoop[root@hadoop001 hadoop]# scp -r /opt/software/hadoop root@hadoop005:/hadoop 9.启动步骤和详细过程：9.1 启动ZK在所有的ZK节点执行命令： zkServer.sh start 可借助命令 zkServer.sh status 查看各个ZK的从属关系 9.2 启动 hadoop(HDFS+YARN)9.2.1. 格式化前,先在 journalnode 节点机器上先启动 JournalNode 进程12345678910111213141516171819202122232425[root@hadoop003 ~]# cd /opt/software/hadoop/sbin[root@hadoop003 sbin]# hadoop-daemon.sh start journalnodestarting journalnode, logging to /opt/software/hadoop/logs/hadoop-root-journalnode-hadoop001.out[root@hadoop003 sbin]# jps4016 Jps3683 QuorumPeerMain3981 JournalNode[root@hadoop004 hadoop]# cd /opt/software/hadoop/sbin[root@hadoop004 sbin]# hadoop-daemon.sh start journalnodestarting journalnode, logging to /opt/software/hadoop/logs/hadoop-root-journalnode-hadoop002.out[root@hadoop004 sbin]# jps9891 Jps9609 QuorumPeerMain9852 JournalNode[root@hadoop005 hadoop]# cd /opt/software/hadoop/sbin[root@hadoop005 sbin]# hadoop-daemon.sh start journalnodestarting journalnode, logging to /opt/software/hadoop/logs/hadoop-root-journalnode-hadoop003.out[root@hadoop005 sbin]# jps4425 JournalNode4460 Jps4191 QuorumPeerMain 9.2.2.NameNode 格式化12[root@hadoop001 sbin]# cd ../[root@hadoop001 hadoop]# hadoop namenode -format 9.2.3.同步 NameNode 元数据同步 hadoop001 元数据到 hadoop002主要是： dfs.namenode.name.dir， dfs.namenode.edits.dir 还应该确保共享存储目录下(dfs.namenode.shared.edits.dir ) 包含 NameNode 所有的元数据。 123456789101112[root@hadoop001 hadoop]# pwd/opt/software/hadoop[root@hadoop001 hadoop]# scp -r data/ root@hadoop002:/opt/software/hadoopin_use.lock 100% 14 0.0KB/s 00:00VERSION 100% 167 0.2KB/s 00:00seen_txid 100% 2 0.0KB/s 00:00VERSION 100% 220 0.2KB/s 00:00fsimage_0000000000000000000.md5 100% 62 0.1KB/s00:00fsimage_0000000000000000000 100% 306 0.3KB/s00:00[root@hadoop001 hadoop]# 9.2.4.初始化 ZFCK1[root@hadoop001 bin]# hdfs zkfc -formatZK 9.2.5.启动 HDFS 分布式存储系统集群启动,在 hadoop001 执行 start-dfs.sh集群关闭,在 hadoop001 执行 stop-dfs.sh 集群启动 123[root@hadoop001 sbin]# start-dfs.sh[root@hadoop001 hadoop]# start-dfs.sh#补充集群启动信息 如果需要使用单独启动某个进程，请使用以下命令 12345678NameNode(hadoop001, hadoop002):hadoop-daemon.sh start namenodeDataNode(hadoop003, hadoop004, hadoop005):hadoop-daemon.sh start datanodeJournamNode(hadoop003, hadoop003, hadoop005):hadoop-daemon.sh start journalnodeZKFC(hadoop001, hadoop002):hadoop-daemon.sh start zkfc 9.2.6.验证 namenode,datanode,zkfc HDFS进程检查 hadoop001和hadoop002节点的进程相同，hadoop003、hadoop004、hadoop005节点的进程相同,下面用hadoop001、hadoop003的jps 命令输出为示例 12345678910[root@hadoop001 sbin]# jps12712 Jps12593 DFSZKFailoverController12278 NameNode[root@hadoop003 ~]# jps6348 JournalNode8775 Jps559 QuorumPeerMain8509 DataNode webUI页面检查NN、DN节点情况 hadoop001:http://hadoop001:50070/ hadoop002:http://hadoop002:50070/ 9.2.7.启动 YARN 框架 集群启动在hadoop001 启动 Yarn，命令所在目录： $HADOOP_HOME/sbin 1234567[root@hadoop001 hadoop]# start-yarn.shstarting yarn daemonsstarting resourcemanager, logging to /opt/software/hadoop/logs/yarn-root-resourcemanagerhadoop001.outhadoop002: starting nodemanager, logging to /opt/software/hadoop/logs/yarn-rootnodemanager-hadoop002.outhadoop003: starting nodemanager, logging to /opt/software/hadoop/logs/yarn-rootnodemanager-hadoop003.outhadoop001: starting nodemanager, logging to /opt/software/hadoop/logs/yarn-rootnodemanager-hadoop001.out[root@hadoop001 hadoop]# 在hadoop002 备机启动 RM 123[root@hadoop002 ~]# yarn-daemon.sh start resourcemanagerstarting resourcemanager, logging to /opt/software/hadoop/logs/yarn-root-resourcemanagerhadoop002.out[root@hadoop002 ~]# 如果需要单独启动，请使用下面命令 12345678910111) ResourceManager(hadoop001, hadoop002)yarn-daemon.sh start resourcemanager2) NodeManager(hadoop001, hadoop002, hadoop003)yarn-daemon.sh start nodemanager- **如果需要关闭YARN集群可以使用下面命令**[root@hadoop001 sbin]# stop-yarn.sh#包含 namenode 的 resourcemanager 进程， datanode 的 nodemanager 进程[root@hadoop002 sbin]# yarn-daemon.sh stop resourcemanager 9.2.7.验证 resourcemanager,nodemanager YARN进程验证 hadoop001和hadoop002节点的进程相同，hadoop003、hadoop004、hadoop005节点的进程相同,下面用hadoop001、hadoop003的jps 命令输出为示例 12345678910111213141516[root@hadoop001 ~]# jps5043 NodeManager3683 QuorumPeerMain4679 DFSZKFailoverController5355 Jps4348 DataNode3981 JournalNode4943 ResourceManager[root@hadoop003 ~]# jps4518 DataNode4679 NodeManager4425 JournalNode4794 Jps4191 QuorumPeerMain 页面验证 ResourceManger（ Active） ： http://192.168.194.111:8088 ResourceManger（ Standby）： http://192.168.194.112:8088/cluster/cluster","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://xiejm.com/tags/Hadoop/"}]},{"title":"Hadoop--HDFS的shell(命令行客户端)操作","date":"2017-10-03T13:01:46.000Z","path":"Hadoop/Hadoop--HDFS_Shell.html","text":"Hadoop有两个常用命令 hadoop 和 hdfs hadoop fs &lt;====&gt; hdfs dfs 这两个命令时相等的 1.hadoop 命令使用 image 命令行客户端支持的命令参数： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253[root@hadoop001 bin]# hadoop fsUsage: hadoop fs [generic options] [-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;] [-cat [-ignoreCrc] &lt;src&gt; ...] [-checksum &lt;src&gt; ...] [-chgrp [-R] GROUP PATH...] [-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...] [-chown [-R] [OWNER][:[GROUP]] PATH...] [-copyFromLocal [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;] [-copyToLocal [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;] [-count [-q] [-h] [-v] &lt;path&gt; ...] [-cp [-f] [-p | -p[topax]] &lt;src&gt; ... &lt;dst&gt;] [-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]] [-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;] [-df [-h] [&lt;path&gt; ...]] [-du [-s] [-h] &lt;path&gt; ...] [-expunge] [-find &lt;path&gt; ... &lt;expression&gt; ...] [-get [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;] [-getfacl [-R] &lt;path&gt;] [-getfattr [-R] &#123;-n name | -d&#125; [-e en] &lt;path&gt;] [-getmerge [-nl] &lt;src&gt; &lt;localdst&gt;] [-help [cmd ...]] [-ls [-d] [-h] [-R] [&lt;path&gt; ...]] [-mkdir [-p] &lt;path&gt; ...] [-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;] [-moveToLocal &lt;src&gt; &lt;localdst&gt;] [-mv &lt;src&gt; ... &lt;dst&gt;] [-put [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;] [-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;] [-rm [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...] [-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; ...] [-setfacl [-R] [&#123;-b|-k&#125; &#123;-m|-x &lt;acl_spec&gt;&#125; &lt;path&gt;]|[--set &lt;acl_spec&gt; &lt;path&gt;]] [-setfattr &#123;-n name [-v value] | -x name&#125; &lt;path&gt;] [-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...] [-stat [format] &lt;path&gt; ...] [-tail [-f] &lt;file&gt;] [-test -[defsz] &lt;path&gt;] [-text [-ignoreCrc] &lt;src&gt; ...] [-touchz &lt;path&gt; ...] [-usage [cmd ...]]Generic options supported are-conf &lt;configuration file&gt; specify an application configuration file-D &lt;property=value&gt; use value for given property-fs &lt;local|namenode:port&gt; specify a namenode-jt &lt;local|resourcemanager:port&gt; specify a ResourceManager-files &lt;comma separated list of files&gt; specify comma separated files to be copied to the map reduce cluster-libjars &lt;comma separated list of jars&gt; specify comma separated jar files to include in the classpath.-archives &lt;comma separated list of archives&gt; specify comma separated archives to be unarchived on the compute machines.The general command line syntax isbin/hadoop command [genericOptions] [commandOptions] 2.常用命令参数介绍12-help功能：输出这个命令参数手册 12345-ls功能：显示目录信息示例： hadoop fs -ls hdfs://hadoop001:9000/备注：这些参数中，所有的hdfs路径都可以简写--&gt;hadoop fs -ls / 等同于上一条命令的效果 123-mkdir功能：在hdfs上创建目录示例：hadoop fs -mkdir -p /aaa/bbb/cc/dd 123-moveFromLocal功能：从本地剪切粘贴到hdfs示例：hadoop fs -moveFromLocal /home/hadoop/a.txt /aaa/bbb/cc/dd 12345--appendToFile功能：追加一个文件到已经存在的文件末尾示例：hadoop fs -appendToFile ./hello.txt hdfs://hadoop001:9000/hello.txt可以简写为：Hadoop fs -appendToFile ./hello.txt /hello.txt 123-cat功能：显示文件内容示例：hadoop fs -cat /hello.txt 123-tail功能：显示一个文件的末尾示例：hadoop fs -tail /hello.txt 123-text功能：以字符形式打印一个文件的内容示例：hadoop fs -text /hello.txt 1234567-chgrp-chmod-chown功能：linux文件系统中的用法一样，对文件所属权限示例：hadoop fs -chmod 666 /hello.txthadoop fs -chown someuser:somegrp /hello.txt 123-copyFromLocal功能：从本地文件系统中拷贝文件到hdfs路径去示例：hadoop fs -copyFromLocal ./jdk.tar.gz /aaa/ 123-copyToLocal功能：从hdfs拷贝到本地示例：hadoop fs -copyToLocal /aaa/jdk.tar.gz 123-cp功能：从hdfs的一个路径拷贝hdfs的另一个路径示例： hadoop fs -cp /aaa/jdk.tar.gz /bbb/jdk.tar.gz.2 123-mv功能：在hdfs目录中移动文件示例： hadoop fs -mv /aaa/jdk.tar.gz / 123-get功能：等同于copyToLocal，就是从hdfs下载文件到本地示例：hadoop fs -get /aaa/jdk.tar.gz 1234-getmerge功能：合并下载多个文件示例：比如hdfs的目录 /aaa/下有多个文件:log.1, log.2,log.3,...hadoop fs -getmerge /aaa/log.* ./log.sum 123-put功能：等同于copyFromLocal示例：hadoop fs -put /aaa/jdk.tar.gz /bbb/jdk.tar.gz.2 123-rm功能：删除文件或文件夹示例：hadoop fs -rm -r /aaa/bbb/ 123-rmdir功能：删除空目录示例：hadoop fs -rmdir /aaa/bbb/ccc 1234567-df功能：统计文件系统的可用空间信息示例：hadoop fs -df -h /[root@hadoop001 ~]# hadoop fs -df -h /Filesystem Size Used Available Use%hdfs://hadoop001:9000 15.1 G 17.1 M 7.6 G 0% 1234567-du功能：统计文件夹的大小信息示例：hadoop fs -du -s -h /aaa/*[root@hadoop001 ~]# hadoop fs -du -s -h /aaa/*419.8 K 419.8 K /aaa/bbbb 123-count功能：统计一个指定目录下的文件节点数量示例：hadoop fs -count /aaa/ 1234-setrep功能：设置hdfs中文件的副本数量示例：hadoop fs -setrep 3 /aaa/jdk.tar.gz&lt;这里设置的副本数只是记录在namenode的元数据中，是否真的会有这么多副本，还得看datanode的数量&gt;","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://xiejm.com/tags/Hadoop/"}]},{"title":"Hadoop 源码编译安装","date":"2017-10-03T12:58:16.000Z","path":"Hadoop/Hadoop_source_code_building.html","text":"本文主要记录Hadoop2.x源码编译的详细步骤： 1.编译基础环境 CentOS 6.7 minimal Hadoop 2.7.4 JDK 1.7+ Maven 3.0 or later Findbugs 1.3.9 (if running findbugs) ProtocolBuffer 2.5.0 CMake 2.6 or newer (if compiling native code) Zlib devel (if compiling native code) openssl devel ( if compiling native hadoop-pipes and to get the best HDFS encryption performance ) Internet connection for first build 系统原始环境特别说明：系统选择minimal最小系统安装，并自定义选装以下包： Base System中的Base、Compatbillty libraries、Debugging Tools; Development中的 Development tools 2.yum 源配置参考本博客的yum源配置的文章:yum源配置为阿里云镜像站 3.下载Hadoop源码包1234567[root@hadoop001 sourcecode]# pwd/opt/sourcecode[root@hadoop001 sourcecode]# mkdir -p /opt/sourcecode[root@hadoop001 sourcecode]#wget http://mirror.bit.edu.cn/apache/hadoop/common/hadoop-2.7.4/hadoop-2.7.4-src.tar.gz[root@hadoop001 sourcecode]#tar -xzvf hadoop-2.7.4-src.tar.gz[root@hadoop001 sourcecode]#cat ./hadoop-2.7.4-src/BUILDING.txt#从BUILDING文件中我们可以看到编译的要求 4.JDK安装参考本博客的JDK安装的文章:JDK安装 5.Maven安装1234567891011mkdir -p /opt/softwarecd /opt/software上传apache-maven-3.3.9-bin.zipunzip apache-maven-3.3.9-bin.zip#添加maven环境变量echo '#SET MAVEN'&gt;&gt;/etc/profileecho 'export MAVEN_HOME=/opt/software/apache-maven-3.3.9' &gt;&gt;/etc/profileecho 'export MAVEN_OPTS=\"-Xms256m -Xmx512m\"'&gt;&gt;/etc/profileecho 'export PATH=$MAVEN_HOME/bin:$PATH'&gt;&gt;/etc/profilesource /etc/profilemvn --version 6.protobuf安装protobuf要编译安装，需安装gcc、gcc-c++、 make上传 protobuf-2.5.0.tar.gz 1234567891011tar -xzvf protobuf-2.5.0.tar.gzcd protobuf-2.5.0yum install -y gcc gcc-c++ make./configure --prefix=/usr/local/protobufmake &amp;&amp; make install#添加protobuf环境变量echo '#SET PROTOBUF'&gt;&gt;/etc/profileecho 'export PROTOC_HOME=/usr/local/protobuf'&gt;&gt;/etc/profileecho 'export PATH=$PROTOC_HOME/bin:$PATH'&gt;&gt;/etc/profilesource /etc/profileprotoc --version 7.Findbugs安装上传 findbugs-1.3.9.zip 12345unzip findbugs-1.3.9.zip#添加Findbugs环境变量echo '#SET Findbugs'&gt;&gt;/etc/profileecho 'export FINDBUGS_HOME=/opt/software/findbugs-1.3.9'&gt;&gt;/etc/profileecho 'export PATH=$FINDBUGS_HOME/bin:$PATH'&gt;&gt;/etc/profile 8.Snappy压缩库安装安装snappy1.1.4,使Hadoop支持snappy压缩 123456wget https://github.com/google/snappy/releases/download/1.1.4/snappy-1.1.4.tar.gztar -zxvf snappy-1.1.4.tar.gzcd snappy-1.1.4./configuremake &amp;&amp; make installll -h /usr/local/lib |grep snappy 9.其他依赖安装12yum install -y ant openssl openssl-devel svn ncurses-devel zlib-devel libtool svnyum install -y snappy snappy-devel bzip2 bzip2-devel lzo lzo-devel lzop autoconf automake 10.编译进入Hadooop源码目录 12cd hadoop-2.7.4-srcmvn clean package -DskipTests -Pdist,native -Dtar -Dsnappy.lib=/usr/local/lib -Dbundle.snappy 11.生成tar包1/opt/sourcecode/hadoop-2.7.4-src/hadoop-dist/target/hadoop-2.7.4.tar.gz 注意事项： 由于Maven仓库在墙外，Maven在编译项目时下载包卡住情况，ctrl+c 中断，重新执行编译。 如果出现提示缺少了某个文件的情况，则要先清理maven(使用命令 mvn clean) 再重新编译。","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://xiejm.com/tags/Hadoop/"},{"name":"Maven","slug":"Maven","permalink":"http://xiejm.com/tags/Maven/"}]},{"title":"Linux Extended VM Disk In VMware","date":"2017-10-03T11:59:04.000Z","path":"Linux/Extended_VM Disk_In_ VMware.html","text":"前言在做大数据相关开发的测试时候，会用到VM虚拟机。用了时间久了以后，虚拟机磁盘爆满，这时就需要给虚拟机磁盘扩容了。 由于在当初创建虚拟机时选择了LVM分区，给现在扩容带来了便利。 查看本地磁盘使用情况磁盘使用情况如下： 123456[root@hadoop001 ~]# df -ThFilesystem Type Size Used Avail Use% Mounted on/dev/mapper/vg_master-lvm_root ext4 18G 2.3G 15G 14% /tmpfs tmpfs 932M 0 932M 0% /dev/shm/dev/sda1 ext4 190M 41M 140M 23% /boot 查看分区情况原磁盘为20G，先扩充到40G 123456789101112131415161718[root@hadoop001 ~]# fdisk /dev/sdaCommand (m for help): pDisk /dev/sda: 42.9 GB, 42949672960 bytes255 heads, 63 sectors/track, 5221 cylindersUnits = cylinders of 16065 * 512 = 8225280 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk identifier: 0x00012968 Device Boot Start End Blocks Id System/dev/sda1 * 1 26 204800 83 LinuxPartition 1 does not end on cylinder boundary./dev/sda2 26 281 2048000 82 Linux swap / SolarisPartition 2 does not end on cylinder boundary./dev/sda3 281 2611 18717696 8e Linux LVM 对新盘再分区123456[root@hadoop001 ~]# fdisk /dev/sdaWARNING: DOS-compatible mode is deprecated. It's strongly recommended to switch off the mode (command 'c') and change display units to sectors (command 'u').Command (m for help): 请安步骤输入下面操作 123456789101112131415161718192021p 查看已分区数量（我看到有两个 /dev/sda1 /dev/sda2）n 新增加一个分区p 分区类型我们选择为主分区4 分区号选3（因为1,2已经用过了，见上）回车 默认（起始扇区）回车 默认（结束扇区）t 修改分区类型4 选分区38e 修改为LVM（8e就是LVM）w 写分区表(可能直接退出了，没有关系的)q 完成，退出fdisk命令 重启并格式化1[root@hadoop001 ~]# reboot 格式 /dev/sda4 123[root@hadoop001 ~]# ll /dev/sda4brw-rw----. 1 root disk 8, 4 Sep 23 08:19 /dev/sda4[root@hadoop001 ~]# mkfs.ext4 /dev/sda4 添加新的LVM实现扩容12345678910111213141516#进入lvm管理[root@hadoop001 ~]# lvm#创建PV 这是初始化刚才的分区，必须的lvm&gt; pvcreate /dev/sda4#将初始化过的分区加入到虚拟卷组vg_master(卷组名根据系统内的实际卷组名为准)lvm&gt; vgextend vg_master /dev/sda3#扩展已有卷的容量lvm&gt;lvextend -l +100% /dev/mapper/vg_master-lvm_root#查看卷容量，这时你会看到一个很大的卷了lvm&gt;pvdisplaylvm&gt;quit 备注：lvextend扩展命令这一步，还可以选择扩容大小 12#扩展已有卷的容量（只扩充10G）lvm&gt;lvextend -L +10G /dev/mapper/vg_master-lvm_root 刷新磁盘容量1234567[root@hadoop001 ~]# resize2fs /dev/mapper/vg_master-lvm_rootresize2fs 1.41.12 (17-May-2010)Filesystem at /dev/mapper/vg_master-lvm_root is mounted on /; on-line resizing requiredold desc_blocks = 2, new_desc_blocks = 3Performing an on-line resize of /dev/mapper/vg_master-lvm_root to 9919488 (4k) blocks.The filesystem on /dev/mapper/vg_master-lvm_root is now 9919488 blocks long. 再次查看磁盘分区情况123456[root@hadoop001 ~]# df -ThFilesystem Type Size Used Avail Use% Mounted on/dev/mapper/vg_master-lvm_root ext4 38G 2.4G 33G 7% /tmpfs tmpfs 932M 0 932M 0% /dev/shm/dev/sda1 ext4 190M 41M 140M 23% /boot 扩容后/dev/mapper/vg_master-lvm_root的大小是38G了 至此扩容完成！","tags":[{"name":"Linux","slug":"Linux","permalink":"http://xiejm.com/tags/Linux/"}]},{"title":"Hadoop伪分布式部署","date":"2017-10-03T06:16:06.000Z","path":"Hadoop/Hadoop_Pseudo-Distributed_Mode.html","text":"本文主要记录Hadoop伪分布式（Pseudo-Distributed Mode）部署的详细步骤：伪分布模式在“单节点集群”上运行Hadoop，其中所有的守护进程都运行在同一台机器上。该模式在单机模式之上增加了代码调试功能，允许你检查内存使用情况，HDFS输入输出，以及其他的守护进程交互。 1.基本环境 CentOS 6.7 JDK1.7+ Hadoop 2.7.4 系统原始环境特别说明：系统选择minimal最小系统安装，并自定义选装以下包：Base System中的Base、Compatbillty libraries、Debugging Tools; Development中的 Development tools 12345678910111213#关闭防火墙[root@hadoop001 ~]# service iptables stopiptables: Setting chains to policy ACCEPT: filter [ OK ]iptables: Flushing firewall rules: [ OK ]iptables: Unloading modules: [ OK ][root@hadoop001 ~]# chkconfig iptables off#关闭SELINUX[root@hadoop001 ~]# sed -i 's#SELINUX=enforcing#SELINUX=disabled#g' /etc/selinux/config#关闭当前的SELINUX状态,如果不用setenforce那么需要重启系统[root@hadoop001 ~]# setenforce 0[root@hadoop001 ~]# grep SELINUX=disabled /etc/selinux/configSELINUX=disabled 2.JDK安装本文采用JDK1.7.79,详细安装步骤参考本博客的JDK安装的文章:JDK安装 3.创建Hadoop用户增加ID号为1000的hadoopGroup,增加ID为2000的用户并加入hadoopGroup组 固定用户和组的ID号是为了用户和组的权限一致，我遇到过一次在两台机器上用户名和组名相同，ID号不同结果导致权限有问题 12[root@hadoop001 ~]# groupadd -g 1000 hadoopGroup[root@hadoop001 ~]# useradd -u 2000 -g hadoopGroup hadoop 设置hadoop用户的密码为hadoop，下面的这种方法免去了修改密码时的密码确认 1[root@hadoop001 ~]# echo \"hadoop\"|passwd --stdin hadoop 设置hadoop用户sodu权限 12[root@hadoop001 ~]# vi /etc/sudoershadoop ALL=(ALL) ALL 4.上传（下载）解压Hadoop包hadoop-2.7.4.tar.gz包使用rz命令上传或通过wget命令去官网下载 123456789101112[root@hadoop001 ~]# mkdir -p /opt/software#将hadoop安装包放到/opt/software[root@hadoop001 ~]# cd /opt/software[root@hadoop001 software]# tar -zxvf hadoop-2.7.4.tar.gz#设置软连接[root@hadoop001 software]# ln -s /opt/software/hadoop-2.7.4 /opt/software/hadoop#设置hadoop目录所有者、所属组[root@hadoop001 software]# chown -R hadoop:hadoopGroup hadoop/*[root@hadoop001 software]# chown -R hadoop:hadoopGroup hadoop-2.7.4[root@hadoop001 software]# chown -R hadoop:hadoopGroup hadoop-2.7.4/* 5.配置环境变量1234567891011121314151617181920212223[root@hadoop001 software]# vim /etc/profile#按组合键shift+g 或大写G 到最后一行，按字母o 插入下列内容 #SET HADOOP exprot HADOOP_HOME=/opt/software/hadoop#如果JDK环境变量没有配置，请加入下列内容 #SET JDK export JAVA_HOME=/usr/java/jdk1.7.0_79 export JRE_HOME=$JAVA_HOME/jre export CLASS_PATH=.:$JAVA_HOME/lib:$JAVA_HOME/lib/tools.jar#设置PATH export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$&#123;JAVA_HOME&#125;/bin:$&#123;JRE_HOME&#125;/bin:$PATH#添加完成后 :wq 保存#使修改生效：[root@hadoop001 software]#source !$#验证JDK有效性[root@hadoop001 software]# java -versionjava version \"1.7.0_79\"Java(TM) SE Runtime Environment (build 1.7.0_79-b15)Java HotSpot(TM) 64-Bit Server VM (build 24.79-b02, mixed mode) 6.配置SSH123456789101112131415161718192021222324#检查sshd[root@hadoop001 software]# service sshd statusopenssh-daemon (pid 1844) is running...[root@hadoop001 software]# su - hadoop[hadoop@hadoop001 ~]# cd /opt/software/hadoop#SSH 免密设置#由于是伪分布式需要用到ssh localhost的情况，所以需要在本机生成公钥[hadoop@hadoop001 hadoop]$ ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa[hadoop@hadoop001 hadoop]$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys[hadoop@hadoop001 hadoop]$ chmod 0600 ~/.ssh/authorized_keys# 验证SSH[hadoop@hadoop001 hadoop]$ssh hadoop001 dateThe authenticity of host 'hadoop001 (192.168.137.130)' can't be established.RSA key fingerprint is 09:f6:4a:f1:a0:bd:79:fd:34:e7:75:94:0b:3c:83:5a.Are you sure you want to continue connecting (yes/no)? yes #第一次回车输入yesWarning: Permanently added 'hadoop001,192.168.137.130' (RSA) to the list of known hosts.Sun Aug 20 14:22:28 CST 2017[hadoop@hadoop001 hadoop]$ssh hadoop001 date #不需要回车输入yes,即SSH配置成功Sun Aug 20 14:22:29 CST 2017 7.配置core-site.xml12345678[hadoop@hadoop001 hadoop]$ vi etc/hadoop/core-site.xml#配置HDFS地址&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop001:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 8.配置hdfs-site.xml12345678910111213141516[hadoop@hadoop001 hadoop]$ vi etc/hadoop/hdfs-site.xml#配置HDFS副本数&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;hadoop001:50090&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.https-address&lt;/name&gt; &lt;value&gt;hadoop001:50091&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 9.修改mapred-site.xml12345678[hadoop@hadoop001 hadoop]$ vi etc/hadoop/mapred-site.xml:#设置mapreduce框架为yarn&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 10.修改yarn-site.xml12345678[hadoop@hadoop001 hadoop]$ etc/hadoop/yarn-site.xml:&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 11.修改slave文件将本机的主机名写入slave节点 1ehco 'hadoop001' &gt;/opt/software/hadoop/etc/hadooop/slave 12.启动HDFS和YARN1234567891011121314151617#添加环境变量[hadoop@hadoop001 hadoop]$ vi etc/hadoop/hadoop-env.shexport JAVA_HOME=/usr/java/jdk1.7.0_79[hadoop@hadoop001 hadoop]$ which hdfs/opt/software/hadoop/bin/hdfs[hadoop@hadoop001 hadoop]$ hdfs namenode -format........17/08/20 16:43:19 INFO common.Storage: Storage directory/tmp/hadoop-hadoop/dfs/name has been successfully formatted.[hadoop@hadoop001 hadoop]$ which start-dfs.sh/opt/software/hadoop/sbin/start-dfs.sh[hadoop@hadoop001 hadoop]$[hadoop@hadoop001 hadoop]$ start-dfs.sh[hadoop@hadoop001 hadoop]$ sbin/start-yarn.sh[hadoop@hadoop001 hadoop]$ sbin/mr-jobhistory-daemon.sh start historyserver 如果要结束进程，使用如下命令 123[hadoop@hadoop001 hadoop]$ sbin/stop-yarn.sh[hadoop@hadoop001 hadoop]$ stop-dfs.sh[hadoop@hadoop001 hadoop]$ sbin/mr-jobhistory-daemon.sh stop historyserver 13.检查启动进程通过jps命令查看 12345678[hadoop@hadoop001 hadoop]$ jps19536 DataNode19440 NameNode19876 Jps19740 SecondaryNameNode19888 ResourceManager19345 NodeManager4613 JobHistoryServer 通过WebUI查看 Browse the web interface for the NameNode - Browse the web interface for the ResourceManager - 14.验证MapReduce我们使用hadoop 的mapreduce-examples jar包来计算PI 12345678#Run a MapReduce job.[hadoop@hadoop001 mapreduce]$ pwd/opt/software/hadoop/share/hadoop/mapreduce[hadoop@hadoop002 mapreduce]$[hadoop@hadoop002 mapreduce]$ ll-rw-r--r--. 1 hadoop hadoop 301740 Aug 20 15:12 hadoop-mapreduce-examples-2.7.4.jar[hadoop@hadoop002 mapreduce]$ hadoop jar hadoop-mapreduce-examples-2.7.4.jar pi 5 10 15.参考资料 Setting up a Single Node Cluster：","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://xiejm.com/tags/Hadoop/"}]},{"title":"Hive--安装","date":"2017-10-02T15:44:44.000Z","path":"Hive/Hive--install.html","text":"本文主要是Hive部署的详细步骤 1.基本环境 CentOS 6.7 JDK1.8 Hadoop-2.6.0-cdh5.7.0 Hive-1.1.0-cdh5.7.0 Hive是基于Hadoop的，Hadoop的部署本文不在复述。 2.上传解压1tar -zxvf Hive-1.1.0-cdh5.7.0.tar.gz 3.配置环境变量123456vim /etc/profile#添加下面内容到/etc/profile文件的最后#set HIVEexport HIVE_HOME=/opt/software/hiveexport PATH=$HIVE_HOME/bin:$PATH 保存退出后使用命令source /etc/profile更新环境变量 4.修改hive-env.sh1234//hadoop环境设置HADOOP_HOME=/opt/software/hadoop//Hive的控制目录export HIVE_CONF_DIR=/opt/software/hive/conf 5.修改Hive-site.xml1vim $HIVE_HOME/conf/hive-site.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;?xml version=\"1.0\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;configuration&gt;&lt;!--######配置MySQL数据库连接串#######--&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost:3306/xiejm_hive?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt;&lt;!--连接驱动--&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt;&lt;!--连接用户名--&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt;&lt;!--连接密码--&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt;&lt;!--仓库路径配置：使用时注意权限，要有写的权限--&gt;&lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive/warehouse&lt;/value&gt; &lt;description&gt;location of default database for the warehouse&lt;/description&gt;&lt;/property&gt;&lt;!--在cli命令行上显示当前数据库，以及查询表的行头信息:$HIVE_HOME/conf/hive-site.xml--&gt;&lt;property&gt; &lt;name&gt;hive.cli.print.header&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;Whether to print the names of the columns in query output.&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.cli.print.current.db&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;Whether to include the current database in the Hive prompt.&lt;/description&gt;&lt;/property&gt;&lt;/configuration&gt; 6.修改日志输出12#copy日志配置文件的模板cp $HIVE_HOME/conf/conf/hive-log4j.properties.template hive-log4j.properties 123456hive.log.dir=$&#123;java.io.tmpdir&#125;/$&#123;user.name&#125;hive.log.file=hive.log#将上面内容改成下面的hive.log.dir=/opt/software/hive-1.1.0-cdh5.7.0/log/hive.log.file=xjm_hive.log 7.Hive启动验证1234567891011121314151617[root@hadoop001 ~]# which hive/opt/software/hive-1.1.0-cdh5.7.0/bin/hive###启动前记得加上mysql驱动包###//启动hive[root@hadoop001 ~]# hiveLogging initialized using configuration in file:/opt/software/hive-1.1.0-cdh5.7.0/conf/hive-log4j.propertiesWARNING: Hive CLI is deprecated and migration to Beeline is recommended.//创建表empcreate table emp(empno int,ename string,job string,mgr int,hiredate string,sal double,deptno int)row format delimited fields terminated by '\\t';//创建表加载数据load data local inpath \"/opt/emp.txt\" into table pepole;hive&gt; 至此安装结束 8.Hive的基本操作12345678910111213141516171819202122232425262728#查看数据库show databases;#使用数据库use datbase_name;#查看表show tables;#创建数据库create database datbase_name ;#创建表create table pepole(pid int,name string,phone string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';#加载HDFS数据到表load data inpath '/user/hive/warehouse/pepole/customers.csv' into table pepole ;#加载本地数据表load data local inpath \"/opt/datas/customers.csv\" into table pepole;#查询表select * from table-name ;#查看表属性desc table-name;#查看表详细属性desc extended table-name ;#查看格式化表属性desc formatted table-name ;#查看函数show functions ;#查看函数使用desc function fun-name ;#查看函数详细属性desc function extended upper ;","tags":[{"name":"Hive","slug":"Hive","permalink":"http://xiejm.com/tags/Hive/"}]},{"title":"Hadoop--HDFS架构","date":"2017-10-02T15:42:22.000Z","path":"Hadoop/Hadoop--HDFSdesign.html","text":"本文主要介绍HDFS架构概要Hadoop分布式文件系统(HDFS)是一个高度容错性的系统，适合部署在廉价的机器上。HDFS提供了高吞吐量的数据访问，适合大规模数据集的应用。HDFS采用master/slave的主从架构，HDFS集群是由一个NameNode()和一定数目的DataNode组成。 1.设计前提和目标 专为存储超大文件而设计：hdfs应该能够支持GB级别大小的文件；它应该能够提供很大的数据带宽并且能够在集群中拓展到成百上千个节点；它的一个实例应该能够支持千万数量级别的文件 适用于流式的数据访问：hdfs适用于批处理的情况而不是交互式处理；它的重点是保证高吞吐量而不是低延迟的用户响应 容错性：完善的冗余备份机制 支持简单的一致性模型：HDFS需要支持一次写入多次读取的模型，而且写入过程文件不会经常变化 移动计算优于移动数据：HDFS提供了使应用计算移动到离它最近数据位置的接口 兼容各种硬件和软件平台 2.不适合的场景 大量小文件：文件的元数据都存储在NameNode内存中，大量小文件会占用大量内存。 低延迟数据访问：hdfs是专门针对高数据吞吐量而设计的 多用户写入，任意修改文件 3. HDFS架构和设计： HDFS架构 HDFS主要由3个组件构成，分别是NameNode、SecondaryNameNode和DataNode： NameNode NameNode是整个文件系统的管理节点； 它维护着整个文件系统的文件及目录，以及接收HDFS Client的操作请求； NameNode 只有三种交互。 client访问NameNode获取相关DataNode信息。 DataNode心跳汇报当前block情况。 SecondaryNameNode做checkpoint交互。 DataNode 提供真实文件数据的存储服务 文件块（block）：最基本的存储单位。HDFS默认Block大小是64MB(1.0版本),128(2.0版本)，如果一个文件小于一个数据块的大小，HDFS并不占用整个数据块存储空间。Replication：多复本，默认是三个。 SecondaryNameNode HA的一个解决方案。但不支持热备。 执行过程：从NameNode上下载元数据信息（fsimage,edits），然后把二者合并，生成新的fsimage，在本地保存，并将其推送到NameNode，同时重置NameNode的edits.（默认在安装在NameNode节点上，但这样…不安全！） 参考资料：Hadoop官网","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://xiejm.com/tags/Hadoop/"}]},{"title":"Python 基础","date":"2017-09-14T02:04:14.000Z","path":"Python/Python-Base.html","text":"Python 基础：定义函数、常用数据结构、条件判断语句、for循环、字符串处理、异常处理、模块引入 1. 定义函数：12345678910111213#!/usr/bin/python#函数定义格式：def 函数名(参数)def print_your_name(name): print \"badou hadoop study!, your name\", name return (\"%s, is good student\" % name)#函数调用格式： 函数名(参数)print '================='print print_your_name('zhangsan')print '================='print print_your_name('lisi')print '=================' 2.常用数据结构哈希表：hashmap,数组：array,集合：set python中： hashmap -&gt; dict array -&gt; list set -&gt; set 12345678910#字典代码演示#dict color = &#123;\"red\":0.2,\"green\":0.4,\"blue\":0.4&#125; print color ['red'] print color ['green'] #输出结果 0.2 0.4 1234567#数组代码演示#listcolor_list = ['red','blue','green','yellow']print color_list[2]#输出结果blue 123456789101112131415#集合代码演示#set#使用前需声明a_set = set()a_set.add('111')a_set.add('222')a_set.add('333')a_set.add('111')print a_set#输出结果set(['111','222','333'])#set()的特性：在元素内不会出现重复元素 3.条件判断语句12345678910111213#ifa=1if a &gt; 0: print 'a gt 0'elif a == 0: print 'a eq 0'else: print 'a lt 0' # gt: 大于，greater than缩写# eq: 等于，equality 缩写# lt: 小于，less-than 缩写 4.for循环123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#for#数组a_list = []a_list.append['111']a_list.append['222']a_list.append['333']a_list.append['444']for value in a_list: print value #输出结果111222333444#集合 seta_set = set()a_set.add('a')a_set.add('b')a_set.add('d')a_set.add('d')a_set.add('e')for value in a_set: print value #输出结果： set内部通过hash排序，所以输出顺序与追加的item顺序不一致acbed#字典 dictb_dict = &#123;&#125;b_dict['aaa'] = 1b_dict['bbb'] = 2b_dict['ccc'] = 3b_dict['ddd'] = 4b_dict['eee'] = 5#字段 读取需要用到key,valuefor key, value int b_dict.items(): print key + \" ===&gt;\" + str(value)#输出结果 :字典内部顺序通过hashmap排序eee ===&gt; 5aaa ===&gt; 1bbb ===&gt; 2ccc ===&gt; 3ddd ===&gt; 4sum = 0for value in range(1,11): sum += valueprint sum 1234567891011121314#whilecnt = 2while cnt &gt; 0: print 'i love python' cnt -= 1# print duali = 1while i &lt; 10: i += 1 if i % 2 &gt; 0: continue print i 4.字符串处理123456789101112#stringstr = 'qwert'print len(str)#输出第二到第五个字符#string 取字符print str[2:5]str 'AbcDEf'#全部字符转换为小写print str.lower() 5.异常处理 123456789#exception#try... catch#try:a=6b=a/0exception Exception e: print Exception,\":\" e 6.模块引入12345678910111213141516import math #引入模块print math.pow(2,3)print round(4,9)import randomitem = [1,2,3,4,5,6]random.shuffle(items)print itemsa = random.randint(1,3)print as_list = random.sample('abcdefg',3)print s_list","tags":[{"name":"Python","slug":"Python","permalink":"http://xiejm.com/tags/Python/"}]},{"title":"Hadoop--HDFS之机架感知和副本策略","date":"2017-09-14T02:02:32.000Z","path":"Hadoop/Hadoop--HDFSreplication.html","text":"本文主要介绍Hadoop HDFS的机架感知和副本策略 其实NameNode在挑选合适的DataNode去存储Block的时候，不仅仅考虑了DataNode的存储空间够不够，还会考虑这些DataNode在不在同一个机架上。 这就需要NameNode必须知道所有的DataNode分别位于哪个机架上(所以也称为机架感知)。 当然，默认情况下NameNode是不会知道机架的存在的，也就是说，默认情况下，NameNode会认为所有的DataNode都在同一个机架上(/defaultRack)。 除非我们在hdfs-site.xml里面配置topology.script.file.name选项，这个选项的值是一个可执行文件的位置，而该只执行文件的作用是将输入的DataNode的ip地址按照一定规则计算，然后输出它所在的机架的名字，如/rack1, /rack2之类。借助这个文件，NameNode就具备了机架感知了。当它在挑选DataNode去存储Block的时候，它会遵循以下原则： 首先挑选跟HDFS Client所在的DataNode作为存放第一个Block副本的位置，如果HDFS Client不在任何一个DataNode上，比如说Hadoop集群外你自己的电脑，那么就任意选取一个DataNode。 其次，会借助NameNode的机架感知特性，选取跟第一个Block副本所在DataNode不同的机架上的任意一个DataNode来存放Block的第二个副本，比如说/rack2。Block的第三个副本也会存在这个/rack2上，但是是另外一个DataNode。 最后，如果我们设置的副本的数量大于3，那么剩下的副本则随意存储在集群中。 我们把上面的描述简单说下： 1st replica. 如果写请求方所在机器是其中一个个DataNode,则直接存放在本地,否则随机在集群中选择一个DataNode。 2nd replica. 第二个副本存放于不同第一个副本的所在的机架。 3rd replica.第三个副本存放于第二个副本所在的机架,但是属于不同的节点。","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://xiejm.com/tags/Hadoop/"},{"name":"HDFS","slug":"HDFS","permalink":"http://xiejm.com/tags/HDFS/"}]},{"title":"Hadoop--HDFS读写流程","date":"2017-09-13T13:57:06.000Z","path":"Hadoop/Hadoop--HDFSReadAndWrite.html","text":"本文主要记录Hadoop–HDFS读写流程的详细步骤：HDFS的设计是一次写入，多次读取，并且不支持文件修改，所以文件的读写就变的相当重要，下面我们就来一起看看HDFS文件的读写流程！ 1. 文件写入流程 HDFS的文件写入流程可以分为两个部分： 准备工作，包括与NameNode的通信等； 真正的写入操作。 我们先来看看第一部分的流程：建立连接 写流程的第一阶段 首先HDFS Client会去询问NameNode有哪些DataNode可以存储文件。 HDFS Client会把文件拆分，如果上图中文件被拆分成A、B、C三个Block NameNode通过查询它的元数据信息，发现DataNode1、2、7上有空间可以存储Block A，于是将此信息告诉HDFS Client。 HDFS Client接到NameNode返回的DataNode列表信息后，会直接与列表中的第一个DataNode联系，让它准备好接收Block A（建立TCP连接）并把NameNode返回的DataNode List信息发送给DataNode1 DataNode1与Client建立好TCP连接后，它会把HDFS Client要写入Block A的请求按顺序发送给DataNode2。同理，传递给DataNode7。 当DataNode7准备好后，会依次回传准备好接收Block A的反馈信息。 HDFS Client接收到反馈信息后，表示都准备好了，就可以写数据了。 写入数据下图展示了HDFS Client如何写入数据的流程 写流程的第二阶段 HDFS Client开始往DataNode1写入Block A数据。与第一部分的建立连接一样，当DataNode1接收完Block A数据后，它会按顺序将Block A数据传输给DataNode2，然后DataNode2再传输给DataNode7。 每个DataNode在接收完Block A数据后，会发消息给NameNode，告诉它Block数据已经接收完毕。 NameNode同时会根据它接收到的消息更新它保存的文件系统元数据信息。 当Block A成功写入3个DataNode之后，DataNode1会发送一个成功信息给HDFS Client，同时HDFS Client也会发一个Block A成功写入的信息给NameNode。之后，HDFS Client才能开始继续处理下一个Block-Block B。 2. 读取文件流程现在，我们来看看HDFS Client是如何从DataNode读取数据的。 如上图所示： 首先HDFS Client会去联系NameNode，询问file.txt总共有几个Block，这些Block存放在哪些DataNode上。 。由于每个Block都会存在几个副本，所以NameNode会把file.txt文件组成的Block所对应的所有DataNode列表都返回给HDFS Client。 然后HDFS Client会选择DataNode列表里的第一个DataNode去读取对应的Block，比如由于Block A存储在DataNode1，2，7，那么HDFS Client会到DataNode1去读取Block A；Block C存储在DataNode7，8，9，那么HDFS Client就回到DataNode7去读取Block C。 全部读完后关闭连接 参考资料： http://www.jianshu.com/p/7e52a7f8d16d","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://xiejm.com/tags/Hadoop/"},{"name":"HDFS","slug":"HDFS","permalink":"http://xiejm.com/tags/HDFS/"}]},{"title":"Hadoop-2.6.0-cdh5.7.0-native库缺失的问题","date":"2017-09-13T12:25:44.000Z","path":"Hadoop/Hadoop-2.6.0-cdh5.7.0-native.html","text":"在cloudera下载的 hadoop-2.6.0-cdh5.7.0，/lib/native 的静态库文件不存在 发现过程执行hadoop命令的时候会出现如下警告信息112/09/17 10:46:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 开启调试模式1export HADOOP_ROOT_LOGGER=DEBUG,console 再次执行hadoop命令的时候出现如下信息1234512/09/17 16:46:48 DEBUG util.NativeCodeLoader: Trying to load the custom-built native-hadoop library...12/09/17 16:46:48 DEBUG util.NativeCodeLoader: Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path12/09/17 16:46:48 DEBUG util.NativeCodeLoader: java.library.path=/opt/software/hadoop-2.6.0-cdh5.7.0/lib/native12/09/17 16:46:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable12/09/17 16:46:48 DEBUG util.PerformanceAdvisory: Falling back to shell based 发现hadoop的native本地库文件不存在1cd /opt/software/hadoop-2.6.0-cdh5.7.0/lib/native 解决办法下载hadoop-2.6.0-cdh5.7.0-src.tar.gz源码包，本地编译。具体编译流程详见本博的Hadoop源码文章编译完成后hadoop-2.6.0-cdh5.7.0-src/hadoop-dist/target/hadoop-2.6.0-cdh5.7.0/lib/native，里面有lib/native本地库文件，复制过去就可以了。 你也可以从我编译好的文件中下载，链接：http://pan.baidu.com/s/1bDtxXO 密码：8fif","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://xiejm.com/tags/Hadoop/"},{"name":"native","slug":"native","permalink":"http://xiejm.com/tags/native/"}]},{"title":"Spark2.2基于CentOS编译","date":"2017-09-12T03:04:36.000Z","path":"Spark/SparkBuildOnCentOS.html","text":"本文主要介绍Spark2.2的源码编译详细步骤。我们在生产环境中，Spark的版本并一定和Hadoop集群匹配，这样我们就需要编译Spark 1.基本环境要求 Spar编译的官方文档 中介绍可知Spark2.2需要JDK1.8+,Maven 3.3.9 1The Maven-based build is the build of reference for Apache Spark. Building Spark using Maven requires Maven 3.3.9 or newer and Java 8+. Note that support for Java 7 was removed as of Spark 2.2.0. 编译所需环境如下： CentOS 6.7 JDK1.8+ Maven 3.3.9 Scala 2.11 Git 2.编译环境安装JDK安装参考本博客的JDK安装的文章:JDK安装 Maven安装12345678910111213141516[root@hadoop001 software]# mkdir -p /opt/software[root@hadoop001 software]# cd /opt/software#上传apache-maven-3.3.9-bin.zip[root@hadoop001 software]# unzip apache-maven-3.3.9-bin.zip#添加maven环境变量[root@hadoop001 software]# echo '#SET MAVEN'&gt;&gt;/etc/profile[root@hadoop001 software]# echo 'export MAVEN_HOME=/opt/software/apache-maven-3.3.9' &gt;&gt;/etc/profile[root@hadoop001 software]# echo 'export MAVEN_OPTS=\"-Xmx2g -XX:ReservedCodeCacheSize=512m\"'&gt;&gt;/etc/profile[root@hadoop001 software]# echo 'export PATH=$MAVEN_HOME/bin:$PATH'&gt;&gt;/etc/profile# 让配置文件生效[root@hadoop001 software]# source /etc/profile#执行mvn 查看版本，如果安装成功会输出Maven版本号[root@hadoop001 software]# mvn --version Scala安装12345678910111213141516#解压scala-2.11.11[root@hadoop001 software]# tar -zxvf scala-2.11.11.tgz#添加Scala环境变量[root@hadoop001 software]# echo '#SET SCALA'&gt;&gt;/etc/profile[root@hadoop001 software]# echo 'export SCALA_HOME=/opt/software/scala-2.11.11' &gt;&gt;/etc/profile[root@hadoop001 software]# echo 'export PATH=$SCALA_HOME/bin:$PATH'&gt;&gt;/etc/profile# 让配置文件生效[root@hadoop001 software]# source /etc/profile# 验证Scala[root@hadoop001 software]# scala -versionScala code runner version 2.11.11 -- Copyright 2002-2017, LAMP/EPFL Git安装使用YUM命令安装git1[root@hadoop001 software]# yum install -y git 3.源码包下载从Spark官网下载源码包：http://spark.apache.org/downloads.html image 下载后将源码包上传至CentOS的/opt/sourcecode解压源码包1[root@hadoop001 sourcecode]# tar -zxvf spark-2.2.0.tgz 4.Maven相关配置我们采用CDH版本Hadoop，所以需要修改Spark源码目录下的pom.xml文件，在&lt;repositories&gt;&lt;/repositories&gt;标签对内添加以下内容12345&lt;repository&gt; &lt;id&gt;cdh&lt;/id&gt; &lt;name&gt;cloudera repository&lt;/name&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt;&lt;/repository&gt; 因Maven中心库在墙外，下载依赖包时会出现地址解析失败的情况，这里加上一个阿里云的Maven仓库来解决此问题12345&lt;repository&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt;&lt;/repository&gt; make-distribution.sh编译脚本执行时会检查系统Scala的版本，这个过程非常占时间，用vim命令把该脚本的120-133行注释掉，再添加以下内容：1234VERSION=2.2.0134 SCALA_VERSION=2.11135 SPARK_HADOOP_VERSION=2.6.0-cdh5.7.0136 SPARK_HIVE=1 4.开始编译执行以下脚本：1./dev/make-distribution.sh --name 2.6.0-cdh5.7.0 --tgz -Pyarn -Phadoop-2.6 -Phive -Phive-thriftserver -Dhadoop.version=2.6.0-cdh5.7.0 参数说明 -name:指定Spark编译成功后的安装包名称 -tgz:指定tgz压缩方式 -Pyarn：Spark on yarn -Phadoop-2.6:指定Hadoop版本 -Phive 和 -Phive-thriftserver：使支持Hive -Dhadoop.version=2.6.0-cdh5.7.0：指定Hadoop版本号 编译过程耗时很长，等编译成功后会在Spark源码目录下生成一个spark-2.2.0-bin-2.6.0-cdh5.7.0.tgz文件 参考资料： Spark官网文档","tags":[{"name":"Spark","slug":"Spark","permalink":"http://xiejm.com/tags/Spark/"}]},{"title":"CentOS 安装JDK常用的两种方法","date":"2017-09-12T00:17:16.000Z","path":"Linux/CentOS_setup_JDK.html","text":"本文主要记录在CentOS6.x中JDK的安装方法和详细步骤。JDK的安装方式一般有两种RPM包安装和tar包安装使用JDK1.7.79做示例 一.tar包安装通过rz命令上传JDK的tar包，或者使用wget命令下载 1234567891011121314151617181920212223242526#1.在/usr/目录下创建java目录[root@localhost ~]# mkdir -p /usr/java[root@localhost ~]# cd /usr/java#2.下载jdk,然后解压[root@localhost java]# wget http://download.Oracle.com/otn-pub/java/jdk/7u79-b15/jdk-7u79-linux-x64.tar.gz [root@localhost java]# tar -zxvf jdk-7u79-linux-x64.tar.gz[root@localhost java]#chown -R root:root jdk1.7.0_79#3.设置JDK环境变量[root@localhost java]# vi /etc/profile #SET JDK export JAVA_HOME=/usr/java/jdk1.7.0_79 export JRE_HOME=$JAVA_HOME/jre export CLASS_PATH=.:$JAVA_HOME/lib:$JAVA_HOME/lib/tools.jar export PATH=$&#123;JAVA_HOME&#125;/bin:$&#123;JRE_HOME&#125;/bin:$PATH #4.添加完成后 :wq 保存#5.使修改生效：[root@localhost java]#source !$#6.验证JDK有效性[root@localhost java]# java -versionjava version \"1.7.0_79\"Java(TM) SE Runtime Environment (build 1.7.0_79-b15)Java HotSpot(TM) 64-Bit Server VM (build 24.79-b02, mixed mode) 二.RPM包安装123456789#1.下载rpm安装文件[root@localhost ~]$ wget http://download.oracle.com/otn-pub/java/jdk/7u79-b15/jdk-7u79-linux-x64.rpm#2.使用rpm命令安装[root@localhost ~]# rpm -ivh jdk-7u79-linux-x64.rpm接下来的设置与tar包安装的第3-6步骤一样","tags":[{"name":"Linux","slug":"Linux","permalink":"http://xiejm.com/tags/Linux/"},{"name":"JDK","slug":"JDK","permalink":"http://xiejm.com/tags/JDK/"}]},{"title":"将CentOS的yum源更换为国内的阿里云源","date":"2017-09-12T00:15:38.000Z","path":"Linux/Change_CentOS_yum_repo_to_aliyun.html","text":"我们使用默认的yum源，有时会连接到国外的镜像站导致yum下载比较慢。所以将默认的yum源替换为阿里云的镜像站阿里云Linux安装镜像源地址：http://mirrors.aliyun.com/ 1.备份你的原镜像文件，以免出错后可以恢复。1mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup 2.下载新的CentOS-Base.repo 到/etc/yum.repos.d/12345678#CentOS 5 使用下面的链接wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-5.repo#CentOS 6 使用下面的链接wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repo#CentOS 7 使用下面的链接wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo 3.运行yum makecache生成缓存12yum clean allyum makecache","tags":[{"name":"Linux","slug":"Linux","permalink":"http://xiejm.com/tags/Linux/"},{"name":"yum","slug":"yum","permalink":"http://xiejm.com/tags/yum/"},{"name":"aliyun","slug":"aliyun","permalink":"http://xiejm.com/tags/aliyun/"}]},{"title":"Linux(CentOS 6)配置SSH无密码登陆","date":"2017-09-12T00:14:24.000Z","path":"Linux/sshRsaLogin.html","text":"最近在搭建Hadoop环境需要设置无密码登陆，所谓无密码登陆其实是指通过证书认证的方式登陆，使用一种被称为”公私钥”认证的方式来进行ssh登录。这篇教程介绍使用SSH Key来实现SSH无密码登录，而且使用scp复制文件时也不需要再输入密码．除了方便SSH登录，scp复制文件外，SSH无密码登录也为Linux服务器增加了又一道安全防线． 1.何为“公私钥“认证方式:首先在客户端上创建一对公私钥 （公钥文件：~/.ssh/id_rsa.pub； 私钥文件：~/.ssh/id_rsa）。然后把公钥放到服务器上（~/.ssh/authorized_keys）, 自己保留好私钥.在使用ssh登录时,ssh程序会发送私钥去和服务器上的公钥做匹配.如果匹配成功就可以登录了。 2.配置环境说明： CentOS 6.7 minimal 集群节点4台，并已经配置/etc/hosts主机映射 192.168.194.111 hadoop001 192.168.194.112 hadoop002 192.168.194.113 hadoop003 192.168.194.114 hadoop004 如果你有更多的节点，操作是一样的。当你的集群达到一定规模时，可以使用shell脚本或者python脚本来实现SSH免密配置 3.方法一配置步骤以下使用hadoop用户做示例,如果是其他用户请参照本示例操作 3.1 检查sshd每个节点都要操作123[root@hadoop001 software]# service sshd statusopenssh-daemon (pid 1844) is running...[root@hadoop001 software]# su - hadoop 3.2 生成公私钥在所有节点终端生成公钥，生成的密钥在.ssh 目录下，并进入.ssh目录 123[hadoop@hadoop001 hadoop]$ ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa[hadoop@hadoop001 hadoop]$ cd .ssh/[hadoop@hadoop001 hadoop]$ chmod 0600 ~/.ssh/authorized_keys` 3.3 合成并分发公钥只需在hadoop001节点操作，复制所有节点的公钥文件合并到authorized_keys12345678[hadoop@hadoop001 hadoop]$ ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa[hadoop@hadoop001 hadoop]$ cd .ssh/[hadoop@hadoop001 hadoop]$ chmod 0600 ~/.ssh/authorized_keys[hadoop@hadoop001 .ssh]$ ssh hadoop001 cat /home/hadoop/.ssh/id_rsa.pub &gt;&gt;authorized_keys[hadoop@hadoop001 .ssh]$ ssh hadoop002 cat /home/hadoop/.ssh/id_rsa.pub &gt;&gt;authorized_keys[hadoop@hadoop001 .ssh]$ ssh hadoop003 cat /home/hadoop/.ssh/id_rsa.pub &gt;&gt;authorized_keys[hadoop@hadoop001 .ssh]$ ssh hadoop004 cat /home/hadoop/.ssh/id_rsa.pub &gt;&gt;authorized_keys 3.4 分发authorized_keys1234[hadoop@hadoop001 .ssh]$ chmod 0600 ~/.ssh/authorized_keys[hadoop@hadoop001 .ssh]$ scp authorized_keys hadoop@hadoop002:/home/hadoop/.ssh/[hadoop@hadoop001 .ssh]$ scp authorized_keys hadoop@hadoop003:/home/hadoop/.ssh/[hadoop@hadoop001 .ssh]$ scp authorized_keys hadoop@hadoop004:/home/hadoop/.ssh/ 3.5 分发known_hosts1234[hadoop@hadoop001 .ssh]$ chmod 0600 ~/.ssh/authorized_keys[hadoop@hadoop001 .ssh]$ scp known_hosts hadoop@hadoop002:/home/hadoop/.ssh/[hadoop@hadoop001 .ssh]$ scp known_hosts hadoop@hadoop003:/home/hadoop/.ssh/[hadoop@hadoop001 .ssh]$ scp known_hosts hadoop@hadoop004:/home/hadoop/.ssh/ 3.6验证SSH12[hadoop@hadoop001 .ssh]$ssh hadoop001 date #不需要回车输入yes,即SSH配置成功Sat Sep 9 20:14:57 CST 2017 4.方法二配置步骤可以使用ssh-copy-id命令来完成．ssh-copy-id命令会自动上传SSH公钥到其他节点的.ssh/authorized_keys文件中．1234[hadoop@hadoop001 .ssh]$ssh-copy-id hadoop@hadoop001[hadoop@hadoop001 .ssh]$ssh-copy-id hadoop@hadoop002[hadoop@hadoop001 .ssh]$ssh-copy-id hadoop@hadoop003[hadoop@hadoop001 .ssh]$ssh-copy-id hadoop@hadoop004 同理其他节点也使用ssh-copy-id分发SSH公钥,这样也能达到相互免密登录的效果","tags":[{"name":"Linux","slug":"Linux","permalink":"http://xiejm.com/tags/Linux/"},{"name":"SSH","slug":"SSH","permalink":"http://xiejm.com/tags/SSH/"}]}]